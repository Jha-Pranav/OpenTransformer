# OpenTransformer

From Theory to Triumph: Embracing the Application of Learnings !!

This project focuses on learning and implementing various aspects of transformer architecture, particularly tailored towards building small, efficient language models suitable for low-end consumer-grade devices. The goal is to understand and create minimalist language models with fewer than 20 million parameters, optimized for specific tasks.

## Motivation

Large language models like demand substantial hardware resources, making them unsuitable for deployment on low-end consumer grade devices. This project aims to explore transformer architecture, develop a deep understanding, and implement efficient models that can run on resource-constrained devices.

## Libraries Used

- pytorch
- pytorch-lightning
- sentencepiece-tokenizer
- hf datasets
- w&b

## Usage

1. Clone the repository:

   ```bash
   git clone https://github.com/Jha-Pranav/OpenTransformer.git
   cd multiformer
   ```

2. Install dependencies:

   ```bash
   pip install -e .
   ```

3. Explore the codebase and run specific scripts for data preparation, training, fine-tuning, or inference.

## License

This project is licensed under the MIT License. See the [LICENSE](multiformer/LICENSE) file for details.

## Acknowledgements

- [karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)
- [karpathy/llama2.c](https://github.com/karpathy/llama2.c)
- [rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)
- [rasbt/MachineLearning-QandAI-book](https://github.com/rasbt/MachineLearning-QandAI-book)
- [meta-llama/llama](https://github.com/meta-llama/llama)
- [pytorch/torchtune](https://github.com/pytorch/torchtune)

Feel free to reach out with any questions, suggestions, or feedback! Let's build something amazing together! ðŸš€
