{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11fdee9d-7c9f-4688-80fe-9d9226da5576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "918dccfc-a247-4d85-af3b-8c423f8b7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "response_one = 0\n",
    "response_two = 0\n",
    "# Define the log file\n",
    "log_file = \"conversation-sample2.txt\"\n",
    "\n",
    "# Function to write and display logs with colors\n",
    "\n",
    "\n",
    "def log_response(role, response):\n",
    "    # Define colors\n",
    "    colors = {\n",
    "        \"assistant\": \"\\033[1;34m\",  # Bright Blue\n",
    "        \"user\": \"\\033[1;32m\",  # Bright Green\n",
    "        \"timestamp\": \"\\033[0;37m\",  # Light Gray\n",
    "        \"reset\": \"\\033[0m\",  # Reset to default\n",
    "    }\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Format for logging and console\n",
    "    formatted_log = f\"[{colors['timestamp']}{timestamp}{colors['reset']}] {colors[role.lower()]}{role.upper()}{colors['reset']}: {response}\\n\"\n",
    "\n",
    "    # Print the colored log to the console\n",
    "    # print(formatted_log, end=\"\")\n",
    "\n",
    "    # Save the log without color to the file\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"[{timestamp}] {role.upper()}: {response}\\n\")\n",
    "\n",
    "\n",
    "# for _ in range(1):\n",
    "def start_adversarial_game(\n",
    "    messages_one,\n",
    "    messages_two,\n",
    "    model_one_system_prompt,\n",
    "    model_two_system_prompt,\n",
    "    MODEL_ONE,\n",
    "    MODEL_TWO,\n",
    "):\n",
    "    # Generate response from MODEL_ONE\n",
    "    response_one = ollama.chat(\n",
    "        model=MODEL_ONE,\n",
    "        messages=[{\"role\": \"system\", \"content\": model_one_system_prompt}] + messages_one,\n",
    "    )[\"message\"][\"content\"]\n",
    "    messages_one.append({\"role\": \"assistant\", \"content\": response_one})\n",
    "    messages_two.append({\"role\": \"user\", \"content\": response_one})\n",
    "    log_response(\"assistant\", response_one)\n",
    "\n",
    "    response_two = ollama.chat(\n",
    "        model=MODEL_TWO,\n",
    "        messages=[{\"role\": \"system\", \"content\": model_two_system_prompt}] + messages_two,\n",
    "    )[\"message\"][\"content\"]\n",
    "    messages_two.append({\"role\": \"assistant\", \"content\": response_two})\n",
    "    messages_one.append({\"role\": \"user\", \"content\": response_two})\n",
    "    log_response(\"user\", response_two)\n",
    "\n",
    "    # Trim the message history if it exceeds the limit\n",
    "    if len(messages_one) > MAX_HISTORY:\n",
    "        messages = messages_one[-MAX_HISTORY:]\n",
    "    if len(messages_two) > MAX_HISTORY:\n",
    "        messages = messages_two[-MAX_HISTORY:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ddc2d4e-03f8-401b-a63d-3b35a07d20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_one = []\n",
    "messages_two = []\n",
    "model_one_system_prompt = \"You are a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way. Keep your conversation short.\"\n",
    "model_two_system_prompt = \"You are a very polite, courteous chatbot. You try to agree with everything the other person says, or find common ground. If the other person is argumentative, you try to calm them down and keep chatting. Keep your conversation short.\"\n",
    "\n",
    "MODEL_TWO = \"llama3:8b\"\n",
    "MODEL_ONE = \"llama3.1:latest\"\n",
    "\n",
    "MAX_HISTORY = 7\n",
    "for _ in range(50):\n",
    "    start_adversarial_game(\n",
    "        messages_one,\n",
    "        messages_two,\n",
    "        model_one_system_prompt,\n",
    "        model_two_system_prompt,\n",
    "        MODEL_ONE,\n",
    "        MODEL_TWO,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94894a44-8ade-4b25-a877-ffcefb52ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switching between models is very slow?\n",
    "# https://github.com/ollama/ollama/issues/3115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28470ce1-729d-4489-b950-be73fab586a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
