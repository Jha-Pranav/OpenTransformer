{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b16ed059-f58e-48ac-b714-052041c13543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORCH VERSION : 2.2.1\n",
      "GPU  :  cuda\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(123)\n",
    "print(\"TORCH VERSION :\", version(\"torch\"))\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backend.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(\"GPU  : \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e003266-526d-44c2-b017-cca392bcdd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ab2583e2-91f3-4629-bcd8-480877752aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1878c-327a-4544-b116-9efce702eb37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Layer normalization centers the activations of a neural network layer around a mean of 0 and normalizes their variance to 1\n",
    "\n",
    "Let's validate the claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2e1e39e-b34f-4f16-b163-e5a72cfa0f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tensor([[0.4593, 0.0000, 0.2737],\n",
      "        [0.5204, 0.0000, 0.3925]], grad_fn=<ReluBackward0>)\n",
      "****************************************\n",
      "Before LayerNorm\n",
      "****************************************\n",
      "Mean tensor([[0.2444],\n",
      "        [0.3043]], grad_fn=<MeanBackward1>)\n",
      "Var tensor([[0.0534],\n",
      "        [0.0735]], grad_fn=<VarBackward0>)\n",
      "****************************************\n",
      "After LayerNorm\n",
      "****************************************\n",
      ">> Cal it manually\n",
      "Mean tensor([[     0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "Var tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n",
      ">> Cal it nn.LayeNorm\n",
      "Mean tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Var tensor([[1.4996],\n",
      "        [1.4997]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 4)\n",
    "layer = nn.Sequential(nn.Linear(4, 3), nn.ReLU())\n",
    "output = layer(x)\n",
    "print(\"output\", output)\n",
    "print(\n",
    "    \"**\" * 20,\n",
    ")\n",
    "print(\"Before LayerNorm\")\n",
    "print(\n",
    "    \"**\" * 20,\n",
    ")\n",
    "print(\"Mean\", output.mean(dim=-1, keepdim=True))\n",
    "print(\"Var\", output.var(dim=-1, keepdim=True))\n",
    "print(\n",
    "    \"**\" * 20,\n",
    ")\n",
    "print(\"After LayerNorm\")\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\n",
    "    \"**\" * 20,\n",
    ")\n",
    "\n",
    "print(\">> Cal it manually\")\n",
    "\n",
    "ln_out = (output - output.mean(dim=-1, keepdim=True)) / torch.sqrt(\n",
    "    output.var(dim=-1, keepdim=True)\n",
    ")\n",
    "print(\"Mean\", ln_out.mean(dim=-1, keepdim=True))\n",
    "print(\"Var\", ln_out.var(dim=-1, keepdim=True))\n",
    "\n",
    "print(\">> Cal it nn.LayeNorm\")\n",
    "ln_out = nn.LayerNorm(3)(output)\n",
    "print(\"Mean\", ln_out.mean(dim=-1, keepdim=True))\n",
    "print(\"Var\", ln_out.var(dim=-1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac2e2f3d-042f-412e-b46b-2c8a0ec28fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we see var for layer norm calculated using nn.LayerNorm is not equal to abs 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b0ed8c7-d21c-434a-af7a-c229a7fc1f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[     1.1169,     -1.1728,      0.0684,     -1.1630,      0.7117,\n",
       "               0.8475,     -1.9342,      0.9364,      0.3283,      0.2608],\n",
       "         [     1.1683,      2.0452,      0.0321,     -1.4593,     -0.2916,\n",
       "              -1.1637,      0.4462,     -0.0822,     -0.8129,      0.1180],\n",
       "         [     1.7241,     -1.1016,     -0.7970,     -1.3138,     -0.4516,\n",
       "               1.5688,      0.7992,      0.0929,     -0.2861,     -0.2352],\n",
       "         [     0.7670,     -0.0220,     -1.2466,     -0.7707,      1.4821,\n",
       "               0.2672,     -1.0640,      0.4171,      1.4403,     -1.2705],\n",
       "         [    -0.9695,     -0.4962,      0.2585,     -0.2121,      0.0002,\n",
       "              -1.4888,      0.9955,      1.7004,      1.2416,     -1.0296]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Use nn.LayerNorm\n",
    "batch, seq_len, embedding_dim = 1, 5, 10\n",
    "x = torch.rand(1, 5, 10)\n",
    "\n",
    "ln = nn.LayerNorm(embedding_dim)\n",
    "ln(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56198e58-05e8-42c1-8745-e446f0bb1efd",
   "metadata": {},
   "source": [
    "In the std calculation, setting unbiased=False means using the formula to compute the variance where n is the sample size (here, the number of features or columns); this formula does not include Bessel's correction (which uses n-1 in the denominator), thus providing a biased estimate of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eefa3b57-8d1c-4b41-b24e-36c7f8b794c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, eps: float = 10**-6, unbiased: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.unbiased = unbiased\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # multiplicative parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(1))  # additive parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True, unbiased=self.unbiased)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4b59523d-9330-4053-bfee-9147ae5c920e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 12])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 10, 12)\n",
    "x = torch.exp(x)\n",
    "LayerNormalization()(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53b2a7-1d60-464a-a4b8-606c4dffab2a",
   "metadata": {},
   "source": [
    "Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da64834-c8ef-4287-9f22-9115d1c42a21",
   "metadata": {},
   "source": [
    "re-centering invariance in LayerNorm is dispensable .RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a0a42ed2-a1ef-4a7d-862c-b34cdeac988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ec7d83d-afe1-4269-bad1-be8ed6500098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9024, 0.1123],\n",
       "        [0.2685, 0.6591],\n",
       "        [0.1735, 0.9247],\n",
       "        [0.6166, 0.3608],\n",
       "        [0.5325, 0.6559]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(torch.ones(1)) * torch.rand(5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cc940971-4126-45cc-a7f6-1e525e7176ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.w = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rvariance = torch.rsqrt(\n",
    "            (x.pow(2)).mean(dim=-1, keepdim=True) + self.eps\n",
    "        )  # 1/variance\n",
    "        norm = (x * rvariance).type_as(x)\n",
    "        return self.w * norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "99dab20e-58c1-4b56-9b9d-bc4669952ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 12])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1, 10, 12)\n",
    "\n",
    "RMSLayerNormalization()(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69302ad-a2d7-41c3-8559-ad8ff6da2def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ba1cb127-3f81-498f-a043-9b3f72f8f3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Before LayerNorm\n",
      "****************************************\n",
      "Mean tensor(0.1235, grad_fn=<MeanBackward0>)\n",
      "Var tensor(0.0346, grad_fn=<MeanBackward0>)\n",
      "****************************************\n",
      "After LayerNorm\n",
      "****************************************\n",
      ">> Class : LayerNormalization\n",
      "Mean tensor(    0.0000, grad_fn=<MeanBackward0>)\n",
      "Var tensor(1.0007, grad_fn=<MeanBackward0>)\n",
      ">> Class : RMSLayerNormalization\n",
      "Mean tensor(0.5532, grad_fn=<MeanBackward0>)\n",
      "Var tensor(0.6944, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 1024, 2048)\n",
    "layer = nn.Sequential(nn.Linear(2048, 1500), nn.ReLU())\n",
    "output = layer(x)\n",
    "# print(\"output\",output)\n",
    "print(\n",
    "    \"**\" * 20,\n",
    ")\n",
    "print(\"Before LayerNorm\")\n",
    "print(\n",
    "    \"**\" * 20,\n",
    ")\n",
    "print(\"Mean\", output.mean(dim=-1, keepdim=True).mean())\n",
    "print(\"Var\", output.var(dim=-1, keepdim=True).mean())\n",
    "print(\n",
    "    \"**\" * 20,\n",
    ")\n",
    "print(\"After LayerNorm\")\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\n",
    "    \"**\" * 20,\n",
    ")\n",
    "\n",
    "print(\">> Class : LayerNormalization\")\n",
    "\n",
    "ln_out = LayerNormalization()(output)\n",
    "print(\"Mean\", ln_out.mean(dim=-1, keepdim=True).mean())\n",
    "print(\"Var\", ln_out.var(dim=-1, keepdim=True).mean())\n",
    "\n",
    "print(\">> Class : RMSLayerNormalization\")\n",
    "ln_out = RMSLayerNormalization()(output)\n",
    "print(\"Mean\", ln_out.mean(dim=-1, keepdim=True).mean())\n",
    "print(\"Var\", ln_out.var(dim=-1, keepdim=True).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bd821-311a-4169-aaf5-b768924b5d75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
