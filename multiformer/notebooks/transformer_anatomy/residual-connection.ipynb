{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd145239-fb9e-46cd-b5f6-3c553639267c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORCH VERSION : 2.2.1\n",
      "GPU  :  cuda\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "\n",
    "print(\"TORCH VERSION :\", version(\"torch\"))\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'mps' if torch.backend.mps.is_available() else 'cpu'\n",
    "print('GPU  : ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dd4dd8b-20dd-4284-9393-fba1d86233e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c0108d-c491-42d4-9263-9d31ea44a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnectionLN(nn.Module):\n",
    "    def __init__(self,normalized_shape:int,dropout:float=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(normalized_shape)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84675455-c1e4-4bd2-83be-9a9582d80dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Short Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "061d996b-2ed8-498f-9832-e0d22dafa236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DummyNW(nn.Module):\n",
    "    def __init__(self,dim:torch.Tensor,target=torch.tensor([0.]),short_circuit=True):\n",
    "        super().__init__()\n",
    "        self.short_circuit = short_circuit\n",
    "        self.layers = nn.ModuleList([nn.Sequential(nn.Linear(dim[i],dim[i+1]),nn.GELU()) for i in range(len(dim)-1)])\n",
    "\n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            out = layer(x)\n",
    "            if self.short_circuit:\n",
    "                x = x + out\n",
    "            else:\n",
    "                x = out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c6fe016-6f48-4619-b147-fd66cf02514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim = [3, 3, 3, 3, 3, 1]  \n",
    "dim = [100,100,100,100,100,100,100,100,1]\n",
    "x = torch.rand(32,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5c9eed3f-e551-4d62-b885-1e99f75167a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_grad(model,x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    \n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "685ae064-1c75-4693-a59a-e4342e0767f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.027854595333337784\n",
      "layers.1.0.weight has gradient mean of 0.027477923780679703\n",
      "layers.2.0.weight has gradient mean of 0.02715429663658142\n",
      "layers.3.0.weight has gradient mean of 0.027854714542627335\n",
      "layers.4.0.weight has gradient mean of 0.03017064556479454\n",
      "layers.5.0.weight has gradient mean of 0.032704733312129974\n",
      "layers.6.0.weight has gradient mean of 0.030148791149258614\n",
      "layers.7.0.weight has gradient mean of 1.4742094278335571\n"
     ]
    }
   ],
   "source": [
    "# Next, let's print the gradient values with short_circuit connections:\n",
    "get_grad( DummyNW(dim),x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ae4f747a-e2cb-43b0-9bf8-77d354136f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 1.1193534277254003e-07\n",
      "layers.1.0.weight has gradient mean of 8.519539562712453e-08\n",
      "layers.2.0.weight has gradient mean of 9.869211226032348e-08\n",
      "layers.3.0.weight has gradient mean of 2.509366083813802e-07\n",
      "layers.4.0.weight has gradient mean of 7.714727985330683e-07\n",
      "layers.5.0.weight has gradient mean of 3.048964117624564e-06\n",
      "layers.6.0.weight has gradient mean of 1.2048118151142262e-05\n",
      "layers.7.0.weight has gradient mean of 0.0005355386529117823\n"
     ]
    }
   ],
   "source": [
    "\n",
    "get_grad(DummyNW(dim,short_circuit=False),x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce038f-284b-4c1c-9843-57ad512443b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can see based on the output above, short_circuit connections prevent the gradients from vanishing in the early layers (towards layer.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
