{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3f41c2-8cf0-4ae3-b2e5-441a501ac6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer as HFTokenizer\n",
    "\n",
    "hf_tokenizer = HFTokenizer.from_file(\n",
    "    \"/home/pranav-pc/projects/OpenTransformer/multiformer/tokenizer_checkpoints/tokenizer.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7a5d1a1-e4d2-4be7-b28a-15a304ae23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "sp_tokenizer = SentencePieceProcessor(\n",
    "    \"/home/pranav-pc/projects/OpenTransformer/multiformer/tokenizer_checkpoints/tokenizer.model\",\n",
    "    add_bos=True,\n",
    "    add_eos=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36733a-307c-4a7a-95fe-6b03e57b617a",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031013b2-4918-449e-a34d-d337729345e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face datasets library\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "raw_text = dataset[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c9b844-66b4-4987-b001-f517c315d273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.68 s ± 146 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit [hf_tokenizer.encode(text).ids for text in raw_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30c6d3e4-945d-48c0-8f6d-4f4a48d9eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503 ms ± 16.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sp_tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c044ed42-acba-4a11-8d55-e0e03b422d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.78 s ± 32.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit [hf_tokenizer.decode(hf_tokenizer.encode(text).ids) for text in raw_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f61ff0d-46a2-4474-b641-3f276921778c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "726 ms ± 31.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sp_tokenizer.decode(sp_tokenizer.encode(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5ebce-edf7-4ea8-86c8-3e1fb9f33b9e",
   "metadata": {},
   "source": [
    "It's quite apparent that while Hugging Face may offer feature-rich functionality, SentencePieceTokenizer suffices for our use case and boasts significantly faster performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020cacae-301f-49c0-b32b-3aa2a20a0b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
