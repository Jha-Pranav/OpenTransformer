{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d32caf4-5896-4322-87fc-004d0bdfbfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORCH VERSION : 2.2.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "\n",
    "print(\"TORCH VERSION :\", version(\"torch\"))\n",
    "device = (\n",
    "\"cuda\"\n",
    "if torch.cuda.is_available()\n",
    "else \"mps\" if torch.backend.mps.is_available() else \"cpu\"\n",
    ")\n",
    "# print(\"Device  : \", device.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392b2032-0d25-4e47-930b-3f2dc199c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "dtype = (\n",
    "\"bfloat16\"\n",
    "if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "else \"float16\"\n",
    ")\n",
    "\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7594d4-3558-4e8b-9a60-44dd1388495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b44cf651-0ccc-4cf5-a567-7eaa487dea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    vocab_size: int = 32000  # llama2 tokenizer has 32k vocab size\n",
    "    emebdding_dim: int = 4096\n",
    "    max_seq_len = 2048\n",
    "    embedding_dropout: float = 0.0\n",
    "\n",
    "    rms_norm_eps: float = 1e-05\n",
    "\n",
    "    rope_scaling: float = 1.0\n",
    "    rope_theta: float = 10000.0\n",
    "\n",
    "    attention_bias: bool = False\n",
    "    attention_dropout: float = 0.0\n",
    "    num_attention_heads: int = 32\n",
    "    num_key_value_heads: int = 32\n",
    "    use_cache: bool = True\n",
    "    use_sliding_window: bool = True\n",
    "    residual_dropout = 0.1\n",
    "\n",
    "    mlp_hidden_size = int(1.3 * emebdding_dim)  # set some lambda function or scaling factor\n",
    "    mlp_dropout: float = 0.0\n",
    "\n",
    "    num_layers: int = 32\n",
    "\n",
    "    device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backend.mps.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab8e4f2-d4dd-4865-85bb-afe830d416fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cells.position import RotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78f2a86e-d61d-4071-b660-d3ac869b599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,args:ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        args.num_attention_heads % args.num_key_value_heads == 0, \"Head counts should be divisible KV Heads counts\"\n",
    "        self.group_factor = args.num_attention_heads // args.num_key_value_heads \n",
    "        self.head_dim = args.emebdding_dim // args.num_attention_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.emebdding_dim, args.emebdding_dim,bias=args.attention_bias)\n",
    "        self.wk = nn.Linear(args.emebdding_dim, args.emebdding_dim//self.group_factor,bias=args.attention_bias)\n",
    "        self.wv = nn.Linear(args.emebdding_dim, args.emebdding_dim//self.group_factor,bias=args.attention_bias)\n",
    "        self.wo = nn.Linear(args.emebdding_dim, args.emebdding_dim,bias=args.attention_bias)\n",
    "\n",
    "        self.dropout = args.attention_dropout\n",
    "        self.residual_dropout = nn.Dropout(args.residual_dropout)\n",
    "        self.rope_q = RotaryEmbedding(self.head_dim,args.max_seq_len,device=args.device)\n",
    "        self.rope_k = RotaryEmbedding(self.head_dim,args.max_seq_len,device=args.device)\n",
    "\n",
    "        # Freeze the parameters rope_q and rope_k\n",
    "        self.rope_q.requires_grad_(False)\n",
    "        self.rope_k.requires_grad_(False)\n",
    "\n",
    "\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        b,seqlen,_ = x.shape\n",
    "        # QKV\n",
    "        xq,xk,xv = self.wq(x).view(b,seqlen,-1,self.head_dim),self.wk(x).view(b,seqlen,-1,self.head_dim),self.wv(x).view(b,seqlen,-1,self.head_dim)\n",
    "        # RoPE on Q,K\n",
    "        xq = self.rope_q(xq).transpose(1, 2)  \n",
    "        xk = self.rope_k(xk).transpose(1, 2)\n",
    "        \n",
    "        xv = xv.transpose(1, 2)\n",
    "        if hasattr(F, 'scaled_dot_product_attention'):\n",
    "            output = F.scaled_dot_product_attention(xq, xk, xv, attn_mask=None, dropout_p=self.dropout if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            raise NotImplemented('Upgrade to pytorch version >= 2.0')\n",
    "            \n",
    "        # restore time as batch dimension and concat heads\n",
    "        output = output.transpose(1, 2).contiguous().view(b, seqlen, -1)\n",
    "        return self.residual_dropout(self.wo(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4289072-4bc2-408c-af94-3dd6bfa9f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelArgs()\n",
    "model = Attention(config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78713717-7362-4d14-b35d-089e86f3b491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2048, 4096])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(8,2048,4096,device=device)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff4db45-4618-49f4-a56f-cd59314dbf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.cells.normalization import RMSLayerNorm\n",
    "from src.cells.attention import GQMultiHeadAttention\n",
    "from src.cells.feedforward import FeedForward\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,args) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.norms = RMSLayerNorm(args.emebdding_dim,eps=args.rms_norm_eps)\n",
    "        self.attention = GQMultiHeadAttention(args)\n",
    "        self.mlp = FeedForward(args.emebdding_dim,args.mlp_hidden_size,dropout=args.mlp_dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attention(self.norms(x))\n",
    "        x = x + self.mlp(self.norms(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abeba340-128c-467a-a9f7-b019322b8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2048, 4096])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity - should be of same size as input\n",
    "Block(config).to(device)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94f33f5a-8018-4819-9821-956ff0dd0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from src.models.blm.config import ModelArgs\n",
    "from src.models.blm.block import Block\n",
    "\n",
    "from src.cells.normalization import RMSLayerNorm\n",
    "from typing import Optional\n",
    "class BabyLangModel(nn.Module):\n",
    "    def __init__(self,args:ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_embd = nn.Embedding(args.vocab_size,args.emebdding_dim)\n",
    "        self.dropout = nn.Dropout(args.embedding_dropout)\n",
    "        self.layers = nn.ModuleList([Block(args) for lid in range(args.num_layers)])\n",
    "\n",
    "        self.norm =  RMSLayerNorm(args.emebdding_dim,eps=args.rms_norm_eps)\n",
    "        self.output = nn.Linear(args.emebdding_dim, args.vocab_size, bias=False)\n",
    "\n",
    "        # share the unembedding parameters with the embedding parameters\n",
    "        self.tok_embd.weight = self.output.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('wo.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * args.n_layers))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens : torch.Tensor ,targets: Optional[torch.Tensor] = None) -> torch.Tensor\n",
    "        b, seqlen = tokens.shape\n",
    "        x = self.dropout(self.tok_embd(tokens))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)     ## How about we add residual connection here also ?\n",
    "        x = self.norm(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.output(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the output on the very last position\n",
    "            logits = self.output(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3d814-7feb-4ff0-a309-b1947dac838d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
