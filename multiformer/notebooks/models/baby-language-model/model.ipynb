{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d32caf4-5896-4322-87fc-004d0bdfbfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORCH VERSION : 2.2.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "\n",
    "print(\"TORCH VERSION :\", version(\"torch\"))\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backend.mps.is_available() else \"cpu\"\n",
    ")\n",
    "# print(\"Device  : \", device.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "392b2032-0d25-4e47-930b-3f2dc199c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")\n",
    "\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc7594d4-3558-4e8b-9a60-44dd1388495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44cf651-0ccc-4cf5-a567-7eaa487dea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    vocab_size: int = 32000  # llama2 tokenizer has 32k vocab size\n",
    "    emebdding_dim: int = 4096\n",
    "    max_seq_len: int = 2048\n",
    "    embedding_dropout: float = 0.0\n",
    "\n",
    "    rms_norm_eps: float = 1e-05\n",
    "\n",
    "    rope_scaling: float = 1.0\n",
    "    rope_theta: float = 10000.0\n",
    "\n",
    "    attention_bias: bool = False\n",
    "    attention_dropout: float = 0.0\n",
    "    num_attention_heads: int = 32\n",
    "    num_key_value_heads: int = 32\n",
    "    use_cache: bool = True\n",
    "    use_sliding_window: bool = True\n",
    "    residual_dropout: float = 0.1\n",
    "\n",
    "    mlp_hidden_size: int = int(\n",
    "        1.3 * emebdding_dim\n",
    "    )  # set some lambda function or scaling factor\n",
    "    mlp_dropout: float = 0.0\n",
    "\n",
    "    num_layers: int = 32\n",
    "\n",
    "    device: str = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.backend.mps.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ab8e4f2-d4dd-4865-85bb-afe830d416fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cells.position import RotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78f2a86e-d61d-4071-b660-d3ac869b599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        args.num_attention_heads % args.num_key_value_heads == 0, \"Head counts should be divisible KV Heads counts\"\n",
    "        self.group_factor = args.num_attention_heads // args.num_key_value_heads\n",
    "        self.head_dim = args.emebdding_dim // args.num_attention_heads\n",
    "\n",
    "        self.wq = nn.Linear(\n",
    "            args.emebdding_dim, args.emebdding_dim, bias=args.attention_bias\n",
    "        )\n",
    "        self.wk = nn.Linear(\n",
    "            args.emebdding_dim,\n",
    "            args.emebdding_dim // self.group_factor,\n",
    "            bias=args.attention_bias,\n",
    "        )\n",
    "        self.wv = nn.Linear(\n",
    "            args.emebdding_dim,\n",
    "            args.emebdding_dim // self.group_factor,\n",
    "            bias=args.attention_bias,\n",
    "        )\n",
    "        self.wo = nn.Linear(\n",
    "            args.emebdding_dim, args.emebdding_dim, bias=args.attention_bias\n",
    "        )\n",
    "\n",
    "        self.dropout = args.attention_dropout\n",
    "        self.residual_dropout = nn.Dropout(args.residual_dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, rope_q: RotaryEmbedding, rope_k: RotaryEmbedding\n",
    "    ):\n",
    "        b, seqlen, _ = x.shape\n",
    "        # QKV\n",
    "        xq, xk, xv = (\n",
    "            self.wq(x).view(b, seqlen, -1, self.head_dim),\n",
    "            self.wk(x).view(b, seqlen, -1, self.head_dim),\n",
    "            self.wv(x).view(b, seqlen, -1, self.head_dim),\n",
    "        )\n",
    "        # RoPE on Q,K\n",
    "        xq = rope_q(xq).transpose(1, 2)\n",
    "        xk = rope_k(xk).transpose(1, 2)\n",
    "\n",
    "        xv = xv.transpose(1, 2)\n",
    "        if hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            output = F.scaled_dot_product_attention(\n",
    "                xq,\n",
    "                xk,\n",
    "                xv,\n",
    "                attn_mask=None,\n",
    "                dropout_p=self.dropout if self.training else 0.0,\n",
    "                is_causal=True,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplemented(\"Upgrade to pytorch version >= 2.0\")\n",
    "\n",
    "        # restore time as batch dimension and concat heads\n",
    "        output = output.transpose(1, 2).contiguous().view(b, seqlen, -1)\n",
    "        return self.residual_dropout(self.wo(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4289072-4bc2-408c-af94-3dd6bfa9f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = ModelArgs()\n",
    "# model = Attention(config)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78713717-7362-4d14-b35d-089e86f3b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(8,2048,4096,device=device)\n",
    "# model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ff4db45-4618-49f4-a56f-cd59314dbf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.cells.normalization import RMSLayerNorm\n",
    "from src.cells.attention import GQMultiHeadAttention\n",
    "from src.cells.feedforward import FeedForward\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, args) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.norms = RMSLayerNorm(args.emebdding_dim, eps=args.rms_norm_eps)\n",
    "        self.attention = GQMultiHeadAttention(args)\n",
    "        self.mlp = FeedForward(\n",
    "            args.emebdding_dim, args.mlp_hidden_size, dropout=args.mlp_dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x, rope_q, rope_k):\n",
    "        x = x + self.attention(self.norms(x), rope_q, rope_k)\n",
    "        x = x + self.mlp(self.norms(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abeba340-128c-467a-a9f7-b019322b8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanity - should be of same size as input\n",
    "# Block(config).to(device)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94f33f5a-8018-4819-9821-956ff0dd0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from src.models.blm.config import ModelArgs\n",
    "from src.models.blm.block import Block\n",
    "\n",
    "from src.cells.normalization import RMSLayerNorm\n",
    "from typing import Optional\n",
    "from src.cells.position import RotaryEmbedding\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_embd = nn.Embedding(args.vocab_size, args.emebdding_dim)\n",
    "        self.dropout = nn.Dropout(args.embedding_dropout)\n",
    "        head_dim = args.emebdding_dim // args.num_attention_heads\n",
    "        q_kv_factor = args.num_attention_heads // args.num_key_value_heads\n",
    "        self.rope_q = RotaryEmbedding(\n",
    "            head_dim,\n",
    "            args.max_seq_len,\n",
    "            device=args.device,\n",
    "        )\n",
    "        self.rope_k = RotaryEmbedding(\n",
    "            head_dim // q_kv_factor,\n",
    "            args.max_seq_len,\n",
    "            device=args.device,\n",
    "        )\n",
    "\n",
    "        # Freeze the parameters rope_q and rope_k\n",
    "        self.rope_q.requires_grad_(False)\n",
    "        self.rope_k.requires_grad_(False)\n",
    "\n",
    "        self.layers = nn.ModuleList([Block(args) for lid in range(args.num_layers)])\n",
    "\n",
    "        self.norm = RMSLayerNorm(args.emebdding_dim, eps=args.rms_norm_eps)\n",
    "        self.output = nn.Linear(args.emebdding_dim, args.vocab_size, bias=False)\n",
    "\n",
    "        # share the unembedding parameters with the embedding parameters\n",
    "        self.tok_embd.weight = (\n",
    "            self.output.weight\n",
    "        )  # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"wo.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    p, mean=0.0, std=0.02 / math.sqrt(2 * args.num_layers)\n",
    "                )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.get_num_params()} Million Params Model\"\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.tok_embd.weight.numel()\n",
    "        return n_params / 1e6  # In Million\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(\n",
    "        self, tokens: torch.Tensor, targets: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.dropout(self.tok_embd(tokens))\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x, self.rope_q, self.rope_k\n",
    "            )  ## How about we add residual connection here also ?\n",
    "        x = self.norm(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.output(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n",
    "            )\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the output on the very last position\n",
    "            logits = self.output(\n",
    "                x[:, [-1], :]\n",
    "            )  # note: using list [-1] to preserve the time dim\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da35dfa2-ad63-463a-afc8-2f3ae9379ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"emebdding_dim\": 64,\n",
    "    \"max_seq_len\": 512,\n",
    "    \"embedding_dropout\": 0.0,\n",
    "    \"rms_norm_eps\": 1e-05,\n",
    "    \"rope_scaling\": 1.0,\n",
    "    \"rope_theta\": 10000.0,\n",
    "    \"attention_bias\": False,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"num_attention_heads\": 8,\n",
    "    \"num_key_value_heads\": 4,\n",
    "    \"use_cache\": True,\n",
    "    \"use_sliding_window\": True,\n",
    "    \"residual_dropout\": 0.1,\n",
    "    \"mlp_dropout\": 0.0,\n",
    "    \"mlp_hidden_size\": int(1.3 * 64),\n",
    "    \"num_layers\": 5,\n",
    "    \"device\": \"cuda\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfc3d814-7feb-4ff0-a309-b1947dac838d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.115679 Million Params Model"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ModelArgs(**conf)\n",
    "model1 = Transformer(config)\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abd50556-643d-4258-9ec6-0a9116819f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, 64, (8, 64), device=device)\n",
    "model(x)[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
