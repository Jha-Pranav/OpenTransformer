{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010208af-07b3-4a3b-82e9-d5dbe5e7f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d01c63-8a04-4e62-8988-adb7dbce9c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face datasets library\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "DATA_DIR = f\"/home/pranav-pc/projects/OpenTransformer/multiformer/data\"\n",
    "dataset_train = load_dataset(\n",
    "    path=os.path.join(DATA_DIR, \"downloads/TinyStories\"),\n",
    "    trust_remote_code=True,\n",
    "    split=\"train\",\n",
    ")\n",
    "dataset_val = load_dataset(\n",
    "    path=os.path.join(DATA_DIR, \"downloads/TinyStories\"),\n",
    "    trust_remote_code=True,\n",
    "    split=\"validation\",\n",
    ")\n",
    "\n",
    "dataset = concatenate_datasets([dataset_train, dataset_val])\n",
    "del dataset_train\n",
    "del dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6072e42b-6c03-4040-97ff-7e99647c3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenize.tokenizer import Tokenizer\n",
    "\n",
    "TOKENIZER_CHECKPOINT = (\n",
    "    \"/home/pranav-pc/projects/OpenTransformer/multiformer/tokenizer_checkpoints/\"\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(TOKENIZER_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e950ae5b-447e-47b2-8d86-7fa4357b8149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = dataset[0][\"text\"]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "656a92d0-462f-4900-bad2-6b542dacb8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(text, out_type=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b2dad36-8912-426c-965f-6d96d530c78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 29871, 13, 13, 2]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"\\n\\n\", out_type=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f86b91b4-10eb-4600-b175-12e1b31386fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3118, 2462, 29892, 263, 2217, 7826, 4257, 365, 2354, 1476, 263, 817, 280, 297, 902, 5716, 29889, 2296, 6363, 372, 471, 5189, 304, 1708, 411, 372, 1363, 372, 471, 15301, 29889, 365, 2354, 5131, 304, 6232, 278, 817, 280, 411, 902, 16823, 29892, 577, 1183, 1033, 409, 29893, 263, 2826, 373, 902, 528, 2728, 29889, 13, 13, 29931, 2354, 3512, 304, 902, 16823, 322, 1497, 29892, 376, 29924, 290, 29892, 306, 1476, 445, 817, 280, 29889, 1815, 366, 6232, 372, 411, 592, 322, 409, 29893, 590, 528, 2728, 3026, 2439, 16823, 25156, 322, 1497, 29892, 376, 8241, 29892, 365, 2354, 29892, 591, 508, 6232, 278, 817, 280, 322, 2329, 596, 528, 2728, 1213, 13, 13, 29911, 12966, 29892, 896, 7258, 278, 817, 280, 322, 409, 8734, 278, 2826, 373, 365, 2354, 29915, 29879, 528, 2728, 29889, 739, 471, 451, 5189, 363, 963, 1363, 896, 892, 19383, 322, 19912, 1269, 916, 29889, 2860, 896, 7743, 29892, 365, 2354, 6452, 287, 902, 16823, 363, 19383, 278, 817, 280, 322, 27826, 902, 528, 2728, 29889, 2688, 1716, 7091, 9796, 1363, 896, 750, 7258, 322, 3796, 4208, 29889, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(text, out_type=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a7c3c76-233a-4db1-9ccd-9748046bc821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily\n",
      "\n",
      " wanted to\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.decode_ids(\n",
    "        [\n",
    "            1,\n",
    "            3118,\n",
    "            2462,\n",
    "            29892,\n",
    "            263,\n",
    "            2217,\n",
    "            7826,\n",
    "            4257,\n",
    "            365,\n",
    "            2354,\n",
    "            1476,\n",
    "            263,\n",
    "            817,\n",
    "            280,\n",
    "            297,\n",
    "            902,\n",
    "            5716,\n",
    "            29889,\n",
    "            2296,\n",
    "            6363,\n",
    "            372,\n",
    "            471,\n",
    "            5189,\n",
    "            304,\n",
    "            1708,\n",
    "            411,\n",
    "            372,\n",
    "            1363,\n",
    "            372,\n",
    "            471,\n",
    "            15301,\n",
    "            29889,\n",
    "            365,\n",
    "            2354,\n",
    "            13,\n",
    "            13,\n",
    "            5131,\n",
    "            304,\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf15580-4d1e-4897-a967-391767aea970",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.map(\n",
    "    lambda example: {\n",
    "        \"idx\": [en[:block_size] for en in tokenizer.encode(example[\"text\"])]\n",
    "    },\n",
    "    batch_size=512,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "\n",
    "# Define collate function to handle padding\n",
    "def collate_fn(batch):\n",
    "    x_batch = [torch.tensor(en[:-1]) for en in batch]  # Extract x (remove last token)\n",
    "    y_batch = [torch.tensor(en[1:]) for en in batch]  # Extract y (remove first token)\n",
    "    x_padded = pad_sequence(\n",
    "        x_batch, batch_first=True, padding_value=tokenizer.eos_id()\n",
    "    )  # Pad x sequences\n",
    "    y_padded = pad_sequence(\n",
    "        y_batch, batch_first=True, padding_value=tokenizer.eos_id()\n",
    "    )  # Pad y sequences\n",
    "    return x_padded, y_padded\n",
    "\n",
    "\n",
    "# Sort the data and turn off shuffle - Simplest way of implementing Seq leng batch sampling\n",
    "train_data = sorted(data[\"train\"][\"idx\"], key=lambda x: len(x))\n",
    "\n",
    "# Create DataLoader with collate function\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size=batch, collate_fn=collate_fn, shuffle=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
