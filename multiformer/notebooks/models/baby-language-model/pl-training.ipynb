{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47b5ab2-2729-43c5-96f8-5597e8e2f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch._dynamo\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from src.cells.normalization import RMSLayerNorm\n",
    "from src.cells.optim_func import config_optimizer\n",
    "from src.cells.position import RotaryEmbedding\n",
    "from src.models.blm.block import Block\n",
    "from src.models.blm.config import ModelArgs\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "pl.seed_everything(123, workers=True)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backend.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960b5ea0-a6c0-4b05-8400-2c2ba1b61500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataloader(pl.LightningDataModule):\n",
    "    def __init__(self, data_path_train, data_path_val, tokenizer_path, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.data_path_train = data_path_train\n",
    "        self.data_path_val = data_path_val\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.tokenizer = self._load_tokenizer(tokenizer_path)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_path):\n",
    "        from src.tokenize.tokenizer import Tokenizer\n",
    "\n",
    "        return Tokenizer(tokenizer_path)\n",
    "\n",
    "    def _collate_fn(self, batch: int, padding_id: int):\n",
    "        batch = pad_sequence(\n",
    "            (torch.LongTensor(_[\"idx\"]) for _ in batch),\n",
    "            batch_first=True,\n",
    "            padding_value=padding_id,\n",
    "        )  # TODO : ShortTensor suffice our need but nn.Embedding don't support it. Using LOngTensor is a unnecessary waste of GPU memory\n",
    "        x_batch = torch.stack([en[:-1] for en in batch])  # Extract x (remove last token)\n",
    "        y_batch = torch.stack([en[1:] for en in batch])  # Extract y (remove first token)\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def setup(self, stage):\n",
    "\n",
    "        self.train_data = load_from_disk(self.data_path_train)\n",
    "        self.val_data = load_from_disk(self.data_path_val)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=functools.partial(self._collate_fn, padding_id=self.tokenizer.eos_id()),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=functools.partial(self._collate_fn, padding_id=self.tokenizer.eos_id()),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f5e99cc0-d131-46f3-9fb3-6ea8646925dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        self.tok_embd = nn.Embedding(\n",
    "            args.vocab_size, args.emebdding_dim, padding_idx=args.padding_idx\n",
    "        )\n",
    "        self.dropout = nn.Dropout(args.embedding_dropout)\n",
    "        self.rope_q = RotaryEmbedding(\n",
    "            args.emebdding_dim // args.num_attention_heads,\n",
    "            args.max_seq_len,\n",
    "            device=args.device,\n",
    "        )\n",
    "        self.rope_k = RotaryEmbedding(\n",
    "            args.emebdding_dim // args.num_key_value_heads,\n",
    "            args.max_seq_len,\n",
    "            device=args.device,\n",
    "        )\n",
    "\n",
    "        # Freeze the parameters rope_q and rope_k\n",
    "        self.rope_q.requires_grad_(False)\n",
    "        self.rope_k.requires_grad_(False)\n",
    "\n",
    "        self.layers = nn.ModuleList([Block(args) for lid in range(args.num_layers)])\n",
    "\n",
    "        self.norm = RMSLayerNorm(args.emebdding_dim, eps=args.rms_norm_eps)\n",
    "        self.output = nn.Linear(args.emebdding_dim, args.vocab_size, bias=False)\n",
    "\n",
    "        # share the unembedding parameters with the embedding parameters\n",
    "        self.tok_embd.weight = self.output.weight  # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"wo.weight\"):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * args.num_layers))\n",
    "        self.lr = 1e-4\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.get_num_params()} Million Params Model\"\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.tok_embd.weight.numel()\n",
    "        return n_params / 1e6  # In Million\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout(self.tok_embd(tokens))\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x, self.rope_q, self.rope_k\n",
    "            )  ## How about we add residual connection here also ?\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def _common_step(self, batch, batch_index):\n",
    "        x, targets = batch\n",
    "        logits = self.output(self.forward(x))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        if trainer.global_step == 0:\n",
    "            wandb.define_metric(\"train_loss\", summary=\"mean\")\n",
    "        self.log_dict(\n",
    "            {\"train_loss\": loss, \"lr\": self.lr}, prog_bar=True, on_step=True, on_epoch=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        self.log_dict({\"val_loss\": loss}, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": 1e-2},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        return torch.optim.AdamW(optim_groups, lr=self.lr, betas=(0.9, 0.95), fused=False)\n",
    "\n",
    "    def predict_step(\n",
    "        self,\n",
    "        batch,\n",
    "        batch_idx,\n",
    "        max_new_tokens=30,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "        conditional_break=[13, 13, 1],\n",
    "    ):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # trim the token to the max_len\n",
    "            if batch.shape[1] > self.max_seq_len:\n",
    "                batch = batch[:, -self.max_seq_len :]\n",
    "\n",
    "            # inference-time mini-optimization: only forward the output on the very last position\n",
    "            logits = self.output(\n",
    "                self(batch)[:, [-1], :]\n",
    "            )  # note: using list [-1] to preserve the time dim\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            batch = torch.cat((batch, idx_next), dim=1)\n",
    "            last_three_tokens = batch[:, -3:]\n",
    "            if torch.equal(\n",
    "                last_three_tokens, torch.LongTensor(conditional_break).to(batch.device)\n",
    "            ):\n",
    "                break\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff84ca15-01f3-4052-b1c7-b2f334782ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "892ada04-b997-4352-89d5-bc8af0977197",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"/home/pranav-pc/projects/OpenTransformer/multiformer\"\n",
    "MODEL_CHECKPOINT_PATH = BASE_URL + \"/model_checkpoints/blm/last-v3.ckpt\"\n",
    "data_path_train = BASE_URL + \"/data/interim/TinyStories_train_65>tk>512.hf\"\n",
    "data_path_val = BASE_URL + \"/data/interim/TinyStories_val_65>tk>512.hf\"\n",
    "tokenizer_path = BASE_URL + \"/tokenizer_checkpoints\"\n",
    "\n",
    "\n",
    "load_from_checkpoint = True\n",
    "train = False\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 26\n",
    "ds = TinyStoriesDataloader(data_path_train, data_path_val, tokenizer_path, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "007b2881-c39e-45bf-a402-8b16940092ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"emebdding_dim\": 768,\n",
    "    \"max_seq_len\": 512,\n",
    "    \"embedding_dropout\": 0.0,\n",
    "    \"rms_norm_eps\": 1e-05,\n",
    "    \"rope_scaling\": 1.0,\n",
    "    \"rope_theta\": 10000.0,\n",
    "    \"attention_bias\": False,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_key_value_heads\": 12,\n",
    "    \"use_cache\": True,\n",
    "    \"use_sliding_window\": True,\n",
    "    \"residual_dropout\": 0.1,\n",
    "    \"mlp_dropout\": 0.0,\n",
    "    \"mlp_hidden_size\": int(1.3 * 768),\n",
    "    \"num_layers\": 4,\n",
    "    \"device\": device,\n",
    "    \"padding_idx\": ds.tokenizer.eos_id(),\n",
    "}\n",
    "if load_from_checkpoint:\n",
    "    model = Transformer.load_from_checkpoint(MODEL_CHECKPOINT_PATH)\n",
    "    model = torch.compile(model, dynamic=True)\n",
    "else:\n",
    "    config = ModelArgs(**conf)\n",
    "    model = Transformer(config)\n",
    "    model = torch.compile(model, dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eaf39740-9e0d-4d34-8f8d-47bb9c74f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import (\n",
    "    BatchSizeFinder,\n",
    "    EarlyStopping,\n",
    "    GradientAccumulationScheduler,\n",
    "    LearningRateFinder,\n",
    "    ModelCheckpoint,\n",
    "    StochasticWeightAveraging,\n",
    ")\n",
    "\n",
    "# class DynamicBatchSizeFinder(BatchSizeFinder):\n",
    "#     def __init__(self, milestones, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.milestones = milestones\n",
    "\n",
    "#     def on_fit_start(self, *args, **kwargs):\n",
    "#         return\n",
    "\n",
    "#     def on_train_batch_start(self, trainer, pl_module,batch,batch_idx):\n",
    "#         if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "#             if batch_idx % 5 == 0:\n",
    "#                 self.scale_batch_size(trainer, pl_module)\n",
    "\n",
    "\n",
    "class FineTuneLearningRateFinder(LearningRateFinder):\n",
    "    def __init__(self, milestones, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.milestones = milestones\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "\n",
    "            self.lr_find(trainer, pl_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc0aaf79-3974-49d8-baaa-3f4dd5c18d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    name=\"blm\",\n",
    "    save_dir=\"blm/\",\n",
    "    version=\"v1\",\n",
    "    offline=False,\n",
    "    project=\"tiny-stories\",\n",
    "    log_model=\"all\",\n",
    ")\n",
    "import wandb\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c8111d93-5991-4fdf-a35e-2a2a4f247a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "accumulator = GradientAccumulationScheduler(scheduling={0: 4, 4: 3, 6: 1})\n",
    "\n",
    "# logger = pl.loggers.TensorBoardLogger(save_dir=\"./blm-log/\", name=\"blm\", version=1.0)\n",
    "# profiler = pl.profilers.PyTorchProfiler(\n",
    "#     on_trace_ready=torch.profiler.tensorboard_trace_handler('./blm-log/'),\n",
    "#     schedule=torch.profiler.schedule(skip_first=10, wait=10, warmup=1, active=2)\n",
    "# )\n",
    "# saves top-K checkpoints based on \"train_loss\" metric\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=2,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"max\",\n",
    "    dirpath=\"model_checkpoints/\",\n",
    "    filename=\"baby-llm-{epoch:02d}-{train_loss:.3f}\",\n",
    "    save_last=True,\n",
    "    every_n_train_steps=int(1e4),\n",
    "    save_on_train_epoch_end=True,\n",
    ")\n",
    "early_stop = EarlyStopping(\"train_loss\", patience=10, verbose=True)\n",
    "stochastic_weight_avg = StochasticWeightAveraging(swa_lrs=1e-6)\n",
    "# dynamic_batch_size = DynamicBatchSizeFinder(milestones=(6, 20))\n",
    "lr_finder = FineTuneLearningRateFinder(milestones=(5, 20))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    # logger=wandb_logger,\n",
    "    min_epochs=1,\n",
    "    max_epochs=100,\n",
    "    precision=\"bf16-mixed\",\n",
    "    # enable_model_summary=True,\n",
    "    # profiler=profiler,\n",
    "    callbacks=[\n",
    "        early_stop,\n",
    "        checkpoint_callback,\n",
    "        accumulator,\n",
    "        stochastic_weight_avg,\n",
    "        lr_finder,\n",
    "    ],\n",
    "    default_root_dir=\"model_checkpoints/\",\n",
    "    enable_checkpointing=True,\n",
    "    # fast_dev_run=True,\n",
    "    log_every_n_steps=5,\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_val=1.0,\n",
    "    max_steps=5000,\n",
    ")\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "if train:\n",
    "    model.train()\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        ds,\n",
    "        ckpt_path=MODEL_CHECKPOINT_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f66e4949-1ded-48c9-adf2-1dc9018e0a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123d3dcaf92944a4afbe0bd917da917a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_loss_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.2942075729370117     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_loss_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.2942075729370117    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss_epoch': 1.2942075729370117}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "trainer.validate(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e70a62b0-9409-47a1-8e0e-76071c256450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inference\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "06ecc1ab-a110-4872-8106-06ebef470130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Write a story containing the words: dive, job, sorry. Story summary: Bob the big fish finds a shiny rock while searching for food for his friends, but when he tells them about it, they are excited to play with it instead of being sad about not having food.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2306be0f-c52c-4eaa-8b09-528f1d14bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ramesh is a good boy. one day his father called and asked for the last semester exam result. He says he needs to fix the fence. Ramesh helps to scrape his leg.\n",
      "\n",
      "The careful improved packages worked and soon he was done. He examined the fence in all directions. He ran to the fence and looked over. He saw that it was broken. He got to work and fixed the fence. He was very proud of his work.\n",
      "\n",
      "After his work, the fence was fixed. Rames was gone and a happy smile came back. He waved goodbye and went back outside.\n",
      "\n",
      " Once upon a time, there was a little bunny. The bunny liked to hop around in the grass. One day, the bunny saw a big, rich house. The house had lots of cars and a shiny copper pen. The bunny wanted to see the pen up close. So, the bunny hopped over to the pen and hopped inside. The car was very big and shiny. When the bunny got inside, it was dark and she couldn't see anything. The bunny hopped into a room with lots of candy. The end.\n",
      "\n",
      " Once upon a time there was a friendly man. He was very lonely. One day, he had an idea. He wanted to make something special. So he took a piece of paper and some markers. Then he tied it together with a long string.\n",
      "\n",
      "He looked around and found lots of different things. He made a toy kayak out of the string and he was very happy. He took it to a harmless lake and he had lots of fun.\n",
      "\n",
      "The end.\n",
      "\n",
      " Once upon a time there were two friends, Jack and Jill. They loved to play together and always had a lot of fun.\n",
      "\n",
      "One day, Jack had a great idea. He said to Jill, \"Let's complete a race!\"\n",
      "\n",
      "Jill agreed and they started running. But when they were at the end, they stopped. Jack said, \"I have a joke for us.\" \n",
      "Jill laughed and laughed.\n",
      "\n",
      "Jack said,\n"
     ]
    }
   ],
   "source": [
    "# text = \"Sita wanted to watch either a movie or a cartoon. Her mother didn’t let her watch a cartoon so instead she\"\n",
    "text = (\n",
    "    \"Ramesh is a good boy. one day his father called and asked for the last semester exam result\"\n",
    ")\n",
    "tokens = torch.LongTensor(ds.tokenizer.encode(text)).to(\"cuda:0\").view(1, -1)\n",
    "# tokens\n",
    "print(\n",
    "    ds.tokenizer.decode_ids(\n",
    "        model.predict_step(tokens, None, max_new_tokens=450, temperature=0.9, top_k=None)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d282263-d3d9-4a71-b4df-c9048eaac9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
