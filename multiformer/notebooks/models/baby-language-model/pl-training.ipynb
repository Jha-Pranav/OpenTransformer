{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47b5ab2-2729-43c5-96f8-5597e8e2f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch._dynamo\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from lightning.pytorch.loggers import CSVLogger, TensorBoardLogger, WandbLogger\n",
    "from src.cells.normalization import RMSLayerNorm\n",
    "from src.cells.optim_func import config_optimizer\n",
    "from src.cells.position import RotaryEmbedding\n",
    "from src.models.blm.block import Block\n",
    "from src.models.blm.config import ModelArgs\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "pl.seed_everything(123, workers=True)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backend.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960b5ea0-a6c0-4b05-8400-2c2ba1b61500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataloader(pl.LightningDataModule):\n",
    "    def __init__(self, data_path_train, data_path_val, tokenizer_path, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.data_path_train = data_path_train\n",
    "        self.data_path_val = data_path_val\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.tokenizer = self._load_tokenizer(tokenizer_path)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_path):\n",
    "        from src.tokenize.tokenizer import Tokenizer\n",
    "\n",
    "        return Tokenizer(tokenizer_path)\n",
    "\n",
    "    def _collate_fn(self, batch: int, padding_id: int):\n",
    "        batch = pad_sequence(\n",
    "            (torch.LongTensor(_[\"idx\"]) for _ in batch),\n",
    "            batch_first=True,\n",
    "            padding_value=padding_id,\n",
    "        )  # TODO : ShortTensor suffice our need but nn.Embedding don't support it. Using LOngTensor is a unnecessary waste of GPU memory\n",
    "        x_batch = torch.stack([en[:-1] for en in batch])  # Extract x (remove last token)\n",
    "        y_batch = torch.stack([en[1:] for en in batch])  # Extract y (remove first token)\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def setup(self, stage):\n",
    "\n",
    "        self.train_data = load_from_disk(self.data_path_train)\n",
    "        self.val_data = load_from_disk(self.data_path_val)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=functools.partial(self._collate_fn, padding_id=self.tokenizer.eos_id()),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=functools.partial(self._collate_fn, padding_id=self.tokenizer.eos_id()),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5e99cc0-d131-46f3-9fb3-6ea8646925dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        self.tok_embd = nn.Embedding(\n",
    "            args.vocab_size, args.embedding_dim, padding_idx=args.padding_idx\n",
    "        )\n",
    "        self.dropout = nn.Dropout(args.embedding_dropout)\n",
    "        self.rope_q = RotaryEmbedding(\n",
    "            args.embedding_dim // args.num_attention_heads,\n",
    "            args.max_seq_len,\n",
    "            device=args.device,\n",
    "        )\n",
    "        self.rope_k = RotaryEmbedding(\n",
    "            args.embedding_dim // args.num_key_value_heads,\n",
    "            args.max_seq_len,\n",
    "            device=args.device,\n",
    "        )\n",
    "\n",
    "        # Freeze the parameters rope_q and rope_k\n",
    "        self.rope_q.requires_grad_(False)\n",
    "        self.rope_k.requires_grad_(False)\n",
    "\n",
    "        self.layers = nn.ModuleList([Block(args) for lid in range(args.num_layers)])\n",
    "\n",
    "        self.norm = RMSLayerNorm(args.embedding_dim, eps=args.rms_norm_eps)\n",
    "        self.output = nn.Linear(args.embedding_dim, args.vocab_size, bias=False)\n",
    "\n",
    "        # share the unembedding parameters with the embedding parameters\n",
    "        self.tok_embd.weight = self.output.weight  # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"wo.weight\"):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * args.num_layers))\n",
    "        self.lr = 1e-4\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.get_num_params()} Million Params Model\"\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.tok_embd.weight.numel()\n",
    "        return n_params / 1e6  # In Million\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout(self.tok_embd(tokens))\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x, self.rope_q, self.rope_k\n",
    "            )  ## How about we add residual connection here also ?\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def _common_step(self, batch, batch_index):\n",
    "        x, targets = batch\n",
    "        logits = self.output(self.forward(x))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        # if trainer.global_step == 0:\n",
    "        #     wandb.define_metric(\"train_loss\", summary=\"mean\")\n",
    "        self.log_dict(\n",
    "            {\"train_loss\": loss, \"lr\": self.lr},\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        self.log_dict({\"val_loss\": loss}, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": 1e-2},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        return torch.optim.AdamW(optim_groups, lr=self.lr, betas=(0.9, 0.95), fused=False)\n",
    "\n",
    "    def predict_step(\n",
    "        self,\n",
    "        batch,\n",
    "        batch_idx,\n",
    "        max_new_tokens=30,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "        conditional_break=[13, 13, 1],\n",
    "    ):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # trim the token to the max_len\n",
    "            if batch.shape[1] > self.max_seq_len:\n",
    "                batch = batch[:, -self.max_seq_len :]\n",
    "\n",
    "            # inference-time mini-optimization: only forward the output on the very last position\n",
    "            logits = self.output(\n",
    "                self(batch)[:, [-1], :]\n",
    "            )  # note: using list [-1] to preserve the time dim\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            batch = torch.cat((batch, idx_next), dim=1)\n",
    "            if conditional_break:\n",
    "                last_three_tokens = batch[-1][-len(conditional_break) :]\n",
    "                if torch.equal(\n",
    "                    last_three_tokens,\n",
    "                    torch.LongTensor(conditional_break).to(batch.device),\n",
    "                ):\n",
    "\n",
    "                    break\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff84ca15-01f3-4052-b1c7-b2f334782ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "892ada04-b997-4352-89d5-bc8af0977197",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"/home/pranav-pc/projects/OpenTransformer/multiformer\"\n",
    "# MODEL_CHECKPOINT_PATH = BASE_URL + \"/model_checkpoints/blm/last-v3.ckpt\"\n",
    "MODEL_CHECKPOINT_PATH = (\n",
    "    \"/home/pranav-pc/projects/OpenTransformer/multiformer/blm-1024/checkpoints/last.ckpt\"\n",
    ")\n",
    "data_path_train = BASE_URL + \"/data/interim/TinyStories_train_65>tk>1024.hf\"\n",
    "data_path_val = BASE_URL + \"/data/interim/TinyStories_val_65>tk>1024.hf\"\n",
    "tokenizer_path = BASE_URL + \"/tokenizer_checkpoints\"\n",
    "\n",
    "\n",
    "load_from_checkpoint = True\n",
    "train = False\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 26\n",
    "ds = TinyStoriesDataloader(data_path_train, data_path_val, tokenizer_path, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "007b2881-c39e-45bf-a402-8b16940092ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"embedding_dim\": 768,\n",
    "    \"max_seq_len\": 1024,\n",
    "    \"embedding_dropout\": 0.0,\n",
    "    \"rms_norm_eps\": 1e-05,\n",
    "    \"rope_scaling\": 1.0,\n",
    "    \"rope_theta\": 10000.0,\n",
    "    \"attention_bias\": False,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_key_value_heads\": 12,\n",
    "    \"use_cache\": True,\n",
    "    \"use_sliding_window\": True,\n",
    "    \"residual_dropout\": 0.1,\n",
    "    \"mlp_dropout\": 0.0,\n",
    "    \"mlp_hidden_size\": int(1.3 * 768),\n",
    "    \"num_layers\": 4,\n",
    "    \"device\": device,\n",
    "    \"padding_idx\": ds.tokenizer.eos_id(),\n",
    "}\n",
    "if load_from_checkpoint:\n",
    "    model = Transformer.load_from_checkpoint(MODEL_CHECKPOINT_PATH)\n",
    "    model = torch.compile(model, dynamic=True)\n",
    "else:\n",
    "    config = ModelArgs(**conf)\n",
    "    model = Transformer(config)\n",
    "    model = torch.compile(model, dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eaf39740-9e0d-4d34-8f8d-47bb9c74f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import (\n",
    "    BatchSizeFinder,\n",
    "    EarlyStopping,\n",
    "    GradientAccumulationScheduler,\n",
    "    LearningRateFinder,\n",
    "    ModelCheckpoint,\n",
    "    StochasticWeightAveraging,\n",
    ")\n",
    "\n",
    "# class DynamicBatchSizeFinder(BatchSizeFinder):\n",
    "#     def __init__(self, milestones, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.milestones = milestones\n",
    "\n",
    "#     def on_fit_start(self, *args, **kwargs):\n",
    "#         return\n",
    "\n",
    "#     def on_train_batch_start(self, trainer, pl_module,batch,batch_idx):\n",
    "#         if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "#             if batch_idx % 5 == 0:\n",
    "#                 self.scale_batch_size(trainer, pl_module)\n",
    "\n",
    "\n",
    "class FineTuneLearningRateFinder(LearningRateFinder):\n",
    "    def __init__(self, milestones, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.milestones = milestones\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "\n",
    "            self.lr_find(trainer, pl_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc0aaf79-3974-49d8-baaa-3f4dd5c18d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    name=\"blm\",\n",
    "    save_dir=\"blm/\",\n",
    "    version=\"v1\",\n",
    "    offline=False,\n",
    "    project=\"tiny-stories\",\n",
    "    log_model=\"all\",\n",
    ")\n",
    "import wandb\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8111d93-5991-4fdf-a35e-2a2a4f247a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "accumulator = GradientAccumulationScheduler(scheduling={0: 4, 4: 3, 6: 1})\n",
    "\n",
    "# logger = pl.loggers.TensorBoardLogger(save_dir=\"./blm-log/\", name=\"blm\", version=1.0)\n",
    "# profiler = pl.profilers.PyTorchProfiler(\n",
    "#     on_trace_ready=torch.profiler.tensorboard_trace_handler('./blm-log/'),\n",
    "#     schedule=torch.profiler.schedule(skip_first=10, wait=10, warmup=1, active=2)\n",
    "# )\n",
    "# saves top-K checkpoints based on \"train_loss\" metric\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=2,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"max\",\n",
    "    dirpath=\"model_checkpoints/\",\n",
    "    filename=\"baby-llm-{epoch:02d}-{train_loss:.3f}\",\n",
    "    save_last=True,\n",
    "    every_n_train_steps=int(1e4),\n",
    "    save_on_train_epoch_end=True,\n",
    ")\n",
    "logger = TensorBoardLogger(save_dir=\"./lightning-log/\", name=\"TinnyStories\", version=0.1)\n",
    "early_stop = EarlyStopping(\"train_loss\", patience=10, verbose=True)\n",
    "stochastic_weight_avg = StochasticWeightAveraging(swa_lrs=1e-6)\n",
    "# dynamic_batch_size = DynamicBatchSizeFinder(milestones=(6, 20))\n",
    "lr_finder = FineTuneLearningRateFinder(milestones=(5, 20))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    min_epochs=1,\n",
    "    max_epochs=100,\n",
    "    precision=\"bf16-mixed\",\n",
    "    enable_model_summary=True,\n",
    "    # profiler=\"simple\",\n",
    "    callbacks=[\n",
    "        # early_stop,\n",
    "        checkpoint_callback,\n",
    "        accumulator,\n",
    "        # stochastic_weight_avg,\n",
    "        # lr_finder,\n",
    "    ],\n",
    "    default_root_dir=\"model_checkpoints/\",\n",
    "    enable_checkpointing=True,\n",
    "    # fast_dev_run=True,\n",
    "    log_every_n_steps=5,\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_val=1.0,\n",
    "    max_steps=5000,\n",
    "    val_check_interval=400,\n",
    "    check_val_every_n_epoch=None,\n",
    ")\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "if train:\n",
    "    model.train()\n",
    "    trainer.fit(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f66e4949-1ded-48c9-adf2-1dc9018e0a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37aed94e3f19452faf7a60fd98b11267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_loss_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.209062933921814     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_loss_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.209062933921814    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss_epoch': 1.209062933921814}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "trainer.validate(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e70a62b0-9409-47a1-8e0e-76071c256450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inference\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9ed154b-e0f4-4d8a-a350-cb74cef729c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack was hungry, so he went looking for something to eat in the kitchen. He opened the cupboard and saw a big bowl of yummy cereal. He was so excited that he started jumping up and down.\n",
      "\n",
      "He grabbed the bowl and ran out of the kitchen. He was so hungry that he couldnâ€™t wait to start eating. He started eating the cereal and it was so delicious. He ate it all up and it was so yummy. \n",
      "\n",
      "After he finished, he was so full that he decided to go for a walk. He walked around the block and saw a big tree. He stopped and looked up. He was so happy to see the tree and he decided to climb it. He climbed and climbed until he reached the top. He sat down and enjoyed his delicious cereal. He was so happy and full. He had a great day.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Jack was hungry, so he went looking for\"\n",
    "tokens = torch.LongTensor(ds.tokenizer.encode(text)).to(\"cuda:0\").view(1, -1)[:, :-1]\n",
    "# print(tokens)\n",
    "print(\n",
    "    ds.tokenizer.decode_ids(\n",
    "        model.predict_step(tokens, None, max_new_tokens=1000, temperature=0.9, top_k=2)[0].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06ecc1ab-a110-4872-8106-06ebef470130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Write a story containing the words: dive, job, sorry. Story summary: Bob the big fish finds a shiny rock while searching for food for his friends, but when he tells them about it, they are excited to play with it instead of being sad about not having food.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52baff98-b9f7-459f-9006-ae7f467dd1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Once upon a time, there lived a black cat. The cat belonged to a little girl called Katie. Every day, Katie would take her cat for a walk in the park. One day, as Katie and her cat were walking around, they saw a mean looking man. He said he wanted to take the cat, to which she replied ”This cat belongs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2306be0f-c52c-4eaa-8b09-528f1d14bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Linear(in_features=768, out_features=32000, bias=False) torch.Size([1, 1, 768])\n",
      "Jack wanted to read a book,so he went to the store to buy one. He saw a big, shiny novel. He asked the shopkeeper if he could have it, and he said yes! So Jack bought it and took it home.\n",
      "\n",
      "Jack opened the novel and started to read. It was a bit difficult but Jack kept reading. He read and read until he got tired. Then he put the novel away and went to sleep.\n",
      "\n",
      "The next day, Jack woke up and he was very happy. He had read a lot of difficult words. He was so proud of himself.\n",
      "\n",
      "He went to the store again and bought a new novel. This time he read it to his friends. They all said it was a great story.\n",
      "\n",
      "Jack was very happy. He had read the novel and it had helped him become a reader. From then on, Jack read lots of books and was very happy. He was so proud of himself!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text = \"Once upon a time there was a pumpkin. It was a very special pumpkin, it could speak. It was sad because it couldn’t move. Every day, it would say\"\n",
    "# text = \"Jack was hungry, so he went looking for\"\n",
    "# text = \"Tim is a good boy. one day his father called and asked for the school exam result\"\n",
    "# text = \"Jack wanted to read a book,so he went to\"\n",
    "# text = \"My name is Mariama, my favorite\"\n",
    "tokens = torch.LongTensor(ds.tokenizer.encode(text)).to(\"cuda:0\").view(1, -1)[:, :-1]\n",
    "# print(tokens)\n",
    "print(\n",
    "    ds.tokenizer.decode_ids(\n",
    "        model.predict_step(tokens, None, max_new_tokens=1000, temperature=0.9, top_k=2)[0].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389503f5-ef9f-4b70-a6bd-b77eaf7dc3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
