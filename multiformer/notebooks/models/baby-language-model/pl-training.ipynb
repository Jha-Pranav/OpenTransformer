{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b47b5ab2-2729-43c5-96f8-5597e8e2f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch._dynamo\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from src.cells.normalization import RMSLayerNorm\n",
    "from src.cells.optim_func import config_optimizer\n",
    "from src.cells.position import RotaryEmbedding\n",
    "from src.models.blm.block import Block\n",
    "from src.models.blm.config import ModelArgs\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger, CSVLogger\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "pl.seed_everything(123, workers=True)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backend.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "960b5ea0-a6c0-4b05-8400-2c2ba1b61500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataloader(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, data_path_train, data_path_val, tokenizer_path, batch_size, num_workers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_path_train = data_path_train\n",
    "        self.data_path_val = data_path_val\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.tokenizer = self._load_tokenizer(tokenizer_path)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_path):\n",
    "        from src.tokenize.tokenizer import Tokenizer\n",
    "\n",
    "        return Tokenizer(tokenizer_path)\n",
    "\n",
    "    def _collate_fn(self, batch: int, padding_id: int):\n",
    "        batch = pad_sequence(\n",
    "            (torch.LongTensor(_[\"idx\"]) for _ in batch),\n",
    "            batch_first=True,\n",
    "            padding_value=padding_id,\n",
    "        )  # TODO : ShortTensor suffice our need but nn.Embedding don't support it. Using LOngTensor is a unnecessary waste of GPU memory\n",
    "        x_batch = torch.stack(\n",
    "            [en[:-1] for en in batch]\n",
    "        )  # Extract x (remove last token)\n",
    "        y_batch = torch.stack(\n",
    "            [en[1:] for en in batch]\n",
    "        )  # Extract y (remove first token)\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def setup(self, stage):\n",
    "\n",
    "        self.train_data = load_from_disk(self.data_path_train)\n",
    "        self.val_data = load_from_disk(self.data_path_val)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=functools.partial(\n",
    "                self._collate_fn, padding_id=self.tokenizer.eos_id()\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=functools.partial(\n",
    "                self._collate_fn, padding_id=self.tokenizer.eos_id()\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e99cc0-d131-46f3-9fb3-6ea8646925dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        self.tok_embd = nn.Embedding(\n",
    "            args.vocab_size, args.embedding_dim, padding_idx=args.padding_idx\n",
    "        )\n",
    "        self.dropout = nn.Dropout(args.embedding_dropout)\n",
    "        self.rope_q = RotaryEmbedding(\n",
    "            args.embedding_dim // args.num_attention_heads,\n",
    "            args.max_seq_len,\n",
    "            device=args.device,\n",
    "        )\n",
    "        self.rope_k = RotaryEmbedding(\n",
    "            args.embedding_dim // args.num_key_value_heads,\n",
    "            args.max_seq_len,\n",
    "            device=args.device,\n",
    "        )\n",
    "\n",
    "        # Freeze the parameters rope_q and rope_k\n",
    "        self.rope_q.requires_grad_(False)\n",
    "        self.rope_k.requires_grad_(False)\n",
    "\n",
    "        self.layers = nn.ModuleList([Block(args) for lid in range(args.num_layers)])\n",
    "\n",
    "        self.norm = RMSLayerNorm(args.embedding_dim, eps=args.rms_norm_eps)\n",
    "        self.output = nn.Linear(args.embedding_dim, args.vocab_size, bias=False)\n",
    "\n",
    "        # share the unembedding parameters with the embedding parameters\n",
    "        self.tok_embd.weight = (\n",
    "            self.output.weight\n",
    "        )  # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"wo.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    p, mean=0.0, std=0.02 / math.sqrt(2 * args.num_layers)\n",
    "                )\n",
    "        self.lr = 1e-4\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.get_num_params()} Million Params Model\"\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.tok_embd.weight.numel()\n",
    "        return n_params / 1e6  # In Million\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout(self.tok_embd(tokens))\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x, self.rope_q, self.rope_k\n",
    "            )  ## How about we add residual connection here also ?\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def _common_step(self, batch, batch_index):\n",
    "        x, targets = batch\n",
    "        logits = self.output(self.forward(x))\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        if trainer.global_step == 0:\n",
    "            wandb.define_metric(\"train_loss\", summary=\"mean\")\n",
    "        self.log_dict(\n",
    "            {\"train_loss\": loss, \"lr\": self.lr},\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        self.log_dict({\"val_loss\": loss}, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": 1e-2},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        return torch.optim.AdamW(\n",
    "            optim_groups, lr=self.lr, betas=(0.9, 0.95), fused=False\n",
    "        )\n",
    "\n",
    "    def predict_step(\n",
    "        self,\n",
    "        batch,\n",
    "        batch_idx,\n",
    "        max_new_tokens=30,\n",
    "        temperature=1.0,\n",
    "        top_k=None,\n",
    "        conditional_break=[13, 13, 1],\n",
    "    ):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # trim the token to the max_len\n",
    "            if batch.shape[1] > self.max_seq_len:\n",
    "                batch = batch[:, -self.max_seq_len :]\n",
    "\n",
    "            # inference-time mini-optimization: only forward the output on the very last position\n",
    "            logits = self.output(\n",
    "                self(batch)[:, [-1], :]\n",
    "            )  # note: using list [-1] to preserve the time dim\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            batch = torch.cat((batch, idx_next), dim=1)\n",
    "            if conditional_break:\n",
    "                last_three_tokens = batch[-1][-len(conditional_break) :]\n",
    "                if torch.equal(\n",
    "                    last_three_tokens,\n",
    "                    torch.LongTensor(conditional_break).to(batch.device),\n",
    "                ):\n",
    "\n",
    "                    break\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff84ca15-01f3-4052-b1c7-b2f334782ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "892ada04-b997-4352-89d5-bc8af0977197",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"/home/pranav-pc/projects/OpenTransformer/multiformer\"\n",
    "MODEL_CHECKPOINT_PATH = BASE_URL + \"/model_checkpoints/blm/last-v3.ckpt\"\n",
    "data_path_train = BASE_URL + \"/data/interim/TinyStories_train_65>tk>512.hf\"\n",
    "data_path_val = BASE_URL + \"/data/interim/TinyStories_val_65>tk>512.hf\"\n",
    "tokenizer_path = BASE_URL + \"/tokenizer_checkpoints\"\n",
    "\n",
    "\n",
    "load_from_checkpoint = False\n",
    "train = True\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 26\n",
    "ds = TinyStoriesDataloader(\n",
    "    data_path_train, data_path_val, tokenizer_path, batch_size, num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "007b2881-c39e-45bf-a402-8b16940092ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"embedding_dim\": 768,\n",
    "    \"max_seq_len\": 512,\n",
    "    \"embedding_dropout\": 0.0,\n",
    "    \"rms_norm_eps\": 1e-05,\n",
    "    \"rope_scaling\": 1.0,\n",
    "    \"rope_theta\": 10000.0,\n",
    "    \"attention_bias\": False,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_key_value_heads\": 12,\n",
    "    \"use_cache\": True,\n",
    "    \"use_sliding_window\": True,\n",
    "    \"residual_dropout\": 0.1,\n",
    "    \"mlp_dropout\": 0.0,\n",
    "    \"mlp_hidden_size\": int(1.3 * 768),\n",
    "    \"num_layers\": 4,\n",
    "    \"device\": device,\n",
    "    \"padding_idx\": ds.tokenizer.eos_id(),\n",
    "}\n",
    "if load_from_checkpoint:\n",
    "    model = Transformer.load_from_checkpoint(MODEL_CHECKPOINT_PATH)\n",
    "    model = torch.compile(model, dynamic=True)\n",
    "else:\n",
    "    config = ModelArgs(**conf)\n",
    "    model = Transformer(config)\n",
    "    model = torch.compile(model, dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaf39740-9e0d-4d34-8f8d-47bb9c74f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import (\n",
    "    BatchSizeFinder,\n",
    "    EarlyStopping,\n",
    "    GradientAccumulationScheduler,\n",
    "    LearningRateFinder,\n",
    "    ModelCheckpoint,\n",
    "    StochasticWeightAveraging,\n",
    ")\n",
    "\n",
    "# class DynamicBatchSizeFinder(BatchSizeFinder):\n",
    "#     def __init__(self, milestones, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.milestones = milestones\n",
    "\n",
    "#     def on_fit_start(self, *args, **kwargs):\n",
    "#         return\n",
    "\n",
    "#     def on_train_batch_start(self, trainer, pl_module,batch,batch_idx):\n",
    "#         if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "#             if batch_idx % 5 == 0:\n",
    "#                 self.scale_batch_size(trainer, pl_module)\n",
    "\n",
    "\n",
    "class FineTuneLearningRateFinder(LearningRateFinder):\n",
    "    def __init__(self, milestones, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.milestones = milestones\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "\n",
    "            self.lr_find(trainer, pl_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc0aaf79-3974-49d8-baaa-3f4dd5c18d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    name=\"blm\",\n",
    "    save_dir=\"blm/\",\n",
    "    version=\"v1\",\n",
    "    offline=False,\n",
    "    project=\"tiny-stories\",\n",
    "    log_model=\"all\",\n",
    ")\n",
    "import wandb\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8111d93-5991-4fdf-a35e-2a2a4f247a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/pranav-pc/.env/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /home/pranav-pc/projects/OpenTransformer/multiformer/notebooks/models/baby-language-model/model_checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type            | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | tok_embd | Embedding       | 24.6 M | train\n",
      "1 | dropout  | Dropout         | 0      | train\n",
      "2 | rope_q   | RotaryEmbedding | 0      | train\n",
      "3 | rope_k   | RotaryEmbedding | 0      | train\n",
      "4 | layers   | ModuleList      | 18.6 M | train\n",
      "5 | norm     | RMSLayerNorm    | 768    | train\n",
      "6 | output   | Linear          | 24.6 M | train\n",
      "-----------------------------------------------------\n",
      "43.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "43.2 M    Total params\n",
      "172.858   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_inductor/lowering.py:1611: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n",
      "[2024-04-18 01:04:34,265] [15/0_1] torch._guards: [ERROR] Error while creating guard:\n",
      "[2024-04-18 01:04:34,265] [15/0_1] torch._guards: [ERROR] Name: \"L['self']\"\n",
      "[2024-04-18 01:04:34,265] [15/0_1] torch._guards: [ERROR]     Source: local\n",
      "[2024-04-18 01:04:34,265] [15/0_1] torch._guards: [ERROR]     Create Function: NN_MODULE\n",
      "[2024-04-18 01:04:34,265] [15/0_1] torch._guards: [ERROR]     Guard Types: ['ID_MATCH']\n",
      "[2024-04-18 01:04:34,265] [15/0_1] torch._guards: [ERROR]     Code List: [\"___check_obj_id(L['self'], 140097472225296)\"]\n",
      "[2024-04-18 01:04:34,265] [15/0_1] torch._guards: [ERROR]     Object Weakref: <weakref at 0x7f6b3a32fd80; to '_ResultMetric' at 0x7f6afc10e010>\n",
      "[2024-04-18 01:04:34,265] [15/0_1] torch._guards: [ERROR]     Guarded Class Weakref: <weakref at 0x7f6b4de2c6d0; to 'ABCMeta' at 0x976acd0 (_ResultMetric)>\n",
      "[2024-04-18 01:04:34,266] [15/0_1] torch._guards: [ERROR] Created at:\n",
      "[2024-04-18 01:04:34,266] [15/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 245, in __call__\n",
      "[2024-04-18 01:04:34,266] [15/0_1] torch._guards: [ERROR]     vt = self._wrap(value).clone(**self.options())\n",
      "[2024-04-18 01:04:34,266] [15/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 469, in _wrap\n",
      "[2024-04-18 01:04:34,266] [15/0_1] torch._guards: [ERROR]     return self.wrap_module(value)\n",
      "[2024-04-18 01:04:34,266] [15/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 937, in wrap_module\n",
      "[2024-04-18 01:04:34,266] [15/0_1] torch._guards: [ERROR]     return self.tx.output.register_attr_or_module(\n",
      "[2024-04-18 01:04:34,266] [15/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 736, in register_attr_or_module\n",
      "[2024-04-18 01:04:34,266] [15/0_1] torch._guards: [ERROR]     install_guard(source.make_guard(GuardBuilder.NN_MODULE))\n",
      "[2024-04-18 01:04:34,273] [16/0_1] torch._guards: [ERROR] Error while creating guard:\n",
      "[2024-04-18 01:04:34,273] [16/0_1] torch._guards: [ERROR] Name: \"L['self']\"\n",
      "[2024-04-18 01:04:34,273] [16/0_1] torch._guards: [ERROR]     Source: local\n",
      "[2024-04-18 01:04:34,273] [16/0_1] torch._guards: [ERROR]     Create Function: NN_MODULE\n",
      "[2024-04-18 01:04:34,273] [16/0_1] torch._guards: [ERROR]     Guard Types: ['ID_MATCH']\n",
      "[2024-04-18 01:04:34,273] [16/0_1] torch._guards: [ERROR]     Code List: [\"___check_obj_id(L['self'], 140097472225296)\"]\n",
      "[2024-04-18 01:04:34,273] [16/0_1] torch._guards: [ERROR]     Object Weakref: <weakref at 0x7f6b3a32fd80; to '_ResultMetric' at 0x7f6afc10e010>\n",
      "[2024-04-18 01:04:34,273] [16/0_1] torch._guards: [ERROR]     Guarded Class Weakref: <weakref at 0x7f6b4de2c6d0; to 'ABCMeta' at 0x976acd0 (_ResultMetric)>\n",
      "[2024-04-18 01:04:34,274] [16/0_1] torch._guards: [ERROR] Created at:\n",
      "[2024-04-18 01:04:34,274] [16/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 245, in __call__\n",
      "[2024-04-18 01:04:34,274] [16/0_1] torch._guards: [ERROR]     vt = self._wrap(value).clone(**self.options())\n",
      "[2024-04-18 01:04:34,274] [16/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 469, in _wrap\n",
      "[2024-04-18 01:04:34,274] [16/0_1] torch._guards: [ERROR]     return self.wrap_module(value)\n",
      "[2024-04-18 01:04:34,274] [16/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 937, in wrap_module\n",
      "[2024-04-18 01:04:34,274] [16/0_1] torch._guards: [ERROR]     return self.tx.output.register_attr_or_module(\n",
      "[2024-04-18 01:04:34,274] [16/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 736, in register_attr_or_module\n",
      "[2024-04-18 01:04:34,274] [16/0_1] torch._guards: [ERROR]     install_guard(source.make_guard(GuardBuilder.NN_MODULE))\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] WON'T CONVERT resume_in_log /home/pranav-pc/.env/lib/python3.11/site-packages/lightning/pytorch/core/module.py line 429 \n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] due to: \n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] Traceback (most recent call last):\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 727, in _convert_frame\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     result = inner_convert(frame, cache_entry, hooks, frame_state)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 383, in _convert_frame_assert\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     compiled_product = _compile(\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]                        ^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 665, in _compile\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     raise InternalTorchDynamoError(str(e)).with_traceback(\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 646, in _compile\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 562, in compile_inner\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     out_code = transform_code_object(code, transform)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1033, in transform_code_object\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     transformations(instructions, code_options)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 151, in _fn\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return fn(*args, **kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 527, in transform\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     tracer.run()\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2128, in run\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     super().run()\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     and self.step()\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     getattr(self, inst.opname)(inst)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 470, in wrapper\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return inner_fn(self, inst)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1802, in CALL\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     self.call_function(fn, args, kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 652, in call_function\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     self.push(fn.call_function(self, args, kwargs))\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 294, in call_function\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return super().call_function(tx, args, kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 248, in call_function\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return super().call_function(tx, args, kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 81, in call_function\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return tx.inline_user_function_return(\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 688, in inline_user_function_return\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2261, in inline_call\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return cls.inline_call_(parent, func, args, kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2376, in inline_call_\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     tracer.run()\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     and self.step()\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     getattr(self, inst.opname)(inst)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1201, in COMPARE_OP\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     BuiltinVariable(supported_any[op]).call_function(\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py\", line 651, in call_function\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     result = handler(tx, *args, **kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py\", line 1592, in _comparison\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return ConstantVariable.create(op(left.value, right.value))\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"<string>\", line 4, in __eq__\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.InternalTorchDynamoError: '_Metadata' object has no attribute 'fx'\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] \n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] from user code:\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]    File \"/home/pranav-pc/.env/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 516, in resume_in_log\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     results.log(\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py\", line 409, in log\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     elif meta != self[key].meta:\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] \n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] \n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] Traceback (most recent call last):\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 727, in _convert_frame\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     result = inner_convert(frame, cache_entry, hooks, frame_state)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 383, in _convert_frame_assert\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     compiled_product = _compile(\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]                        ^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 665, in _compile\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     raise InternalTorchDynamoError(str(e)).with_traceback(\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 646, in _compile\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     r = func(*args, **kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 562, in compile_inner\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     out_code = transform_code_object(code, transform)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1033, in transform_code_object\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     transformations(instructions, code_options)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 151, in _fn\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return fn(*args, **kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 527, in transform\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     tracer.run()\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2128, in run\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     super().run()\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     and self.step()\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     getattr(self, inst.opname)(inst)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 470, in wrapper\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return inner_fn(self, inst)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1802, in CALL\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     self.call_function(fn, args, kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 652, in call_function\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     self.push(fn.call_function(self, args, kwargs))\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 294, in call_function\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return super().call_function(tx, args, kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 248, in call_function\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return super().call_function(tx, args, kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 81, in call_function\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return tx.inline_user_function_return(\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 688, in inline_user_function_return\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2261, in inline_call\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return cls.inline_call_(parent, func, args, kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2376, in inline_call_\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     tracer.run()\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     and self.step()\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]         ^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     getattr(self, inst.opname)(inst)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1201, in COMPARE_OP\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     BuiltinVariable(supported_any[op]).call_function(\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py\", line 651, in call_function\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     result = handler(tx, *args, **kwargs)\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builtin.py\", line 1592, in _comparison\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     return ConstantVariable.create(op(left.value, right.value))\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"<string>\", line 4, in __eq__\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.InternalTorchDynamoError: '_Metadata' object has no attribute 'fx'\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] \n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] from user code:\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]    File \"/home/pranav-pc/.env/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 516, in resume_in_log\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     results.log(\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py\", line 409, in log\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING]     elif meta != self[key].meta:\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] \n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "[2024-04-18 01:04:35,234] torch._dynamo.convert_frame: [WARNING] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002090e4192b498eae73597ab3482acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accumulator = GradientAccumulationScheduler(scheduling={0: 4, 4: 3, 6: 1})\n",
    "\n",
    "# logger = pl.loggers.TensorBoardLogger(save_dir=\"./blm-log/\", name=\"blm\", version=1.0)\n",
    "# profiler = pl.profilers.PyTorchProfiler(\n",
    "#     on_trace_ready=torch.profiler.tensorboard_trace_handler('./blm-log/'),\n",
    "#     schedule=torch.profiler.schedule(skip_first=10, wait=10, warmup=1, active=2)\n",
    "# )\n",
    "# saves top-K checkpoints based on \"train_loss\" metric\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=2,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"max\",\n",
    "    dirpath=\"model_checkpoints/\",\n",
    "    filename=\"baby-llm-{epoch:02d}-{train_loss:.3f}\",\n",
    "    save_last=True,\n",
    "    every_n_train_steps=int(1e4),\n",
    "    save_on_train_epoch_end=True,\n",
    ")\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"./lightning-log/\", name=\"TinnyStories\", version=0.1\n",
    ")\n",
    "early_stop = EarlyStopping(\"train_loss\", patience=10, verbose=True)\n",
    "stochastic_weight_avg = StochasticWeightAveraging(swa_lrs=1e-6)\n",
    "# dynamic_batch_size = DynamicBatchSizeFinder(milestones=(6, 20))\n",
    "lr_finder = FineTuneLearningRateFinder(milestones=(5, 20))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    min_epochs=1,\n",
    "    max_epochs=100,\n",
    "    precision=\"bf16-mixed\",\n",
    "    enable_model_summary=True,\n",
    "    profiler=\"simple\",\n",
    "    callbacks=[\n",
    "        # early_stop,\n",
    "        checkpoint_callback,\n",
    "        accumulator,\n",
    "        # stochastic_weight_avg,\n",
    "        # lr_finder,\n",
    "    ],\n",
    "    default_root_dir=\"model_checkpoints/\",\n",
    "    enable_checkpointing=True,\n",
    "    # fast_dev_run=True,\n",
    "    log_every_n_steps=5,\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_val=1.0,\n",
    "    max_steps=5000,\n",
    "    val_check_interval=400,\n",
    "    check_val_every_n_epoch=None,\n",
    ")\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "if train:\n",
    "    model.train()\n",
    "    trainer.fit(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "199eb2ea-79b2-400c-869b-52aba6eeba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_sampler(dataset, subset_ratio=0.02):\n",
    "    total_data = len(dataset)\n",
    "    subset_size = int(total_data * subset_ratio)\n",
    "    print(subset_size)\n",
    "    indices = list(range(subset_size))\n",
    "    return torch.utils.data.Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8be36859-48fc-4dc8-ba46-f2159de8a02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25245"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_subset_sampler(load_from_disk(data_path_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "781579ed-e643-4c4a-b491-7cdcf569f975",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'TinyStoriesDataloader' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'TinyStoriesDataloader' has no len()"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e4949-1ded-48c9-adf2-1dc9018e0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "trainer.validate(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e70a62b0-9409-47a1-8e0e-76071c256450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inference\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06ecc1ab-a110-4872-8106-06ebef470130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Write a story containing the words: dive, job, sorry. Story summary: Bob the big fish finds a shiny rock while searching for food for his friends, but when he tells them about it, they are excited to play with it instead of being sad about not having food.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2306be0f-c52c-4eaa-8b09-528f1d14bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tim is a good boy. one day his father called and asked for the school exam resultend. Tim was excited and said yes. He looked at the exam paper and read the words. It said: \"Very well done, Tim, you have got an answer from your father. Come on, let us check the answers together.\"\n",
      "\n",
      "When the exam was ready, Tim stepped into the classroom and tried on the paper. It was very hard but he did it! He felt so proud of himself. His father was very patient and hugged him.\n",
      "\n",
      "At the end of the exam, all of the answers Tim worked together. He was so happy to have the result. He smiled and thanked his father, who was very proud and patient. He was glad that he had finished the exam.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# text = \"Sita wanted to watch either a movie or a cartoon. Her mother didn’t let her watch a cartoon so instead she\"\n",
    "text = (\n",
    "    \"Tim is a good boy. one day his father called and asked for the school exam result\"\n",
    ")\n",
    "tokens = torch.LongTensor(ds.tokenizer.encode(text)).to(\"cuda:0\").view(1, -1)\n",
    "# tokens\n",
    "print(\n",
    "    ds.tokenizer.decode_ids(\n",
    "        model.predict_step(\n",
    "            tokens, None, max_new_tokens=450, temperature=0.9, top_k=None\n",
    "        )[0].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d282263-d3d9-4a71-b4df-c9048eaac9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearWarmupCosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a84421-381a-41f5-b535-28d1893ed2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
