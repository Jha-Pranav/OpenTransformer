{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47b5ab2-2729-43c5-96f8-5597e8e2f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from src.models.blm.config import ModelArgs\n",
    "from src.models.blm.block import Block\n",
    "\n",
    "from src.cells.normalization import RMSLayerNorm\n",
    "from typing import Optional\n",
    "from src.cells.position import RotaryEmbedding\n",
    "from src.cells.optim_func import config_optimizer\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import functools\n",
    "\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960b5ea0-a6c0-4b05-8400-2c2ba1b61500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataloader(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, data_path_train, data_path_val, tokenizer_path, batch_size, num_workers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_path_train = data_path_train\n",
    "        self.data_path_val = data_path_val\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.tokenizer = self._load_tokenizer(tokenizer_path)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_path):\n",
    "        from src.tokenize.tokenizer import Tokenizer\n",
    "\n",
    "        return Tokenizer(tokenizer_path)\n",
    "\n",
    "    def _collate_fn(self, batch: int, padding_id: int):\n",
    "        batch = pad_sequence(\n",
    "            (torch.LongTensor(_[\"idx\"]) for _ in batch),\n",
    "            batch_first=True,\n",
    "            padding_value=padding_id,\n",
    "        )  # TODO : ShortTensor suffice our need but nn.Embedding don't support it. Using LOngTensor is a unnecessary waste of GPU memory\n",
    "        x_batch = torch.stack(\n",
    "            [en[:-1] for en in batch]\n",
    "        )  # Extract x (remove last token)\n",
    "        y_batch = torch.stack(\n",
    "            [en[1:] for en in batch]\n",
    "        )  # Extract y (remove first token)\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def setup(self, stage):\n",
    "\n",
    "        self.train_data = load_from_disk(self.data_path_train)\n",
    "        self.val_data = load_from_disk(self.data_path_val)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=functools.partial(\n",
    "                self._collate_fn, padding_id=self.tokenizer.eos_id()\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=functools.partial(\n",
    "                self._collate_fn, padding_id=self.tokenizer.eos_id()\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e99cc0-d131-46f3-9fb3-6ea8646925dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        self.tok_embd = nn.Embedding(\n",
    "            args.vocab_size, args.emebdding_dim, padding_idx=args.padding_idx\n",
    "        )\n",
    "        self.dropout = nn.Dropout(args.embedding_dropout)\n",
    "        self.rope_q = RotaryEmbedding(\n",
    "            args.emebdding_dim // args.num_attention_heads,\n",
    "            args.max_seq_len,\n",
    "            device=args.device,\n",
    "        )\n",
    "        self.rope_k = RotaryEmbedding(\n",
    "            args.emebdding_dim // args.num_key_value_heads,\n",
    "            args.max_seq_len,\n",
    "            device=args.device,\n",
    "        )\n",
    "\n",
    "        # Freeze the parameters rope_q and rope_k\n",
    "        self.rope_q.requires_grad_(False)\n",
    "        self.rope_k.requires_grad_(False)\n",
    "\n",
    "        self.layers = nn.ModuleList([Block(args) for lid in range(args.num_layers)])\n",
    "\n",
    "        self.norm = RMSLayerNorm(args.emebdding_dim, eps=args.rms_norm_eps)\n",
    "        self.output = nn.Linear(args.emebdding_dim, args.vocab_size, bias=False)\n",
    "\n",
    "        # share the unembedding parameters with the embedding parameters\n",
    "        self.tok_embd.weight = (\n",
    "            self.output.weight\n",
    "        )  # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"wo.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    p, mean=0.0, std=0.02 / math.sqrt(2 * args.num_layers)\n",
    "                )\n",
    "        self.lr = 1e-4\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"{self.get_num_params()} Million Params Model\"\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.tok_embd.weight.numel()\n",
    "        return n_params / 1e6  # In Million\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(\n",
    "        self, tokens: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.dropout(self.tok_embd(tokens))\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x, self.rope_q, self.rope_k\n",
    "            )  ## How about we add residual connection here also ?\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def _common_step(self,batch,batch_index):\n",
    "        x, targets = batch\n",
    "        logits = self.output(self.forward(x))\n",
    "        loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n",
    "            )\n",
    "        return loss\n",
    "        \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        \n",
    "        loss = self._common_step(batch,batch_idx)\n",
    "        if batch_idx % int(1e4) == 0:\n",
    "            self.log_dict({'train_loss':loss},prog_bar=True,on_step=False,on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        \n",
    "        loss = self._common_step(batch,batch_idx)\n",
    "        if batch_idx % int(1e4) == 0:\n",
    "            self.log_dict({'val_loss':loss},prog_bar=True,on_step=False,on_epoch=True)\n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": 1e-2},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        return torch.optim.AdamW(optim_groups,lr=self.lr,betas=(0.9, 0.95),fused=False)\n",
    "\n",
    "        \n",
    "    def predict_step(self,batch,batch_idx,max_new_tokens=30,temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # trim the token to the max_len \n",
    "            if batch.shape[1] > self.max_seq_len:\n",
    "                batch = batch[:,-self.max_seq_len:]\n",
    "            # inference-time mini-optimization: only forward the output on the very last position\n",
    "            logits = self.output(\n",
    "                    self(batch)[:, [-1], :]\n",
    "                )  # note: using list [-1] to preserve the time dim\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            batch = torch.cat((idx, idx_next), dim=1)\n",
    "            return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "892ada04-b997-4352-89d5-bc8af0977197",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"/home/pranav-pc/projects/OpenTransformer/multiformer\"\n",
    "data_path_train = BASE_URL + \"/data/interim/TinyStories_train_65>tk>512.hf\"\n",
    "data_path_val = BASE_URL + \"/data/interim/TinyStories_val_65>tk>512.hf\"\n",
    "tokenizer_path = BASE_URL + \"/tokenizer_checkpoints\"\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 26\n",
    "ds = TinyStoriesDataloader(\n",
    "    data_path_train, data_path_val, tokenizer_path, batch_size, num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "007b2881-c39e-45bf-a402-8b16940092ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"emebdding_dim\": 768,\n",
    "    \"max_seq_len\": 512,\n",
    "    \"embedding_dropout\": 0.0,\n",
    "    \"rms_norm_eps\": 1e-05,\n",
    "    \"rope_scaling\": 1.0,\n",
    "    \"rope_theta\": 10000.0,\n",
    "    \"attention_bias\": False,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_key_value_heads\": 12,\n",
    "    \"use_cache\": True,\n",
    "    \"use_sliding_window\": True,\n",
    "    \"residual_dropout\": 0.1,\n",
    "    \"mlp_dropout\": 0.0,\n",
    "    \"mlp_hidden_size\": int(1.3 * 768),\n",
    "    \"num_layers\": 4,\n",
    "    \"device\": (\"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.backend.mps.is_available() else \"cpu\"),\n",
    "    \"padding_idx\": ds.tokenizer.eos_id(),\n",
    "}\n",
    "\n",
    "config = ModelArgs(**conf)\n",
    "model = Transformer(config)\n",
    "model = torch.compile(model,dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31fbcb-f263-4bf0-a026-3d7203c7a839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | tok_embd | Embedding       | 24.6 M\n",
      "1 | dropout  | Dropout         | 0     \n",
      "2 | rope_q   | RotaryEmbedding | 0     \n",
      "3 | rope_k   | RotaryEmbedding | 0     \n",
      "4 | layers   | ModuleList      | 15.6 M\n",
      "5 | norm     | RMSLayerNorm    | 768   \n",
      "6 | output   | Linear          | 24.6 M\n",
      "---------------------------------------------\n",
      "40.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "40.2 M    Total params\n",
      "160.623   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_inductor/lowering.py:1611: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n",
      "[2024-04-10 18:08:56,724] [15/0_1] torch._guards: [ERROR] Error while creating guard:\n",
      "[2024-04-10 18:08:56,724] [15/0_1] torch._guards: [ERROR] Name: \"L['self']\"\n",
      "[2024-04-10 18:08:56,724] [15/0_1] torch._guards: [ERROR]     Source: local\n",
      "[2024-04-10 18:08:56,724] [15/0_1] torch._guards: [ERROR]     Create Function: NN_MODULE\n",
      "[2024-04-10 18:08:56,724] [15/0_1] torch._guards: [ERROR]     Guard Types: ['ID_MATCH']\n",
      "[2024-04-10 18:08:56,724] [15/0_1] torch._guards: [ERROR]     Code List: [\"___check_obj_id(L['self'], 132062907205584)\"]\n",
      "[2024-04-10 18:08:56,724] [15/0_1] torch._guards: [ERROR]     Object Weakref: <weakref at 0x781c4a3a7bf0; to '_ResultMetric' at 0x781c4aac07d0>\n",
      "[2024-04-10 18:08:56,724] [15/0_1] torch._guards: [ERROR]     Guarded Class Weakref: <weakref at 0x781c910cc400; to 'ABCMeta' at 0xa7e01b0 (_ResultMetric)>\n",
      "[2024-04-10 18:08:56,726] [15/0_1] torch._guards: [ERROR] Created at:\n",
      "[2024-04-10 18:08:56,726] [15/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 245, in __call__\n",
      "[2024-04-10 18:08:56,726] [15/0_1] torch._guards: [ERROR]     vt = self._wrap(value).clone(**self.options())\n",
      "[2024-04-10 18:08:56,726] [15/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 469, in _wrap\n",
      "[2024-04-10 18:08:56,726] [15/0_1] torch._guards: [ERROR]     return self.wrap_module(value)\n",
      "[2024-04-10 18:08:56,726] [15/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 937, in wrap_module\n",
      "[2024-04-10 18:08:56,726] [15/0_1] torch._guards: [ERROR]     return self.tx.output.register_attr_or_module(\n",
      "[2024-04-10 18:08:56,726] [15/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 736, in register_attr_or_module\n",
      "[2024-04-10 18:08:56,726] [15/0_1] torch._guards: [ERROR]     install_guard(source.make_guard(GuardBuilder.NN_MODULE))\n",
      "[2024-04-10 18:08:56,734] [16/0_1] torch._guards: [ERROR] Error while creating guard:\n",
      "[2024-04-10 18:08:56,734] [16/0_1] torch._guards: [ERROR] Name: \"L['self']\"\n",
      "[2024-04-10 18:08:56,734] [16/0_1] torch._guards: [ERROR]     Source: local\n",
      "[2024-04-10 18:08:56,734] [16/0_1] torch._guards: [ERROR]     Create Function: NN_MODULE\n",
      "[2024-04-10 18:08:56,734] [16/0_1] torch._guards: [ERROR]     Guard Types: ['ID_MATCH']\n",
      "[2024-04-10 18:08:56,734] [16/0_1] torch._guards: [ERROR]     Code List: [\"___check_obj_id(L['self'], 132062907205584)\"]\n",
      "[2024-04-10 18:08:56,734] [16/0_1] torch._guards: [ERROR]     Object Weakref: <weakref at 0x781c4a3a7bf0; to '_ResultMetric' at 0x781c4aac07d0>\n",
      "[2024-04-10 18:08:56,734] [16/0_1] torch._guards: [ERROR]     Guarded Class Weakref: <weakref at 0x781c910cc400; to 'ABCMeta' at 0xa7e01b0 (_ResultMetric)>\n",
      "[2024-04-10 18:08:56,735] [16/0_1] torch._guards: [ERROR] Created at:\n",
      "[2024-04-10 18:08:56,735] [16/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 245, in __call__\n",
      "[2024-04-10 18:08:56,735] [16/0_1] torch._guards: [ERROR]     vt = self._wrap(value).clone(**self.options())\n",
      "[2024-04-10 18:08:56,735] [16/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 469, in _wrap\n",
      "[2024-04-10 18:08:56,735] [16/0_1] torch._guards: [ERROR]     return self.wrap_module(value)\n",
      "[2024-04-10 18:08:56,735] [16/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 937, in wrap_module\n",
      "[2024-04-10 18:08:56,735] [16/0_1] torch._guards: [ERROR]     return self.tx.output.register_attr_or_module(\n",
      "[2024-04-10 18:08:56,735] [16/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 736, in register_attr_or_module\n",
      "[2024-04-10 18:08:56,735] [16/0_1] torch._guards: [ERROR]     install_guard(source.make_guard(GuardBuilder.NN_MODULE))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6083957f27cd4fb0ad5d778d3012ef2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = pl.loggers.TensorBoardLogger(save_dir='./blm-log/', name='blm', version=0.1)\n",
    "# profiler = pl.profilers.PyTorchProfiler(\n",
    "#     on_trace_ready=torch.profiler.tensorboard_trace_handler('./blm-log/'),\n",
    "#     schedule=torch.profiler.schedule(skip_first=10, wait=10, warmup=1, active=2)\n",
    "# )\n",
    "# saves top-K checkpoints based on \"train_loss\" metric\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    save_top_k=3,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"max\",\n",
    "    dirpath=\"checkpoints/blm/\",\n",
    "    filename=\"baby-llm-{train_loss:.3f}\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    min_epochs=1,\n",
    "    max_epochs=100,\n",
    "    precision='bf16-mixed',\n",
    "    enable_model_summary=True,\n",
    "    # profiler=profiler,\n",
    "    callbacks=[pl.callbacks.EarlyStopping('val_loss',patience=6,verbose=True),checkpoint_callback],\n",
    "#     default_root_dir=\"mnist_checkpoints/\",\n",
    "    enable_checkpointing  = True,\n",
    "    # fast_dev_run=True,\n",
    "    log_every_n_steps=int(1e3),\n",
    "    enable_progress_bar = True,\n",
    "    accumulate_grad_batches=4,\n",
    "    gradient_clip_val =1.0\n",
    "    \n",
    ")\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "trainer.fit(model, ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a59d846b-3b85-440b-8fa9-0892792a7ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c88bf48e9f497f9c5a5c292928407b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">          9.8125           </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m         9.8125          \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 9.8125}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327c965-a223-4a3d-9194-ae7b6b6f28a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac6ef1-c9f7-454f-8cdc-d5b74bd6d932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87536529-b53a-4e2d-bff5-1ab27263e8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439e3f32-2a84-4fea-80bb-4fca1f340478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4618a71-8d88-4e0b-be5e-901cbd70628d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c564e94-da5e-411e-96e4-e931afd23f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b86e1-8b1e-4cee-b7a1-bdf35be0cfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
