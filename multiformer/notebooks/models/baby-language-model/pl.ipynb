{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47b5ab2-2729-43c5-96f8-5597e8e2f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from src.models.blm.config import ModelArgs\n",
    "from src.models.blm.block import Block\n",
    "\n",
    "from src.cells.normalization import RMSLayerNorm\n",
    "from typing import Optional\n",
    "from src.cells.position import RotaryEmbedding\n",
    "from src.cells.optim_func import config_optimizer\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import functools\n",
    "\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch._dynamo\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "pl.seed_everything(123, workers=True)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backend.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "960b5ea0-a6c0-4b05-8400-2c2ba1b61500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataloader(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, data_path_train, data_path_val, tokenizer_path, batch_size, num_workers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_path_train = data_path_train\n",
    "        self.data_path_val = data_path_val\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.tokenizer = self._load_tokenizer(tokenizer_path)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def _load_tokenizer(self, tokenizer_path):\n",
    "        from src.tokenize.tokenizer import Tokenizer\n",
    "\n",
    "        return Tokenizer(tokenizer_path)\n",
    "\n",
    "    def _collate_fn(self, batch: int, padding_id: int):\n",
    "        batch = pad_sequence(\n",
    "            (torch.LongTensor(_[\"idx\"]) for _ in batch),\n",
    "            batch_first=True,\n",
    "            padding_value=padding_id,\n",
    "        )  # TODO : ShortTensor suffice our need but nn.Embedding don't support it. Using LOngTensor is a unnecessary waste of GPU memory\n",
    "        x_batch = torch.stack(\n",
    "            [en[:-1] for en in batch]\n",
    "        )  # Extract x (remove last token)\n",
    "        y_batch = torch.stack(\n",
    "            [en[1:] for en in batch]\n",
    "        )  # Extract y (remove first token)\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def setup(self, stage):\n",
    "\n",
    "        self.train_data = load_from_disk(self.data_path_train)\n",
    "        self.val_data = load_from_disk(self.data_path_val)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=functools.partial(\n",
    "                self._collate_fn, padding_id=self.tokenizer.eos_id()\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=functools.partial(\n",
    "                self._collate_fn, padding_id=self.tokenizer.eos_id()\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e99cc0-d131-46f3-9fb3-6ea8646925dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.max_seq_len = args.max_seq_len\n",
    "        self.tok_embd = nn.Embedding(\n",
    "            args.vocab_size, args.emebdding_dim, padding_idx=args.padding_idx\n",
    "        )\n",
    "        self.dropout = nn.Dropout(args.embedding_dropout)\n",
    "        self.rope_q = RotaryEmbedding(\n",
    "            args.emebdding_dim // args.num_attention_heads,\n",
    "            args.max_seq_len,\n",
    "            device=args.device,\n",
    "        )\n",
    "        self.rope_k = RotaryEmbedding(\n",
    "            args.emebdding_dim // args.num_key_value_heads,\n",
    "            args.max_seq_len,\n",
    "            device=args.device,\n",
    "        )\n",
    "\n",
    "        # Freeze the parameters rope_q and rope_k\n",
    "        self.rope_q.requires_grad_(False)\n",
    "        self.rope_k.requires_grad_(False)\n",
    "\n",
    "        self.layers = nn.ModuleList([Block(args) for lid in range(args.num_layers)])\n",
    "\n",
    "        self.norm = RMSLayerNorm(args.emebdding_dim, eps=args.rms_norm_eps)\n",
    "        self.output = nn.Linear(args.emebdding_dim, args.vocab_size, bias=False)\n",
    "\n",
    "        # share the unembedding parameters with the embedding parameters\n",
    "        self.tok_embd.weight = (\n",
    "            self.output.weight\n",
    "        )  # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"wo.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    p, mean=0.0, std=0.02 / math.sqrt(2 * args.num_layers)\n",
    "                )\n",
    "        self.lr = 1e-4\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.get_num_params()} Million Params Model\"\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.tok_embd.weight.numel()\n",
    "        return n_params / 1e6  # In Million\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout(self.tok_embd(tokens))\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x, self.rope_q, self.rope_k\n",
    "            )  ## How about we add residual connection here also ?\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def _common_step(self, batch, batch_index):\n",
    "        x, targets = batch\n",
    "        logits = self.output(self.forward(x))\n",
    "        loss = F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        if trainer.global_step == 0:\n",
    "            wandb.define_metric(\"train_loss\", summary=\"mean\")\n",
    "        self.log_dict({\"train_loss\": loss}, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        loss = self._common_step(batch, batch_idx)\n",
    "        if batch_idx % int(1e4) == 0:\n",
    "            self.log_dict(\n",
    "                {\"val_loss\": loss}, prog_bar=True, on_step=True, on_epoch=True\n",
    "            )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": 1e-2},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        return torch.optim.AdamW(\n",
    "            optim_groups, lr=self.lr, betas=(0.9, 0.95), fused=False\n",
    "        )\n",
    "\n",
    "    def predict_step(\n",
    "        self, batch, batch_idx, max_new_tokens=30, temperature=1.0, top_k=None\n",
    "    ):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # trim the token to the max_len\n",
    "            if batch.shape[1] > self.max_seq_len:\n",
    "                batch = batch[:, -self.max_seq_len :]\n",
    "            # inference-time mini-optimization: only forward the output on the very last position\n",
    "            logits = self.output(\n",
    "                self(batch)[:, [-1], :]\n",
    "            )  # note: using list [-1] to preserve the time dim\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            batch = torch.cat((batch, idx_next), dim=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff84ca15-01f3-4052-b1c7-b2f334782ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892ada04-b997-4352-89d5-bc8af0977197",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"/home/pranav-pc/projects/OpenTransformer/multiformer\"\n",
    "MODEL_CHECKPOINT_PATH = BASE_URL + \"/model_checkpoints/blm/last-v3.ckpt\"\n",
    "data_path_train = BASE_URL + \"/data/interim/TinyStories_train_65>tk>512.hf\"\n",
    "data_path_val = BASE_URL + \"/data/interim/TinyStories_val_65>tk>512.hf\"\n",
    "tokenizer_path = BASE_URL + \"/tokenizer_checkpoints\"\n",
    "\n",
    "\n",
    "load_from_checkpoint = False\n",
    "train = True\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 26\n",
    "ds = TinyStoriesDataloader(\n",
    "    data_path_train, data_path_val, tokenizer_path, batch_size, num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "007b2881-c39e-45bf-a402-8b16940092ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"vocab_size\": 32000,\n",
    "    \"emebdding_dim\": 768,\n",
    "    \"max_seq_len\": 512,\n",
    "    \"embedding_dropout\": 0.0,\n",
    "    \"rms_norm_eps\": 1e-05,\n",
    "    \"rope_scaling\": 1.0,\n",
    "    \"rope_theta\": 10000.0,\n",
    "    \"attention_bias\": False,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_key_value_heads\": 12,\n",
    "    \"use_cache\": True,\n",
    "    \"use_sliding_window\": True,\n",
    "    \"residual_dropout\": 0.1,\n",
    "    \"mlp_dropout\": 0.0,\n",
    "    \"mlp_hidden_size\": int(1.3 * 768),\n",
    "    \"num_layers\": 4,\n",
    "    \"device\": device,\n",
    "    \"padding_idx\": ds.tokenizer.eos_id(),\n",
    "}\n",
    "if load_from_checkpoint:\n",
    "    model = Transformer.load_from_checkpoint(MODEL_CHECKPOINT_PATH)\n",
    "    model = torch.compile(model, dynamic=True)\n",
    "else:\n",
    "    config = ModelArgs(**conf)\n",
    "    model = Transformer(config)\n",
    "    model = torch.compile(model, dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf39740-9e0d-4d34-8f8d-47bb9c74f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import (\n",
    "    GradientAccumulationScheduler,\n",
    "    StochasticWeightAveraging,\n",
    "    ModelCheckpoint,\n",
    "    EarlyStopping,\n",
    "    BatchSizeFinder,\n",
    "    LearningRateFinder,\n",
    ")\n",
    "\n",
    "# class DynamicBatchSizeFinder(BatchSizeFinder):\n",
    "#     def __init__(self, milestones, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.milestones = milestones\n",
    "\n",
    "#     def on_fit_start(self, *args, **kwargs):\n",
    "#         return\n",
    "\n",
    "#     def on_train_batch_start(self, trainer, pl_module,batch,batch_idx):\n",
    "#         if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "#             if batch_idx % 5 == 0:\n",
    "#                 self.scale_batch_size(trainer, pl_module)\n",
    "\n",
    "\n",
    "class FineTuneLearningRateFinder(LearningRateFinder):\n",
    "    def __init__(self, milestones, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.milestones = milestones\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n",
    "\n",
    "            self.lr_find(trainer, pl_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc0aaf79-3974-49d8-baaa-3f4dd5c18d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpranav_jha\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    name=\"blm\",\n",
    "    save_dir=\"blm/\",\n",
    "    version=\"v1\",\n",
    "    offline=False,\n",
    "    project=\"tiny-stories\",\n",
    "    log_model=\"all\",\n",
    ")\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8111d93-5991-4fdf-a35e-2a2a4f247a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path blm/wandb/ wasn't writable, using system temp directory.\n",
      "wandb: WARNING Path blm/wandb/ wasn't writable, using system temp directory\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/run-20240412_151904-v1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/pranav_jha/tiny-stories/runs/v1' target=\"_blank\">blm</a></strong> to <a href='https://wandb.ai/pranav_jha/tiny-stories' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pranav_jha/tiny-stories' target=\"_blank\">https://wandb.ai/pranav_jha/tiny-stories</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pranav_jha/tiny-stories/runs/v1' target=\"_blank\">https://wandb.ai/pranav_jha/tiny-stories/runs/v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav-pc/.env/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/pranav-pc/projects/OpenTransformer/multiformer/notebooks/models/baby-language-model/model_checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_inductor/lowering.py:1611: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.\n",
      "  warnings.warn(\n",
      "[2024-04-12 15:19:14,201] [15/0_1] torch._guards: [ERROR] Error while creating guard:\n",
      "[2024-04-12 15:19:14,201] [15/0_1] torch._guards: [ERROR] Name: \"L['self']\"\n",
      "[2024-04-12 15:19:14,201] [15/0_1] torch._guards: [ERROR]     Source: local\n",
      "[2024-04-12 15:19:14,201] [15/0_1] torch._guards: [ERROR]     Create Function: NN_MODULE\n",
      "[2024-04-12 15:19:14,201] [15/0_1] torch._guards: [ERROR]     Guard Types: ['ID_MATCH']\n",
      "[2024-04-12 15:19:14,201] [15/0_1] torch._guards: [ERROR]     Code List: [\"___check_obj_id(L['self'], 133684819005456)\"]\n",
      "[2024-04-12 15:19:14,201] [15/0_1] torch._guards: [ERROR]     Object Weakref: <weakref at 0x7995ec3fb6f0; to '_ResultMetric' at 0x7995ec261010>\n",
      "[2024-04-12 15:19:14,201] [15/0_1] torch._guards: [ERROR]     Guarded Class Weakref: <weakref at 0x79962df78220; to 'ABCMeta' at 0x986a5c0 (_ResultMetric)>\n",
      "[2024-04-12 15:19:14,202] [15/0_1] torch._guards: [ERROR] Created at:\n",
      "[2024-04-12 15:19:14,202] [15/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 245, in __call__\n",
      "[2024-04-12 15:19:14,202] [15/0_1] torch._guards: [ERROR]     vt = self._wrap(value).clone(**self.options())\n",
      "[2024-04-12 15:19:14,202] [15/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 469, in _wrap\n",
      "[2024-04-12 15:19:14,202] [15/0_1] torch._guards: [ERROR]     return self.wrap_module(value)\n",
      "[2024-04-12 15:19:14,202] [15/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 937, in wrap_module\n",
      "[2024-04-12 15:19:14,202] [15/0_1] torch._guards: [ERROR]     return self.tx.output.register_attr_or_module(\n",
      "[2024-04-12 15:19:14,202] [15/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 736, in register_attr_or_module\n",
      "[2024-04-12 15:19:14,202] [15/0_1] torch._guards: [ERROR]     install_guard(source.make_guard(GuardBuilder.NN_MODULE))\n",
      "[2024-04-12 15:19:14,210] [16/0_1] torch._guards: [ERROR] Error while creating guard:\n",
      "[2024-04-12 15:19:14,210] [16/0_1] torch._guards: [ERROR] Name: \"L['self']\"\n",
      "[2024-04-12 15:19:14,210] [16/0_1] torch._guards: [ERROR]     Source: local\n",
      "[2024-04-12 15:19:14,210] [16/0_1] torch._guards: [ERROR]     Create Function: NN_MODULE\n",
      "[2024-04-12 15:19:14,210] [16/0_1] torch._guards: [ERROR]     Guard Types: ['ID_MATCH']\n",
      "[2024-04-12 15:19:14,210] [16/0_1] torch._guards: [ERROR]     Code List: [\"___check_obj_id(L['self'], 133684819005456)\"]\n",
      "[2024-04-12 15:19:14,210] [16/0_1] torch._guards: [ERROR]     Object Weakref: <weakref at 0x7995ec3fb6f0; to '_ResultMetric' at 0x7995ec261010>\n",
      "[2024-04-12 15:19:14,210] [16/0_1] torch._guards: [ERROR]     Guarded Class Weakref: <weakref at 0x79962df78220; to 'ABCMeta' at 0x986a5c0 (_ResultMetric)>\n",
      "[2024-04-12 15:19:14,211] [16/0_1] torch._guards: [ERROR] Created at:\n",
      "[2024-04-12 15:19:14,211] [16/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 245, in __call__\n",
      "[2024-04-12 15:19:14,211] [16/0_1] torch._guards: [ERROR]     vt = self._wrap(value).clone(**self.options())\n",
      "[2024-04-12 15:19:14,211] [16/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 469, in _wrap\n",
      "[2024-04-12 15:19:14,211] [16/0_1] torch._guards: [ERROR]     return self.wrap_module(value)\n",
      "[2024-04-12 15:19:14,211] [16/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py\", line 937, in wrap_module\n",
      "[2024-04-12 15:19:14,211] [16/0_1] torch._guards: [ERROR]     return self.tx.output.register_attr_or_module(\n",
      "[2024-04-12 15:19:14,211] [16/0_1] torch._guards: [ERROR]   File \"/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 736, in register_attr_or_module\n",
      "[2024-04-12 15:19:14,211] [16/0_1] torch._guards: [ERROR]     install_guard(source.make_guard(GuardBuilder.NN_MODULE))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2f4a666c5046d4a8d7986fc804b2ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-12 15:20:15,162] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (8)\n",
      "[2024-04-12 15:20:15,162] torch._dynamo.convert_frame: [WARNING]    function: 'training_step' (/tmp/ipykernel_73791/4236192055.py:83)\n",
      "[2024-04-12 15:20:15,162] torch._dynamo.convert_frame: [WARNING]    last reason: G['trainer'].fit_loop.epoch_loop.automatic_optimization.optim_progress.optimizer.step.total.completed == 0  # return self.optimizer.step.total.completed  # lightning/pytorch/loops/progress.py:276 in optimizer_steps\n",
      "[2024-04-12 15:20:15,162] torch._dynamo.convert_frame: [WARNING] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[2024-04-12 15:20:15,162] torch._dynamo.convert_frame: [WARNING] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at model_checkpoints/.lr_find_3c7e18f4-0361-4646-861c-c61d6deb0371.ckpt\n",
      "Restored all states from the checkpoint at model_checkpoints/.lr_find_3c7e18f4-0361-4646-861c-c61d6deb0371.ckpt\n",
      "\n",
      "  | Name     | Type            | Params\n",
      "---------------------------------------------\n",
      "0 | tok_embd | Embedding       | 24.6 M\n",
      "1 | dropout  | Dropout         | 0     \n",
      "2 | rope_q   | RotaryEmbedding | 0     \n",
      "3 | rope_k   | RotaryEmbedding | 0     \n",
      "4 | layers   | ModuleList      | 15.6 M\n",
      "5 | norm     | RMSLayerNorm    | 768   \n",
      "6 | output   | Linear          | 24.6 M\n",
      "---------------------------------------------\n",
      "40.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "40.2 M    Total params\n",
      "160.623   Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at model_checkpoints/.lr_find_3c7e18f4-0361-4646-861c-c61d6deb0371.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dddfdd8f5bc42e09bc2a9ba5778d5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce324a369efc4df5828c4f934969df2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at model_checkpoints/.lr_find_5f506173-c584-4eaa-9014-ba33b6d7d7c3.ckpt\n",
      "Restored all states from the checkpoint at model_checkpoints/.lr_find_5f506173-c584-4eaa-9014-ba33b6d7d7c3.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric train_loss improved. New best score: 2.190\n",
      "`Trainer.fit` stopped: `max_steps=5000` reached.\n"
     ]
    }
   ],
   "source": [
    "accumulator = GradientAccumulationScheduler(scheduling={0: 4, 4: 3, 6: 1})\n",
    "\n",
    "# logger = pl.loggers.TensorBoardLogger(save_dir=\"./blm-log/\", name=\"blm\", version=1.0)\n",
    "# profiler = pl.profilers.PyTorchProfiler(\n",
    "#     on_trace_ready=torch.profiler.tensorboard_trace_handler('./blm-log/'),\n",
    "#     schedule=torch.profiler.schedule(skip_first=10, wait=10, warmup=1, active=2)\n",
    "# )\n",
    "# saves top-K checkpoints based on \"train_loss\" metric\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=2,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"max\",\n",
    "    dirpath=\"model_checkpoints/\",\n",
    "    filename=\"baby-llm-{epoch:02d}-{train_loss:.3f}\",\n",
    "    save_last=True,\n",
    "    every_n_train_steps=int(1e4),\n",
    "    save_on_train_epoch_end=True,\n",
    ")\n",
    "early_stop = EarlyStopping(\"train_loss\", patience=10, verbose=True)\n",
    "stochastic_weight_avg = StochasticWeightAveraging(swa_lrs=1e-6)\n",
    "# dynamic_batch_size = DynamicBatchSizeFinder(milestones=(6, 20))\n",
    "lr_finder = FineTuneLearningRateFinder(milestones=(5, 20))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    min_epochs=1,\n",
    "    max_epochs=100,\n",
    "    precision=\"bf16-mixed\",\n",
    "    # enable_model_summary=True,\n",
    "    # profiler=profiler,\n",
    "    callbacks=[\n",
    "        early_stop,\n",
    "        checkpoint_callback,\n",
    "        accumulator,\n",
    "        stochastic_weight_avg,\n",
    "        lr_finder,\n",
    "    ],\n",
    "    default_root_dir=\"model_checkpoints/\",\n",
    "    enable_checkpointing=True,\n",
    "    # fast_dev_run=True,\n",
    "    log_every_n_steps=5,\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_val=1.0,\n",
    "    max_steps=5000,\n",
    ")\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "if train:\n",
    "    model.train()\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        ds,\n",
    "        # ckpt_path=MODEL_CHECKPOINT_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f66e4949-1ded-48c9-adf2-1dc9018e0a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d15fad2e78942069f573464429208eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_loss_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">         1.9765625         </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_loss_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m        1.9765625        \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss_epoch': 1.9765625}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "trainer.validate(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e70a62b0-9409-47a1-8e0e-76071c256450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Inference\n",
    "model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "06ecc1ab-a110-4872-8106-06ebef470130",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Write a story containing the words: dive, job, sorry. Story summary: Bob the big fish finds a shiny rock while searching for food for his friends, but when he tells them about it, they are excited to play with it instead of being sad about not having food.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "2306be0f-c52c-4eaa-8b09-528f1d14bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a story containing the words: dive, job, sorry. Story summary: Bob the big fish finds a shiny rock while searching for food for his friends, but when he tells them about it, they are excited to play with it instead of being sad about not having food., a shiny coin! Bob thought for a moment until he had an idea. He took the coin and dove into the sea, hoping to find something special. He searched and searched, until suddenly he saw a big fishâ€”it was shiny, just like his coin! Bob was so happy that he had found something special. From then on, he always enjoyed diving and finding coins, and was never disappointed when he felt something new.\n",
      "\n",
      " Once upon a time, there was a little girl named Lily. She loved to play outside in the garden. One day, she saw a little herb plant growing in a pot. It smelled so good! Lily picked a herb and put it in a vase. She wanted to show it to her mom and dad, so she started to run. But when she got to the herb, the vase was empty! Lily was so sad. She\n"
     ]
    }
   ],
   "source": [
    "# text = \"Lily wanted to get either a cat or a dog. Her mother didn’t let her get a dog so instead she\"\n",
    "tokens = torch.LongTensor(ds.tokenizer.encode(text)).to(\"cuda:0\").view(1, -1)\n",
    "# tokens\n",
    "print(\n",
    "    ds.tokenizer.decode_ids(\n",
    "        model.predict_step(\n",
    "            tokens, None, max_new_tokens=200, temperature=0.8, top_k=None\n",
    "        )[0].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "822a1767-c364-4087-8738-edca65d70760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁”',\n",
       " 'Can',\n",
       " '▁c',\n",
       " 'ows',\n",
       " '▁fly',\n",
       " '?”',\n",
       " ',',\n",
       " '▁Alice',\n",
       " '▁asked',\n",
       " '▁her',\n",
       " '▁mother',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.tokenizer.encode(text, out_type=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ff9f4-674f-4f0c-a211-86dc1a2fd814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a697f7-cded-4018-acd3-4fe61328a615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
