{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ca7d1f1-4864-4f38-9fdf-d7e77c968dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORCH VERSION : 2.2.2\n",
      "Device  :  CUDA\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "\n",
    "print(\"TORCH VERSION :\", version(\"torch\"))\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backend.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(\"Device  : \", device.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14abc783-c156-4b4d-b08b-60078da94260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 81.25M\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "from src.models.gpt2.model import GPT2\n",
    "from src.models.gpt2.config import GPT2Config\n",
    "\n",
    "\n",
    "gpt_conf = {\n",
    "    \"block_size\": 1024,\n",
    "    \"vocab_size\": 32000,\n",
    "    \"n_layer\": 8,\n",
    "    \"n_head\": 12,\n",
    "    \"n_embd\": 768,\n",
    "    \"dropout\": 0.0,\n",
    "    \"bias\": True,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "config = GPT2Config(**gpt_conf)\n",
    "model = GPT2(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8138226c-8aca-47a7-a321-61ab99bd0708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\n",
    "    \"/home/pranav-pc/projects/OpenTransformer/multiformer/model_checkpoint_epoch_1_iter_3000.pt\"\n",
    ")\n",
    "unwanted_prefix = \"_orig_mod.\"\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9abf2d8-b037-447c-a8fb-83502964046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b45b3177-f936-4237-9b65-b62cf6efc534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)\n",
    "s = \"What is the meaning of name 'Pranav'\"\n",
    "idx = encode(s)\n",
    "x = torch.tensor(idx, dtype=torch.long, device=device)[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6e3430d-4b70-4eab-8787-a151b23d1f3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_new_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m         y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(x, \u001b[43mmax_new_tokens\u001b[49m, temperature\u001b[38;5;241m=\u001b[39mtemprature, top_k\u001b[38;5;241m=\u001b[39mtop_k)\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(decode(y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_new_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# run generation\n",
    "with torch.no_grad():\n",
    "    for k in range(2):\n",
    "        y = model.generate(x, max_new_tokens, temperature=temprature, top_k=top_k)\n",
    "        print(decode(y[0].tolist()))\n",
    "        print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300215c-de24-4ffb-be21-0c427b90eab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
