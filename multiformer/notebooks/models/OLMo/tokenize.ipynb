{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6a1d26-c415-4f43-bb36-abe0b440b983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TORCH VERSION : 2.2.1\n",
      "GPU  :  CUDA\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import torch\n",
    "\n",
    "print(\"TORCH VERSION :\", version(\"torch\"))\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print('GPU  : ', device.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ef35a5-b5ec-4c79-b8c8-6fc998d1fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4fed807-f0ea-4d74-88c6-2b420bc8bc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 129173\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load only 2% of the data \n",
    "dataset = load_dataset(\"/home/pranav-pc/projects/OpenTransformer/multiformer/data/wikipedia/\",split='train[:2%]')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27970419-1374-4e8a-8eca-ef184624b5b2",
   "metadata": {},
   "source": [
    "Building blocks of tokenizer\n",
    "\n",
    "\n",
    "    normalizers contains all the possible types of Normalizer you can use.\n",
    "    \n",
    "    pre_tokenizers contains all the possible types of PreTokenizer you can use \n",
    "    \n",
    "    models contains the various types of Model you can use, like BPE, WordPiece, and Unigram \n",
    "    \n",
    "    trainers contains all the different types of Trainer you can use to train your model on a corpus (one per type of model).\n",
    "    \n",
    "    post_processors contains the various types of PostProcessor you can use \n",
    "    \n",
    "    decoders contains the various types of Decoder you can use to decode the outputs of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07573281-b793-4536-a36c-167fef92fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98965677-3dfa-4d3f-a132-d14fe61390df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(Rust) :  True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\",trust_remote_code=True)\n",
    "print('Tokenizer(Rust) : ',old_tokenizer.is_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6505bf74-6950-4dac-8fa9-0ea9116add3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 37min 44s, sys: 1min 9s, total: 38min 54s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(),52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0ccd9b7-4e39-44b9-b34e-252abdd87959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Let', \"'s\", 'Ġhave', 'Ġsome', 'Ġfun']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Let's have some fun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a5bc23-073c-4b53-a870-cbe5ec18970b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wiki-gpt2-tokenizer/tokenizer_config.json',\n",
       " 'wiki-gpt2-tokenizer/special_tokens_map.json',\n",
       " 'wiki-gpt2-tokenizer/vocab.json',\n",
       " 'wiki-gpt2-tokenizer/merges.txt',\n",
       " 'wiki-gpt2-tokenizer/added_tokens.json',\n",
       " 'wiki-gpt2-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"wiki-gpt2-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bff80c-01f3-4105-94ff-08def45b9a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
