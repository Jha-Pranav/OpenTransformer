{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa007655-48f5-4f7d-9d94-03bdc4777756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bs4 import BeautifulSoup\n",
    "from omegaconf import OmegaConf\n",
    "from src.cells.utils.compile_utils import torch_compile\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "# Silence all warnings\n",
    "import warnings\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    GradientAccumulationScheduler,\n",
    "    LearningRateFinder,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    "    StochasticWeightAveraging,\n",
    ")\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaae4244-6d3f-497e-a58e-821e6ae121a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataLoader(pl.LightningDataModule):\n",
    "    def __init__(self, dataset_path, tokenizer_path, batch_size, num_workers, max_len):\n",
    "        super().__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.tokenizer = self._load_tokenizer(tokenizer_path)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        self.ds = load_dataset(self.dataset_path)\n",
    "        # self.ds = self.ds.map(lambda example : {'text':self._remove_html_tags(example['text'])},num_proc=self.num_workers,)\n",
    "        self.label_map = {0: \"neg\", 1: \"pos\"}\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_tokenizer(tokenizer_path):\n",
    "        from src.tokenize.tokenizer import Tokenizer\n",
    "\n",
    "        return Tokenizer(tokenizer_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_html_tags(text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        # Get the text without HTML tags\n",
    "        clean_text = soup.get_text()\n",
    "        return clean_text\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        x, y = [self._remove_html_tags(en[\"text\"]) for en in batch], [en[\"label\"] for en in batch]\n",
    "        x = [torch.tensor(tokens[: self.max_len]) for tokens in self.tokenizer.encode_as_ids(x)]\n",
    "        x = pad_sequence(\n",
    "            x,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.eos_id(),\n",
    "        )\n",
    "        y = torch.tensor(y)\n",
    "        return x, y\n",
    "\n",
    "    def setup(self, stage):\n",
    "\n",
    "        self.train_data = self.ds[\"train\"]\n",
    "        self.val_data = self.ds[\"test\"]\n",
    "        self.test_data = self.ds[\"unsupervised\"]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self._collate_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self._collate_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            collate_fn=self._collate_fn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713a0454-7028-47ec-b7d2-0963b6eb5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity\n",
    "# ds.prepare_data()\n",
    "# ds.setup('train')\n",
    "# for idx,label in ds.train_dataloader():\n",
    "#     print(idx.shape,label)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d3a5bb-9f5e-46cf-a7a0-fbdd479f2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5998b56c-c688-40f8-98d0-748ae563465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLMClassifierModel(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=5e-5, num_classes=2, embedding_dim=768):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "\n",
    "        self.acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=2)\n",
    "        self.classifer = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.model(input_ids)\n",
    "\n",
    "    def _common_step(self, batch):\n",
    "        x, label = batch\n",
    "        hidden_state = self.forward(x)\n",
    "        logits = self.classifer(hidden_state[:, -1, :])\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, targets = batch\n",
    "        logits = self._common_step(batch)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, targets = batch\n",
    "        logits = self._common_step(batch)\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        val_acc = self.acc(preds, targets)\n",
    "        val_f1score = self.f1_score(preds, targets)\n",
    "        self.log_dict(\n",
    "            {\"val_loss\": loss, \"val_acc\": val_acc, \"val_f1score\": val_f1score}, prog_bar=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     x, targets = batch\n",
    "    #     logits = self._common_step(batch)\n",
    "    #     loss = F.cross_entropy(logits, targets)\n",
    "    #     preds = torch.argmax(logits, dim=1)\n",
    "    #     test_acc = self.acc(preds, targets)\n",
    "    #     test_f1score = self.f1_score(preds, targets)\n",
    "    #     self.log_dict({\"test_loss\": loss,\"test_acc\":test_acc,\"test_f1score\":test_f1score}, prog_bar=False)\n",
    "    #     return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": 1e-2},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        lr_scheduler_init = {\"T_max\": 1e04, \"eta_min\": 1e-04}\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optim_groups, lr=self.learning_rate, betas=(0.9, 0.95), fused=False\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": CosineAnnealingLR(optimizer, **lr_scheduler_init),\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 10,\n",
    "        }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09000264-cdd9-4eab-adb5-b89c68530689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n",
      "Seed set to 123\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "def main(args, train=True):\n",
    "\n",
    "    ds = IMDBDataLoader(\n",
    "        args.files.data_path,\n",
    "        args.files.tokenizer_path,\n",
    "        args.trainer_params.batch_size,\n",
    "        args.trainer_params.num_workers,\n",
    "        1024,\n",
    "    )\n",
    "\n",
    "    from src.models.blm.pl_training import Transformer\n",
    "\n",
    "    MODEL_CHECKPOINT = args.paths.base_model_checkpoint\n",
    "\n",
    "    base_model = Transformer.load_from_checkpoint(MODEL_CHECKPOINT)\n",
    "\n",
    "    model = BLMClassifierModel(base_model)\n",
    "    if args.trainer_params.resume_training:\n",
    "        model.load_state_dict(torch.load(args.paths.resume_from_checkpoint)[\"state_dict\"])\n",
    "    # model = torch_compile(model, dynamic=True, TORCH_COMPILE_BACKEND=\"inductor\")\n",
    "    accumulator = GradientAccumulationScheduler(\n",
    "        scheduling=args.trainer_params.gradient_accumulation_scheduler\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(save_dir=\"./lightning-log-ft-imdb/\", name=\"IMDB\", version=0.1)\n",
    "\n",
    "    if args.trainer_params.wandb_enabled:\n",
    "        import wandb\n",
    "\n",
    "        print(\"W&B\")\n",
    "        wandb.login()\n",
    "        logger = WandbLogger(**args.trainer_params.wandb)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(**args.trainer_params.checkpoint)\n",
    "    early_stop = EarlyStopping(**args.trainer_params.earlystopping)\n",
    "    stochastic_weight_avg = StochasticWeightAveraging(swa_lrs=1e-6)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger,\n",
    "        **args.trainer_params.trainer,\n",
    "        callbacks=[\n",
    "            early_stop,\n",
    "            checkpoint_callback,\n",
    "            accumulator,\n",
    "            LearningRateMonitor(logging_interval=\"step\"),\n",
    "            stochastic_weight_avg,\n",
    "            # DeviceStatsMonitor()\n",
    "        ],\n",
    "    )\n",
    "    if train:\n",
    "        trainer.fit(model, ds)\n",
    "    return ds, model, trainer\n",
    "\n",
    "\n",
    "config_path = \"/home/pranav-pc/projects/OpenTransformer/multiformer/src/models/blm/conf/finetune-imdb-classifier.yaml\"\n",
    "args = OmegaConf.load(config_path)\n",
    "ds, model, trainer = main(args, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef777291-a7f4-42b7-8954-8b725f7151aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d03d74e5cb040ecb23225214c2b83e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8829200267791748     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        val_f1score        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8829200267791748     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.1901986598968506     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8829200267791748    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       val_f1score       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8829200267791748    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.1901986598968506    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 1.1901986598968506,\n",
       "  'val_acc': 0.8829200267791748,\n",
       "  'val_f1score': 0.8829200267791748}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdd49883-5601-4fc6-a807-d17da42932c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack was hungry, so he went looking for something to eat. He saw a big, red apple on the ground and he wanted to eat it. He picked up the apple and took a bite. It was so juicy and yummy! Jack was so happy. He ate the apple all up and then he started to play. He ran and jumped around the garden, and he even tried to catch the apple in his mouth.\n",
      "\n",
      "\n",
      "========================================\n",
      "Jack was hungry, so he went looking for something to eat. He looked around the kitchen and he saw a big bowl of cereal. He thought it would be a good snack. He opened the cupboard and saw a big bowl of cereal. He was so excited! He grabbed the big spoon and started to eat the cereal.\n",
      "\n",
      "\n",
      "========================================\n",
      "Jack was hungry, so he went looking for something. He looked in the kitchen and the cupboard, and he looked in his room, but he couldn't find what he was looking for. He looked in the kitchen and the cupboard, but there was nothing there either.\n",
      "\n",
      "\n",
      "========================================\n",
      "============================================================\n",
      "Once upon a time there was a pumpkin. It was a very special pumpkin, it could speak. It was sad because it couldn’t move. Every day, it would say \"hello\" and ask why it was so sad.\n",
      "\n",
      "\n",
      "========================================\n",
      "Once upon a time there was a pumpkin. It was a very special pumpkin, it could speak. It was sad because it couldn’t move. Every day, it would say, \"Why can't I move? I'm so small!\"\n",
      "\n",
      "\n",
      "========================================\n",
      "Once upon a time there was a pumpkin. It was a very special pumpkin, it could speak. It was sad because it couldn’t move. Every day, it would say, \"I'm so tired. I can't speak anymore.\"\n",
      "\n",
      "\n",
      "========================================\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "model.model.eval()\n",
    "model.model.cuda()\n",
    "\n",
    "corpus = [\n",
    "    \"Jack was hungry, so he went looking for\",\n",
    "    \"Once upon a time there was a pumpkin. It was a very special pumpkin, it could speak. It was sad because it couldn’t move. Every day, it would say\",\n",
    "]\n",
    "\n",
    "# print(tokens)\n",
    "for text in corpus:\n",
    "    tokens = torch.LongTensor(ds.tokenizer.encode(text)).to(\"cuda:0\").view(1, -1)[:, :-1]\n",
    "    for _ in range(3):\n",
    "        print(\n",
    "            ds.tokenizer.decode_ids(\n",
    "                model.model.predict_step(\n",
    "                    tokens,\n",
    "                    None,\n",
    "                    max_new_tokens=1000,\n",
    "                    temperature=0.9,\n",
    "                    top_k=2,\n",
    "                    conditional_break=[13, 13],\n",
    "                )[0].tolist()\n",
    "            )\n",
    "        )\n",
    "        print(\"==\" * 20)\n",
    "    print(\"==\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2413af57-50ad-4776-8a34-34cf22f94d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's make sure that inference is done on the new finetunned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb0a11de-9a3c-4626-b14a-646c7f6710ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embd.weight Parameter containing:\n",
      "tensor([[-4.0612e-02, -1.0990e-02, -2.8261e-02,  ...,  3.4319e-02,\n",
      "         -3.4704e-02, -1.8701e-02],\n",
      "        [-2.1640e-03,  1.2131e-02, -1.7759e-02,  ..., -1.2359e-04,\n",
      "         -3.9344e-02,  2.9360e-02],\n",
      "        [-8.2522e-02, -1.5970e-02, -5.1656e-02,  ..., -1.5299e-01,\n",
      "         -4.8903e-02,  9.0692e-02],\n",
      "        ...,\n",
      "        [-4.0577e-02, -1.0985e-02, -2.8255e-02,  ...,  3.4327e-02,\n",
      "         -3.4681e-02, -1.8714e-02],\n",
      "        [-4.0572e-02, -1.0950e-02, -2.8243e-02,  ...,  3.4374e-02,\n",
      "         -3.4712e-02, -1.8676e-02],\n",
      "        [-4.0550e-02, -1.0993e-02, -2.8262e-02,  ...,  3.4358e-02,\n",
      "         -3.4678e-02, -1.8740e-02]], device='cuda:0', requires_grad=True)\n",
      "layers.0.norms.w Parameter containing:\n",
      "tensor([0.1576, 0.1675, 0.1600, 0.1530, 0.1468, 0.1576, 0.1658, 0.2789, 0.1622,\n",
      "        0.1383, 0.0581, 0.1467, 0.1353, 0.1373, 0.1606, 0.1574, 0.1651, 0.1377,\n",
      "        0.1656, 0.1322, 0.1720, 0.1415, 0.1542, 0.1247, 0.1428, 0.1383, 0.1361,\n",
      "        0.2108, 0.1380, 0.1843, 0.1437, 0.1414, 0.1562, 0.1394, 0.1482, 0.1204,\n",
      "        0.2066, 0.1566, 0.1547, 0.1529, 0.1529, 0.1614, 0.1640, 0.1513, 0.1813,\n",
      "        0.1607, 0.1652, 0.1379, 0.1572, 0.1423, 0.0797, 0.1672, 0.1317, 0.1684,\n",
      "        0.1501, 0.1248, 0.1493, 0.1433, 0.1542, 0.1608, 0.1553, 0.1576, 0.1417,\n",
      "        0.1492, 0.1518, 0.1411, 0.1653, 0.1456, 0.1259, 0.1403, 0.1272, 0.1237,\n",
      "        0.1486, 0.1390, 0.1430, 0.1200, 0.1474, 0.1478, 0.0807, 0.1473, 0.1391,\n",
      "        0.1272, 0.1724, 0.1468, 0.1543, 0.1388, 0.1716, 0.1572, 0.1630, 0.1389,\n",
      "        0.1458, 0.1114, 0.1526, 0.1306, 0.1624, 0.1542, 0.1276, 0.1293, 0.1087,\n",
      "        0.1454, 0.1561, 0.1360, 0.2456, 0.1577, 0.1534, 0.1441, 0.1491, 0.0472,\n",
      "        0.1372, 0.1622, 0.1469, 0.1415, 0.1720, 0.1522, 0.1548, 0.1553, 0.1389,\n",
      "        0.1112, 0.1685, 0.1838, 0.1273, 0.1401, 0.1294, 0.1365, 0.0535, 0.1728,\n",
      "        0.1257, 0.1455, 0.1591, 0.1600, 0.1255, 0.1188, 0.1623, 0.1554, 0.1561,\n",
      "        0.1429, 0.1426, 0.1502, 0.1589, 0.1472, 0.1531, 0.1310, 0.1367, 0.1464,\n",
      "        0.1612, 0.1499, 0.1577, 0.1544, 0.1746, 0.1748, 0.1561, 0.1433, 0.1526,\n",
      "        0.1567, 0.1632, 0.1646, 0.1581, 0.1023, 0.1746, 0.1548, 0.1400, 0.1379,\n",
      "        0.1637, 0.1454, 0.1291, 0.1382, 0.1449, 0.1484, 0.2311, 0.1450, 0.0693,\n",
      "        0.1546, 0.1577, 0.1389, 0.1481, 0.1601, 0.1694, 0.1548, 0.1385, 0.1597,\n",
      "        0.1386, 0.1465, 0.1659, 0.1906, 0.1410, 0.1562, 0.1460, 0.1705, 0.1634,\n",
      "        0.1624, 0.1520, 0.1385, 0.1381, 0.1529, 0.1756, 0.1159, 0.1546, 0.1586,\n",
      "        0.1465, 0.1136, 0.1703, 0.1486, 0.1467, 0.1269, 0.1569, 0.1163, 0.1616,\n",
      "        0.1409, 0.1607, 0.1591, 0.1851, 0.1437, 0.1406, 0.1445, 0.1642, 0.1546,\n",
      "        0.1550, 0.1488, 0.1340, 0.1695, 0.1513, 0.1407, 0.2367, 0.1961, 0.1700,\n",
      "        0.1660, 0.1679, 0.1624, 0.1235, 0.1710, 0.1423, 0.1537, 0.1425, 0.1286,\n",
      "        0.1544, 0.1299, 0.1682, 0.1441, 0.1248, 0.1706, 0.1307, 0.1631, 0.1589,\n",
      "        0.1498, 0.1432, 0.1531, 0.1453, 0.1605, 0.1418, 0.1881, 0.1515, 0.1510,\n",
      "        0.1633, 0.1489, 0.1576, 0.1555, 0.1698, 0.2070, 0.1364, 0.1511, 0.1491,\n",
      "        0.1461, 0.1672, 0.1341, 0.1409, 0.1439, 0.1538, 0.1529, 0.1590, 0.1608,\n",
      "        0.1400, 0.1793, 0.1397, 0.1527, 0.1242, 0.1505, 0.1713, 0.1598, 0.1727,\n",
      "        0.1568, 0.1589, 0.1646, 0.1606, 0.1343, 0.1325, 0.1440, 0.1532, 0.1541,\n",
      "        0.1444, 0.1563, 0.1376, 0.1620, 0.1367, 0.1402, 0.1533, 0.1558, 0.1548,\n",
      "        0.1411, 0.1522, 0.1509, 0.1387, 0.1454, 0.1742, 0.1435, 0.1473, 0.1251,\n",
      "        0.1370, 0.1353, 0.1965, 0.1265, 0.1621, 0.1377, 0.1482, 0.1411, 0.1487,\n",
      "        0.1896, 0.1555, 0.1521, 0.1482, 0.1624, 0.1400, 0.1459, 0.1561, 0.1300,\n",
      "        0.1342, 0.1182, 0.1513, 0.1730, 0.1564, 0.1503, 0.1481, 0.1487, 0.1500,\n",
      "        0.1573, 0.1879, 0.1577, 0.1554, 0.1187, 0.1572, 0.1442, 0.1712, 0.1502,\n",
      "        0.1597, 0.1466, 0.1606, 0.1656, 0.1156, 0.1192, 0.1345, 0.1634, 0.1439,\n",
      "        0.1609, 0.1481, 0.1487, 0.1549, 0.1602, 0.1650, 0.1626, 0.1408, 0.1441,\n",
      "        0.1507, 0.1482, 0.1490, 0.1326, 0.1459, 0.1535, 0.1549, 0.1490, 0.1758,\n",
      "        0.1485, 0.1605, 0.1603, 0.1448, 0.1349, 0.1374, 0.1385, 0.1479, 0.1487,\n",
      "        0.1802, 0.1499, 0.1375, 0.1014, 0.1499, 0.1683, 0.1396, 0.1560, 0.1212,\n",
      "        0.1779, 0.1437, 0.1494, 0.1731, 0.1541, 0.1120, 0.1683, 0.1597, 0.1433,\n",
      "        0.1411, 0.1600, 0.1648, 0.1458, 0.0691, 0.1479, 0.1435, 0.1359, 0.1417,\n",
      "        0.1684, 0.1502, 0.1412, 0.0746, 0.0838, 0.1035, 0.1650, 0.1343, 0.1304,\n",
      "        0.1603, 0.1442, 0.1296, 0.1488, 0.1410, 0.1329, 0.1570, 0.1415, 0.1476,\n",
      "        0.1386, 0.1635, 0.1384, 0.1657, 0.1471, 0.1381, 0.3939, 0.1477, 0.2232,\n",
      "        0.1925, 0.1498, 0.1424, 0.1634, 0.1456, 0.1607, 0.1658, 0.1604, 0.1504,\n",
      "        0.1577, 0.1439, 0.2339, 0.1361, 0.1578, 0.1555, 0.1606, 0.1477, 0.1535,\n",
      "        0.1663, 0.1477, 0.1458, 0.1674, 0.1564, 0.1458, 0.1396, 0.1130, 0.1422,\n",
      "        0.1501, 0.1472, 0.1311, 0.1048, 0.1534, 0.1636, 0.1635, 0.1522, 0.1494,\n",
      "        0.1596, 0.1464, 0.1518, 0.1440, 0.1592, 0.1725, 0.1539, 0.1721, 0.1506,\n",
      "        0.1533, 0.1492, 0.1580, 0.1121, 0.1297, 0.1561, 0.1569, 0.1560, 0.1675,\n",
      "        0.1699, 0.1327, 0.3077, 0.1688, 0.1723, 0.1552, 0.1499, 0.1608, 0.1473,\n",
      "        0.1388, 0.1563, 0.1760, 0.1603, 0.1498, 0.1409, 0.1393, 0.1692, 0.1368,\n",
      "        0.1551, 0.1493, 0.1655, 0.1754, 0.1919, 0.1537, 0.1497, 0.1491, 0.1498,\n",
      "        0.1538, 0.1580, 0.1375, 0.1798, 0.1624, 0.1647, 0.1441, 0.1623, 0.1468,\n",
      "        0.1395, 0.1452, 0.1525, 0.1407, 0.1637, 0.1559, 0.1404, 0.1526, 0.1458,\n",
      "        0.1392, 0.1404, 0.1548, 0.1635, 0.1563, 0.1596, 0.1591, 0.1795, 0.1470,\n",
      "        0.1366, 0.1460, 0.1555, 0.1656, 0.1684, 0.1256, 0.1630, 0.1163, 0.1421,\n",
      "        0.1590, 0.1442, 0.1716, 0.1434, 0.2282, 0.1457, 0.1457, 0.0721, 0.1168,\n",
      "        0.1371, 0.1591, 0.1682, 0.1576, 0.1628, 0.1382, 0.1446, 0.1775, 0.1482,\n",
      "        0.1058, 0.1487, 0.1643, 0.0799, 0.1516, 0.1785, 0.1334, 0.1460, 0.1515,\n",
      "        0.1564, 0.1517, 0.1788, 0.1665, 0.1458, 0.1977, 0.1631, 0.2119, 0.1681,\n",
      "        0.1578, 0.1772, 0.0992, 0.1575, 0.1566, 0.1458, 0.1227, 0.1444, 0.1472,\n",
      "        0.0600, 0.1560, 0.1415, 0.1477, 0.1552, 0.1662, 0.1447, 0.1546, 0.1494,\n",
      "        0.1431, 0.1396, 0.1452, 0.1286, 0.1704, 0.1499, 0.1399, 0.1637, 0.2043,\n",
      "        0.1020, 0.1921, 0.1449, 0.1609, 0.1615, 0.1413, 0.1594, 0.1667, 0.1479,\n",
      "        0.1214, 0.1059, 0.1598, 0.1459, 0.1831, 0.1417, 0.1706, 0.1504, 0.1679,\n",
      "        0.1619, 0.1494, 0.1602, 0.0627, 0.1412, 0.1406, 0.1405, 0.1542, 0.1465,\n",
      "        0.1608, 0.1628, 0.1510, 0.1482, 0.1544, 0.1461, 0.1506, 0.1223, 0.1497,\n",
      "        0.1587, 0.1552, 0.1472, 0.1465, 0.1449, 0.1659, 0.1436, 0.1662, 0.1669,\n",
      "        0.1354, 0.1632, 0.1544, 0.1343, 0.1332, 0.1387, 0.0782, 0.1426, 0.1354,\n",
      "        0.1445, 0.1266, 0.1551, 0.1111, 0.0511, 0.1440, 0.1534, 0.1410, 0.1467,\n",
      "        0.1614, 0.1544, 0.1491, 0.1589, 0.1513, 0.1458, 0.1394, 0.1353, 0.1581,\n",
      "        0.1613, 0.1366, 0.1345, 0.1577, 0.1037, 0.1617, 0.1571, 0.1460, 0.1329,\n",
      "        0.1623, 0.1511, 0.1389, 0.1538, 0.1478, 0.1496, 0.1444, 0.1227, 0.1533,\n",
      "        0.1239, 0.1618, 0.1445, 0.1476, 0.1274, 0.1479, 0.1366, 0.1496, 0.1749,\n",
      "        0.2237, 0.1440, 0.1391, 0.1565, 0.1430, 0.1572, 0.1500, 0.1598, 0.1619,\n",
      "        0.1521, 0.1458, 0.1380, 0.1478, 0.1800, 0.1555, 0.1497, 0.1243, 0.1495,\n",
      "        0.1471, 0.1456, 0.1425, 0.1506, 0.1440, 0.1765, 0.1678, 0.1611, 0.1328,\n",
      "        0.1732, 0.1429, 0.0756, 0.1385, 0.1413, 0.1419, 0.1478, 0.1520, 0.1634,\n",
      "        0.1605, 0.1401, 0.1570, 0.1562, 0.1366, 0.1519, 0.1416, 0.1371, 0.1561,\n",
      "        0.1441, 0.1307, 0.1633, 0.1517, 0.1544, 0.1544, 0.1309, 0.1372, 0.1755,\n",
      "        0.1476, 0.1394, 0.1461], device='cuda:0', requires_grad=True)\n",
      "layers.0.attention.wq.weight Parameter containing:\n",
      "tensor([[-0.0551, -0.0593, -0.1306,  ...,  0.2355, -0.0548,  0.1205],\n",
      "        [ 0.0241, -0.0035,  0.0304,  ..., -0.0456, -0.0476, -0.1020],\n",
      "        [-0.0038, -0.2222,  0.0507,  ..., -0.1124, -0.1274, -0.0798],\n",
      "        ...,\n",
      "        [-0.0656,  0.0284, -0.0028,  ..., -0.0639,  0.1866,  0.0498],\n",
      "        [ 0.1778,  0.1131, -0.0553,  ..., -0.0540, -0.2253,  0.0636],\n",
      "        [ 0.0712, -0.2124,  0.0278,  ...,  0.0931, -0.0334, -0.1310]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.attention.wk.weight Parameter containing:\n",
      "tensor([[ 0.0007, -0.1292,  0.0830,  ..., -0.1861,  0.0677, -0.1240],\n",
      "        [ 0.0051, -0.0324, -0.3030,  ...,  0.0382, -0.0740, -0.1158],\n",
      "        [-0.0672, -0.1494, -0.0731,  ...,  0.2241,  0.1088,  0.2096],\n",
      "        ...,\n",
      "        [ 0.1442,  0.0461, -0.1482,  ..., -0.0629,  0.0396,  0.0820],\n",
      "        [ 0.0634, -0.1461,  0.0382,  ..., -0.0179,  0.1379,  0.1750],\n",
      "        [ 0.1158,  0.0560, -0.3389,  ...,  0.1189, -0.0229,  0.0009]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.attention.wv.weight Parameter containing:\n",
      "tensor([[-0.0572, -0.1344,  0.1861,  ..., -0.1495, -0.0052,  0.1468],\n",
      "        [-0.1893, -0.0977, -0.0130,  ..., -0.1071,  0.0925,  0.1401],\n",
      "        [ 0.0674,  0.0695,  0.0651,  ..., -0.1825, -0.1683,  0.0416],\n",
      "        ...,\n",
      "        [-0.1240,  0.1658,  0.1551,  ...,  0.1439, -0.1029, -0.0175],\n",
      "        [-0.0627, -0.0738,  0.1329,  ...,  0.0110, -0.0076,  0.0265],\n",
      "        [ 0.0792, -0.0127, -0.0983,  ..., -0.0745,  0.0178, -0.1366]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.attention.wo.weight Parameter containing:\n",
      "tensor([[ 0.1477, -0.0041,  0.0380,  ...,  0.1025,  0.1192, -0.0300],\n",
      "        [-0.2204, -0.1099,  0.0147,  ...,  0.0032,  0.0099,  0.0322],\n",
      "        [-0.0062,  0.1207,  0.0595,  ...,  0.0968,  0.0691, -0.0202],\n",
      "        ...,\n",
      "        [-0.0887,  0.0440,  0.0710,  ..., -0.0938,  0.0323, -0.1230],\n",
      "        [-0.2015,  0.1403,  0.0246,  ..., -0.0833,  0.0048, -0.0041],\n",
      "        [-0.1747,  0.0232,  0.0789,  ...,  0.0177, -0.0687,  0.0760]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.mlp.linear1.weight Parameter containing:\n",
      "tensor([[ 0.0441,  0.0462, -0.0586,  ...,  0.1517, -0.0984,  0.0803],\n",
      "        [ 0.0068, -0.0558, -0.1333,  ...,  0.2266,  0.2071, -0.0997],\n",
      "        [ 0.2049, -0.1663, -0.1163,  ..., -0.0251,  0.0073, -0.1171],\n",
      "        ...,\n",
      "        [ 0.0753,  0.0039, -0.0828,  ..., -0.0331,  0.1213,  0.1703],\n",
      "        [-0.1464,  0.1243, -0.1429,  ..., -0.0959,  0.1593, -0.1022],\n",
      "        [ 0.0783,  0.1282,  0.0649,  ..., -0.0118, -0.1107,  0.0387]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.mlp.linear2.weight Parameter containing:\n",
      "tensor([[-0.0497, -0.1696, -0.0143,  ...,  0.1835, -0.0682, -0.0095],\n",
      "        [-0.1770,  0.0919, -0.0629,  ...,  0.0694,  0.2327,  0.2268],\n",
      "        [ 0.2094,  0.0570,  0.0518,  ...,  0.0558, -0.0627, -0.0608],\n",
      "        ...,\n",
      "        [ 0.0498,  0.0016, -0.0087,  ...,  0.3232,  0.0332,  0.0716],\n",
      "        [ 0.0017, -0.1073, -0.1780,  ..., -0.0317,  0.0497, -0.1731],\n",
      "        [-0.0161, -0.1539, -0.1072,  ...,  0.0255,  0.0506,  0.0321]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.mlp.linear3.weight Parameter containing:\n",
      "tensor([[-0.0561,  0.0681,  0.0065,  ...,  0.1484, -0.1803, -0.0315],\n",
      "        [ 0.1500,  0.0303,  0.1778,  ..., -0.1802, -0.1297, -0.2744],\n",
      "        [ 0.0206, -0.2514, -0.0624,  ..., -0.0466, -0.3480, -0.1530],\n",
      "        ...,\n",
      "        [-0.0707,  0.0650, -0.1907,  ..., -0.2596,  0.0994, -0.1495],\n",
      "        [ 0.0708,  0.0912, -0.0617,  ..., -0.0138, -0.0141, -0.1425],\n",
      "        [-0.1456,  0.0951,  0.0015,  ...,  0.1158,  0.0522,  0.1500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.norms.w Parameter containing:\n",
      "tensor([0.2890, 0.3042, 0.2817, 0.2800, 0.2714, 0.2903, 0.2785, 0.2123, 0.2682,\n",
      "        0.2785, 0.1378, 0.2792, 0.2558, 0.2789, 0.2636, 0.2707, 0.2533, 0.2502,\n",
      "        0.2854, 0.2686, 0.2493, 0.2924, 0.2835, 0.2469, 0.2573, 0.2872, 0.2709,\n",
      "        0.2925, 0.2632, 0.2926, 0.3051, 0.2544, 0.2855, 0.2767, 0.2652, 0.2200,\n",
      "        0.2713, 0.2950, 0.2717, 0.2668, 0.2810, 0.2668, 0.2835, 0.3030, 0.2261,\n",
      "        0.3015, 0.2528, 0.2913, 0.2587, 0.2649, 0.2402, 0.2810, 0.2647, 0.2750,\n",
      "        0.2692, 0.2454, 0.2557, 0.2673, 0.2768, 0.2874, 0.2896, 0.2530, 0.2867,\n",
      "        0.2392, 0.2806, 0.2578, 0.2827, 0.2533, 0.2386, 0.2592, 0.2790, 0.2638,\n",
      "        0.2772, 0.2766, 0.2731, 0.2720, 0.2527, 0.2940, 0.1389, 0.2451, 0.2794,\n",
      "        0.2441, 0.3029, 0.2709, 0.2690, 0.2679, 0.2799, 0.2706, 0.2782, 0.2547,\n",
      "        0.2555, 0.2060, 0.2858, 0.2664, 0.2870, 0.2634, 0.2164, 0.2544, 0.2612,\n",
      "        0.2742, 0.2541, 0.2692, 0.3028, 0.2674, 0.2763, 0.2552, 0.2777, 0.1942,\n",
      "        0.2702, 0.2828, 0.2744, 0.2808, 0.2977, 0.2771, 0.2582, 0.2697, 0.2622,\n",
      "        0.2466, 0.2505, 0.2748, 0.2406, 0.2835, 0.2707, 0.2659, 0.0966, 0.2618,\n",
      "        0.2613, 0.3030, 0.2747, 0.2784, 0.2794, 0.2650, 0.2571, 0.2652, 0.2765,\n",
      "        0.2662, 0.2569, 0.2644, 0.2823, 0.2875, 0.2785, 0.2985, 0.3080, 0.2704,\n",
      "        0.2733, 0.2438, 0.2583, 0.2677, 0.2747, 0.2634, 0.2976, 0.2622, 0.2707,\n",
      "        0.2773, 0.2682, 0.3006, 0.2847, 0.1893, 0.2801, 0.2655, 0.2659, 0.2616,\n",
      "        0.2973, 0.2497, 0.2486, 0.2397, 0.2687, 0.2634, 0.2738, 0.2709, 0.1297,\n",
      "        0.2760, 0.2730, 0.2875, 0.2716, 0.2839, 0.2985, 0.2705, 0.2724, 0.2765,\n",
      "        0.2578, 0.2780, 0.2973, 0.2574, 0.2569, 0.2766, 0.2857, 0.2801, 0.2797,\n",
      "        0.2765, 0.2810, 0.2590, 0.2682, 0.2777, 0.2689, 0.2630, 0.2723, 0.2844,\n",
      "        0.2448, 0.1998, 0.2808, 0.2740, 0.2775, 0.2581, 0.2772, 0.2455, 0.2608,\n",
      "        0.2399, 0.2656, 0.2578, 0.2838, 0.2902, 0.2697, 0.2700, 0.2780, 0.3003,\n",
      "        0.2782, 0.2437, 0.2421, 0.2866, 0.2749, 0.2777, 0.2859, 0.2613, 0.2693,\n",
      "        0.2679, 0.2615, 0.2999, 0.2741, 0.2963, 0.2513, 0.2623, 0.2488, 0.2523,\n",
      "        0.2633, 0.2433, 0.2836, 0.2768, 0.2361, 0.2242, 0.2643, 0.2803, 0.2724,\n",
      "        0.2774, 0.2714, 0.2596, 0.2616, 0.2937, 0.2629, 0.2522, 0.2681, 0.2741,\n",
      "        0.2709, 0.2885, 0.2842, 0.2761, 0.2708, 0.2851, 0.2835, 0.2880, 0.2815,\n",
      "        0.2868, 0.2950, 0.2714, 0.2671, 0.2488, 0.2808, 0.2632, 0.2773, 0.2952,\n",
      "        0.2722, 0.2827, 0.2472, 0.2859, 0.2485, 0.2850, 0.2794, 0.2867, 0.2886,\n",
      "        0.2740, 0.2598, 0.2916, 0.2999, 0.2787, 0.2723, 0.2837, 0.2853, 0.2936,\n",
      "        0.2658, 0.2291, 0.2851, 0.2809, 0.2635, 0.2812, 0.2780, 0.2674, 0.2809,\n",
      "        0.2569, 0.2824, 0.2784, 0.2663, 0.2638, 0.2570, 0.2570, 0.2618, 0.2263,\n",
      "        0.2589, 0.2680, 0.2768, 0.2071, 0.2790, 0.2509, 0.2824, 0.2631, 0.2776,\n",
      "        0.2852, 0.2578, 0.2716, 0.2930, 0.2909, 0.2532, 0.2490, 0.2693, 0.2225,\n",
      "        0.2393, 0.2059, 0.2827, 0.2920, 0.2828, 0.2795, 0.2695, 0.2587, 0.2468,\n",
      "        0.2698, 0.2601, 0.2767, 0.2900, 0.3048, 0.2788, 0.2955, 0.2776, 0.2897,\n",
      "        0.2626, 0.2758, 0.2651, 0.2593, 0.1809, 0.1817, 0.2618, 0.2460, 0.2947,\n",
      "        0.2823, 0.2574, 0.2943, 0.2899, 0.2734, 0.2746, 0.2928, 0.2550, 0.2728,\n",
      "        0.2798, 0.2841, 0.2855, 0.2613, 0.2747, 0.2793, 0.2678, 0.2798, 0.2766,\n",
      "        0.2674, 0.2734, 0.2682, 0.2608, 0.2524, 0.2575, 0.2303, 0.2617, 0.2729,\n",
      "        0.2778, 0.2685, 0.2879, 0.2275, 0.2822, 0.2739, 0.2765, 0.2797, 0.2643,\n",
      "        0.1987, 0.2485, 0.2576, 0.2866, 0.2904, 0.2310, 0.2772, 0.2671, 0.2719,\n",
      "        0.2917, 0.2451, 0.2892, 0.2663, 0.2471, 0.2939, 0.2809, 0.2360, 0.2730,\n",
      "        0.2754, 0.2779, 0.2837, 0.1078, 0.2511, 0.2686, 0.2583, 0.2636, 0.2515,\n",
      "        0.2970, 0.2545, 0.2672, 0.2949, 0.2578, 0.2543, 0.2707, 0.2880, 0.2781,\n",
      "        0.2727, 0.2503, 0.2548, 0.3076, 0.2533, 0.2720, 0.2620, 0.2740, 0.2856,\n",
      "        0.2557, 0.2727, 0.2750, 0.2650, 0.2635, 0.2901, 0.2757, 0.2749, 0.2631,\n",
      "        0.2702, 0.2955, 0.2106, 0.2700, 0.2878, 0.2543, 0.2841, 0.2874, 0.2882,\n",
      "        0.2826, 0.2552, 0.2535, 0.2640, 0.2676, 0.2703, 0.2338, 0.2645, 0.2588,\n",
      "        0.2696, 0.2863, 0.2701, 0.1747, 0.2681, 0.2817, 0.2611, 0.2689, 0.2939,\n",
      "        0.2752, 0.2721, 0.2664, 0.2630, 0.2748, 0.2511, 0.2917, 0.2833, 0.2778,\n",
      "        0.2532, 0.2763, 0.2895, 0.1850, 0.2877, 0.2670, 0.2815, 0.2655, 0.2747,\n",
      "        0.2868, 0.2785, 0.2554, 0.2312, 0.2894, 0.2623, 0.2465, 0.2625, 0.2831,\n",
      "        0.2501, 0.2613, 0.2834, 0.2806, 0.2675, 0.2674, 0.2849, 0.2828, 0.2458,\n",
      "        0.2470, 0.2777, 0.2662, 0.2793, 0.2862, 0.2463, 0.2904, 0.3075, 0.2800,\n",
      "        0.2719, 0.2359, 0.2745, 0.2753, 0.2415, 0.2953, 0.2778, 0.2809, 0.2780,\n",
      "        0.2837, 0.2767, 0.2940, 0.3127, 0.2676, 0.2554, 0.2258, 0.2737, 0.2875,\n",
      "        0.2522, 0.2738, 0.3011, 0.2851, 0.2625, 0.2743, 0.2526, 0.2668, 0.2639,\n",
      "        0.2692, 0.2692, 0.2712, 0.2400, 0.2791, 0.2773, 0.2960, 0.2371, 0.2813,\n",
      "        0.2718, 0.2766, 0.2772, 0.2769, 0.2438, 0.2825, 0.2796, 0.1482, 0.2473,\n",
      "        0.2379, 0.2763, 0.2815, 0.2750, 0.2785, 0.2624, 0.2599, 0.2778, 0.2693,\n",
      "        0.2679, 0.2437, 0.2643, 0.1363, 0.2706, 0.2681, 0.2557, 0.2665, 0.2913,\n",
      "        0.2749, 0.2857, 0.2774, 0.2509, 0.2518, 0.2790, 0.2908, 0.2904, 0.2667,\n",
      "        0.2829, 0.2740, 0.1998, 0.2635, 0.2794, 0.2931, 0.2417, 0.2770, 0.2774,\n",
      "        0.0839, 0.2610, 0.2917, 0.2645, 0.2778, 0.2551, 0.2796, 0.2798, 0.2714,\n",
      "        0.2803, 0.2691, 0.2646, 0.2637, 0.2783, 0.2602, 0.2671, 0.3042, 0.2314,\n",
      "        0.2578, 0.2903, 0.2605, 0.2697, 0.2718, 0.2829, 0.2801, 0.2865, 0.2741,\n",
      "        0.2576, 0.2099, 0.2656, 0.2773, 0.2666, 0.2803, 0.2645, 0.2828, 0.2892,\n",
      "        0.2719, 0.2926, 0.2785, 0.1256, 0.2840, 0.2936, 0.2634, 0.2812, 0.2606,\n",
      "        0.2723, 0.2679, 0.2619, 0.2850, 0.2888, 0.2646, 0.2768, 0.2690, 0.2782,\n",
      "        0.2669, 0.2826, 0.2952, 0.2641, 0.2487, 0.2782, 0.2521, 0.2718, 0.2754,\n",
      "        0.2788, 0.2889, 0.2818, 0.2641, 0.2278, 0.2814, 0.2198, 0.2779, 0.2363,\n",
      "        0.2722, 0.2721, 0.2741, 0.2554, 0.1003, 0.2871, 0.2746, 0.2758, 0.2525,\n",
      "        0.2966, 0.2854, 0.2766, 0.2717, 0.2700, 0.2918, 0.2443, 0.2651, 0.2888,\n",
      "        0.2841, 0.2282, 0.2229, 0.2970, 0.2668, 0.2696, 0.2708, 0.2617, 0.2412,\n",
      "        0.2884, 0.2834, 0.2828, 0.2774, 0.2721, 0.2876, 0.2988, 0.2102, 0.2791,\n",
      "        0.2157, 0.2682, 0.2799, 0.2656, 0.1930, 0.3027, 0.2592, 0.2667, 0.2763,\n",
      "        0.1664, 0.2459, 0.2731, 0.2792, 0.2808, 0.2858, 0.2766, 0.2654, 0.2716,\n",
      "        0.2775, 0.2650, 0.2707, 0.2562, 0.2886, 0.2641, 0.2707, 0.2348, 0.2639,\n",
      "        0.2665, 0.2746, 0.2478, 0.2771, 0.2653, 0.2917, 0.2686, 0.2783, 0.2665,\n",
      "        0.2865, 0.2503, 0.2168, 0.2759, 0.2874, 0.2515, 0.2351, 0.2909, 0.2997,\n",
      "        0.2819, 0.2802, 0.2906, 0.2656, 0.2960, 0.2719, 0.2487, 0.2515, 0.2793,\n",
      "        0.2721, 0.2751, 0.2754, 0.2810, 0.2791, 0.2741, 0.2417, 0.2720, 0.2736,\n",
      "        0.2792, 0.2768, 0.2662], device='cuda:0', requires_grad=True)\n",
      "layers.1.attention.wq.weight Parameter containing:\n",
      "tensor([[ 0.1369, -0.1419, -0.0113,  ...,  0.2205, -0.0234, -0.1364],\n",
      "        [ 0.0080, -0.0304, -0.1253,  ..., -0.0391, -0.0306,  0.1968],\n",
      "        [-0.0944, -0.0664,  0.1627,  ..., -0.2257,  0.0173,  0.0853],\n",
      "        ...,\n",
      "        [ 0.0113,  0.0865, -0.0198,  ..., -0.3552, -0.0874,  0.0502],\n",
      "        [-0.1678, -0.0015, -0.1251,  ...,  0.2123, -0.0816,  0.2691],\n",
      "        [ 0.0979, -0.1636, -0.1125,  ..., -0.0933, -0.2849,  0.0488]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.attention.wk.weight Parameter containing:\n",
      "tensor([[-0.1871,  0.0796, -0.0178,  ..., -0.0124, -0.1618,  0.1359],\n",
      "        [-0.1277,  0.0360,  0.0576,  ..., -0.0500,  0.0005, -0.1508],\n",
      "        [ 0.0738, -0.0212,  0.0147,  ...,  0.2061, -0.1251,  0.0892],\n",
      "        ...,\n",
      "        [-0.1025, -0.1813, -0.2514,  ...,  0.1687,  0.2681,  0.0587],\n",
      "        [ 0.0783,  0.1612,  0.1496,  ..., -0.2184,  0.1823,  0.1784],\n",
      "        [ 0.1816, -0.2565, -0.2003,  ..., -0.3711, -0.1011, -0.0693]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.attention.wv.weight Parameter containing:\n",
      "tensor([[-0.0689,  0.0777, -0.0962,  ...,  0.0503, -0.0828,  0.0469],\n",
      "        [-0.0411, -0.1447, -0.1401,  ...,  0.0064, -0.0507, -0.0018],\n",
      "        [-0.1799,  0.0799,  0.2193,  ..., -0.0088,  0.0309,  0.1255],\n",
      "        ...,\n",
      "        [ 0.0446, -0.0114, -0.2039,  ..., -0.1325,  0.1646, -0.0418],\n",
      "        [ 0.0073, -0.0789,  0.0992,  ...,  0.1284, -0.0164, -0.1066],\n",
      "        [ 0.1950,  0.1904, -0.0572,  ...,  0.2185, -0.1099,  0.0103]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.attention.wo.weight Parameter containing:\n",
      "tensor([[ 0.1331, -0.0883,  0.0739,  ..., -0.1119, -0.0774, -0.1266],\n",
      "        [-0.0693, -0.0527,  0.1128,  ..., -0.0898, -0.0664,  0.1276],\n",
      "        [-0.1165, -0.0832, -0.1902,  ...,  0.0546,  0.0207, -0.2242],\n",
      "        ...,\n",
      "        [-0.1302, -0.2030,  0.0109,  ...,  0.0119,  0.0361,  0.0425],\n",
      "        [ 0.0938,  0.0610, -0.1496,  ...,  0.0697,  0.0950, -0.2519],\n",
      "        [-0.2006, -0.0442, -0.0448,  ..., -0.0501, -0.0107, -0.1829]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.mlp.linear1.weight Parameter containing:\n",
      "tensor([[-0.1858,  0.2177, -0.0458,  ...,  0.1702, -0.0319,  0.1028],\n",
      "        [-0.1043,  0.0202, -0.1004,  ...,  0.0343,  0.0233,  0.0613],\n",
      "        [-0.1955,  0.2188, -0.3641,  ...,  0.2358,  0.0724, -0.1500],\n",
      "        ...,\n",
      "        [-0.2052, -0.1964,  0.1330,  ..., -0.0700,  0.0561,  0.0090],\n",
      "        [-0.1125, -0.2048, -0.1133,  ...,  0.0447, -0.1842, -0.0376],\n",
      "        [-0.1214, -0.0395, -0.0477,  ...,  0.2699, -0.0065,  0.0576]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.mlp.linear2.weight Parameter containing:\n",
      "tensor([[-0.2334, -0.1868,  0.1181,  ..., -0.1662,  0.1849, -0.0208],\n",
      "        [ 0.1220, -0.0511, -0.1770,  ...,  0.1839,  0.0520, -0.0370],\n",
      "        [ 0.0593,  0.0096,  0.0962,  ...,  0.0636,  0.0039, -0.0657],\n",
      "        ...,\n",
      "        [-0.0742,  0.0545, -0.1524,  ...,  0.1059,  0.0163,  0.0921],\n",
      "        [ 0.0588, -0.0212,  0.1160,  ..., -0.1412,  0.0721, -0.3955],\n",
      "        [ 0.0789, -0.1111, -0.1110,  ...,  0.1779, -0.1033,  0.0979]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.mlp.linear3.weight Parameter containing:\n",
      "tensor([[ 0.0292,  0.1352,  0.1185,  ..., -0.0189,  0.0435,  0.0733],\n",
      "        [-0.0382,  0.0035, -0.0651,  ..., -0.1545,  0.1982,  0.0842],\n",
      "        [-0.0396,  0.1614,  0.0234,  ...,  0.0147,  0.0093,  0.0690],\n",
      "        ...,\n",
      "        [-0.1450,  0.3705, -0.1222,  ...,  0.2533, -0.2011,  0.0422],\n",
      "        [ 0.3139, -0.3989, -0.2639,  ...,  0.0691,  0.0010, -0.0301],\n",
      "        [ 0.0235,  0.1453, -0.0628,  ...,  0.0810,  0.1300, -0.1584]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.2.norms.w Parameter containing:\n",
      "tensor([0.3514, 0.3689, 0.3648, 0.3450, 0.3452, 0.3550, 0.3553, 0.2230, 0.3552,\n",
      "        0.3336, 0.2148, 0.3586, 0.3496, 0.3449, 0.3657, 0.3475, 0.3439, 0.3233,\n",
      "        0.3354, 0.3400, 0.3251, 0.3547, 0.3510, 0.3403, 0.3301, 0.3473, 0.3707,\n",
      "        0.3750, 0.3203, 0.3631, 0.3698, 0.3418, 0.3469, 0.3169, 0.3592, 0.2698,\n",
      "        0.3693, 0.3473, 0.3476, 0.3410, 0.3439, 0.3408, 0.3434, 0.3439, 0.2691,\n",
      "        0.3528, 0.3648, 0.3407, 0.3635, 0.3227, 0.2992, 0.3490, 0.3373, 0.3366,\n",
      "        0.3576, 0.3150, 0.3376, 0.3580, 0.3564, 0.3551, 0.3359, 0.3343, 0.3588,\n",
      "        0.3472, 0.3376, 0.3274, 0.3443, 0.3299, 0.3302, 0.3337, 0.3262, 0.3435,\n",
      "        0.3622, 0.3308, 0.3524, 0.3379, 0.3310, 0.3413, 0.2134, 0.3252, 0.3672,\n",
      "        0.3109, 0.3738, 0.3452, 0.3470, 0.3501, 0.3239, 0.3474, 0.3390, 0.3335,\n",
      "        0.3489, 0.2814, 0.3473, 0.3461, 0.3543, 0.3635, 0.2864, 0.3545, 0.3450,\n",
      "        0.3720, 0.3535, 0.3481, 0.3702, 0.3597, 0.3592, 0.3162, 0.3386, 0.2799,\n",
      "        0.3696, 0.3539, 0.3343, 0.3689, 0.3661, 0.3319, 0.3224, 0.3313, 0.3441,\n",
      "        0.3379, 0.3197, 0.3612, 0.3332, 0.3369, 0.3517, 0.3409, 0.1900, 0.3528,\n",
      "        0.3170, 0.3452, 0.3548, 0.3323, 0.3406, 0.3444, 0.3415, 0.3502, 0.3336,\n",
      "        0.3260, 0.3276, 0.3361, 0.3661, 0.3667, 0.3480, 0.3564, 0.3481, 0.3417,\n",
      "        0.3435, 0.3240, 0.3431, 0.3283, 0.3428, 0.3390, 0.3473, 0.3314, 0.3329,\n",
      "        0.3495, 0.3460, 0.3711, 0.3745, 0.2961, 0.3497, 0.3552, 0.3615, 0.3336,\n",
      "        0.3711, 0.3219, 0.3386, 0.3222, 0.3302, 0.3570, 0.3598, 0.3639, 0.1696,\n",
      "        0.3737, 0.3571, 0.3412, 0.3603, 0.3517, 0.3585, 0.3548, 0.3438, 0.3433,\n",
      "        0.3370, 0.3638, 0.3649, 0.3529, 0.3457, 0.3581, 0.3462, 0.3731, 0.3231,\n",
      "        0.3314, 0.3496, 0.3234, 0.3309, 0.3412, 0.3476, 0.3373, 0.3517, 0.3423,\n",
      "        0.3339, 0.3140, 0.3643, 0.3482, 0.3374, 0.3487, 0.3634, 0.3386, 0.3438,\n",
      "        0.3311, 0.3444, 0.3405, 0.3559, 0.3636, 0.3412, 0.3442, 0.3726, 0.3657,\n",
      "        0.3465, 0.3410, 0.3433, 0.3624, 0.3581, 0.3398, 0.3502, 0.3648, 0.3547,\n",
      "        0.3373, 0.3651, 0.3495, 0.3601, 0.3606, 0.3402, 0.3532, 0.3317, 0.3362,\n",
      "        0.3524, 0.3334, 0.3872, 0.3531, 0.3318, 0.2935, 0.3464, 0.3466, 0.3504,\n",
      "        0.3469, 0.3265, 0.3496, 0.3758, 0.3567, 0.3365, 0.3467, 0.3574, 0.3402,\n",
      "        0.3524, 0.3590, 0.3552, 0.3415, 0.3538, 0.3671, 0.3710, 0.3585, 0.3711,\n",
      "        0.3583, 0.3541, 0.3396, 0.3359, 0.3170, 0.3585, 0.3316, 0.3562, 0.3558,\n",
      "        0.3451, 0.3656, 0.3196, 0.3456, 0.3123, 0.3519, 0.3483, 0.3399, 0.3486,\n",
      "        0.3679, 0.3460, 0.3834, 0.3776, 0.3591, 0.3436, 0.3493, 0.3208, 0.3560,\n",
      "        0.3444, 0.3019, 0.3430, 0.3522, 0.3421, 0.3258, 0.3419, 0.3409, 0.3625,\n",
      "        0.3331, 0.3409, 0.3693, 0.3239, 0.3493, 0.3541, 0.3190, 0.3417, 0.3227,\n",
      "        0.3283, 0.3534, 0.3487, 0.2883, 0.3552, 0.3362, 0.3352, 0.3473, 0.3421,\n",
      "        0.3681, 0.3450, 0.3493, 0.3565, 0.3251, 0.3404, 0.3399, 0.3636, 0.3018,\n",
      "        0.3048, 0.3008, 0.3615, 0.3396, 0.3534, 0.3674, 0.3406, 0.3498, 0.3178,\n",
      "        0.3542, 0.3519, 0.3400, 0.3639, 0.3523, 0.3609, 0.3331, 0.3512, 0.3577,\n",
      "        0.3265, 0.3524, 0.3518, 0.3508, 0.2409, 0.2525, 0.3290, 0.3157, 0.3718,\n",
      "        0.3528, 0.3477, 0.3601, 0.3586, 0.3577, 0.3496, 0.3504, 0.3249, 0.3227,\n",
      "        0.3549, 0.3495, 0.3661, 0.3189, 0.3506, 0.3714, 0.3283, 0.3501, 0.3537,\n",
      "        0.3552, 0.3491, 0.3606, 0.3575, 0.3569, 0.3386, 0.3185, 0.3316, 0.3525,\n",
      "        0.3525, 0.3571, 0.3539, 0.3304, 0.3523, 0.3608, 0.3470, 0.3408, 0.3306,\n",
      "        0.3058, 0.3405, 0.3432, 0.3545, 0.3456, 0.3205, 0.3556, 0.3396, 0.3182,\n",
      "        0.3394, 0.3263, 0.3549, 0.3321, 0.3135, 0.3605, 0.3265, 0.3061, 0.3369,\n",
      "        0.3274, 0.3486, 0.3525, 0.1861, 0.3169, 0.3526, 0.3439, 0.3472, 0.3488,\n",
      "        0.3678, 0.3256, 0.3432, 0.3697, 0.3235, 0.3392, 0.3589, 0.3362, 0.3498,\n",
      "        0.3434, 0.3267, 0.3236, 0.3591, 0.3384, 0.3563, 0.3491, 0.3485, 0.3469,\n",
      "        0.3513, 0.3568, 0.3472, 0.3451, 0.3516, 0.3478, 0.3554, 0.3471, 0.3529,\n",
      "        0.3431, 0.3612, 0.2345, 0.3364, 0.3592, 0.3502, 0.3495, 0.3456, 0.3584,\n",
      "        0.3413, 0.3602, 0.3391, 0.3295, 0.3415, 0.3535, 0.3410, 0.3446, 0.3294,\n",
      "        0.3531, 0.3470, 0.3483, 0.2735, 0.3446, 0.3533, 0.3358, 0.3472, 0.3650,\n",
      "        0.3505, 0.3478, 0.3456, 0.3391, 0.3553, 0.3318, 0.3706, 0.3094, 0.3549,\n",
      "        0.3081, 0.3456, 0.3522, 0.2677, 0.3655, 0.3608, 0.3197, 0.3398, 0.3671,\n",
      "        0.3531, 0.3356, 0.3377, 0.3433, 0.3704, 0.3495, 0.3233, 0.3616, 0.3343,\n",
      "        0.3145, 0.3598, 0.3561, 0.3517, 0.3554, 0.3444, 0.3558, 0.3663, 0.3360,\n",
      "        0.3438, 0.3679, 0.3479, 0.3576, 0.3620, 0.3431, 0.3694, 0.3752, 0.3613,\n",
      "        0.3296, 0.3257, 0.3587, 0.3390, 0.3275, 0.3450, 0.3586, 0.3477, 0.3539,\n",
      "        0.3460, 0.3595, 0.3557, 0.3715, 0.3399, 0.3300, 0.2996, 0.3609, 0.3489,\n",
      "        0.3259, 0.3512, 0.3730, 0.3791, 0.3483, 0.3542, 0.3605, 0.3442, 0.3559,\n",
      "        0.3278, 0.3510, 0.3443, 0.3280, 0.3484, 0.3437, 0.3496, 0.3410, 0.3509,\n",
      "        0.3670, 0.3300, 0.3645, 0.3505, 0.3428, 0.3713, 0.3539, 0.1483, 0.3309,\n",
      "        0.3234, 0.3688, 0.3618, 0.3551, 0.3490, 0.3638, 0.3165, 0.3594, 0.3298,\n",
      "        0.3328, 0.3269, 0.3371, 0.2317, 0.3377, 0.3700, 0.3386, 0.3338, 0.3536,\n",
      "        0.3591, 0.3403, 0.3719, 0.3281, 0.3513, 0.3617, 0.3582, 0.3647, 0.3486,\n",
      "        0.3435, 0.3647, 0.2859, 0.3483, 0.3503, 0.3489, 0.3380, 0.3496, 0.3852,\n",
      "        0.1215, 0.3025, 0.3532, 0.3440, 0.3550, 0.3524, 0.3469, 0.3703, 0.3671,\n",
      "        0.3596, 0.3495, 0.3331, 0.3466, 0.3490, 0.3293, 0.3570, 0.3558, 0.3092,\n",
      "        0.3406, 0.3525, 0.3297, 0.3488, 0.3605, 0.3475, 0.3525, 0.3519, 0.3559,\n",
      "        0.3314, 0.3209, 0.3529, 0.3576, 0.3456, 0.3554, 0.3371, 0.3497, 0.3723,\n",
      "        0.3557, 0.3687, 0.3292, 0.1157, 0.3616, 0.3416, 0.3522, 0.3535, 0.3560,\n",
      "        0.3659, 0.3432, 0.3541, 0.3469, 0.3479, 0.3414, 0.3647, 0.3362, 0.3628,\n",
      "        0.3568, 0.3524, 0.3571, 0.3595, 0.3249, 0.3536, 0.3241, 0.3540, 0.3484,\n",
      "        0.3499, 0.3598, 0.3636, 0.3374, 0.3402, 0.3513, 0.3038, 0.3530, 0.3112,\n",
      "        0.3691, 0.3363, 0.3453, 0.3319, 0.1782, 0.3572, 0.3436, 0.3393, 0.3493,\n",
      "        0.3627, 0.3600, 0.3581, 0.3471, 0.3491, 0.3770, 0.3234, 0.3309, 0.3475,\n",
      "        0.3574, 0.3037, 0.3180, 0.3590, 0.3351, 0.3436, 0.3611, 0.3469, 0.3070,\n",
      "        0.3343, 0.3562, 0.3677, 0.3717, 0.3505, 0.3671, 0.3541, 0.3074, 0.3483,\n",
      "        0.2940, 0.3316, 0.3362, 0.3366, 0.2638, 0.3477, 0.3501, 0.3511, 0.3586,\n",
      "        0.2324, 0.3344, 0.3579, 0.3504, 0.3723, 0.3558, 0.3313, 0.3422, 0.3407,\n",
      "        0.3487, 0.3349, 0.3428, 0.3515, 0.3456, 0.3461, 0.3456, 0.3243, 0.3447,\n",
      "        0.3513, 0.3665, 0.3429, 0.3687, 0.3853, 0.3488, 0.3560, 0.3345, 0.3408,\n",
      "        0.3562, 0.3425, 0.2974, 0.3590, 0.3374, 0.3252, 0.3384, 0.3637, 0.3610,\n",
      "        0.3571, 0.3562, 0.3827, 0.3412, 0.3436, 0.3361, 0.3401, 0.3340, 0.3674,\n",
      "        0.3419, 0.3526, 0.3450, 0.3570, 0.3591, 0.3483, 0.3351, 0.3625, 0.3599,\n",
      "        0.3405, 0.3652, 0.3385], device='cuda:0', requires_grad=True)\n",
      "layers.2.attention.wq.weight Parameter containing:\n",
      "tensor([[ 0.0159, -0.0399,  0.1447,  ..., -0.0047, -0.0293,  0.0306],\n",
      "        [ 0.1200, -0.1233,  0.0817,  ..., -0.1196,  0.0233,  0.0101],\n",
      "        [ 0.1176,  0.0308, -0.1056,  ...,  0.0974, -0.0524,  0.0713],\n",
      "        ...,\n",
      "        [-0.0556,  0.0216,  0.1317,  ..., -0.1764,  0.1099,  0.0802],\n",
      "        [ 0.1343, -0.1887, -0.0166,  ...,  0.1014,  0.1413,  0.0035],\n",
      "        [-0.1324,  0.1973,  0.0125,  ..., -0.0576, -0.0830,  0.1535]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.2.attention.wk.weight Parameter containing:\n",
      "tensor([[-0.0659, -0.0190,  0.1170,  ...,  0.1426, -0.0831,  0.1318],\n",
      "        [-0.1043,  0.0781,  0.0381,  ...,  0.0318, -0.0881, -0.1404],\n",
      "        [-0.0496, -0.0535, -0.0036,  ...,  0.0501, -0.0478,  0.0096],\n",
      "        ...,\n",
      "        [-0.0398, -0.0843,  0.0154,  ...,  0.0582,  0.1595, -0.0175],\n",
      "        [ 0.0988, -0.0672, -0.0726,  ..., -0.2828,  0.0285, -0.1712],\n",
      "        [-0.1340,  0.0723,  0.1075,  ..., -0.0070,  0.0776,  0.0935]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.2.attention.wv.weight Parameter containing:\n",
      "tensor([[ 0.0072, -0.0909,  0.2285,  ...,  0.3607, -0.1008, -0.0095],\n",
      "        [-0.0426, -0.0173,  0.2208,  ..., -0.0356, -0.0843,  0.0044],\n",
      "        [-0.0486,  0.0289, -0.0735,  ...,  0.1767, -0.0904,  0.0734],\n",
      "        ...,\n",
      "        [ 0.0223, -0.3698,  0.1256,  ..., -0.4010, -0.0289, -0.0139],\n",
      "        [-0.0225,  0.3073, -0.1742,  ...,  0.0692, -0.0504, -0.0095],\n",
      "        [-0.1388,  0.0036, -0.2082,  ...,  0.2211, -0.0077, -0.3095]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.2.attention.wo.weight Parameter containing:\n",
      "tensor([[-0.0656, -0.2325, -0.0317,  ...,  0.2328, -0.0285,  0.1140],\n",
      "        [ 0.0809, -0.1888, -0.0695,  ...,  0.1650, -0.1692, -0.1055],\n",
      "        [ 0.0076, -0.0973,  0.1741,  ...,  0.0386,  0.0156, -0.0591],\n",
      "        ...,\n",
      "        [-0.1230, -0.1989, -0.1453,  ...,  0.1009, -0.1289,  0.0713],\n",
      "        [ 0.0277,  0.0627, -0.0267,  ...,  0.1328, -0.0152, -0.2205],\n",
      "        [-0.1182,  0.0585, -0.0936,  ..., -0.1690,  0.1197,  0.4180]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.2.mlp.linear1.weight Parameter containing:\n",
      "tensor([[ 0.1722,  0.0491, -0.1581,  ..., -0.0764,  0.0652, -0.0031],\n",
      "        [ 0.1196, -0.1294,  0.1643,  ...,  0.0084, -0.0855,  0.0067],\n",
      "        [ 0.0183,  0.0870, -0.0316,  ..., -0.1324,  0.0240, -0.1844],\n",
      "        ...,\n",
      "        [ 0.1540, -0.0279, -0.0591,  ..., -0.2358,  0.1866,  0.1083],\n",
      "        [-0.1130, -0.0599, -0.0847,  ...,  0.1289, -0.0222,  0.0718],\n",
      "        [-0.0910,  0.2147,  0.1596,  ...,  0.1899, -0.0879,  0.0764]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.2.mlp.linear2.weight Parameter containing:\n",
      "tensor([[-8.3962e-02,  1.5913e-01, -1.8051e-01,  ..., -2.0494e-04,\n",
      "         -1.2843e-01, -1.7828e-01],\n",
      "        [ 4.0549e-02, -2.7222e-02,  1.9276e-01,  ..., -2.3205e-01,\n",
      "         -9.0915e-02, -4.3274e-02],\n",
      "        [ 1.7445e-02,  4.1524e-02,  2.8008e-02,  ..., -1.5447e-01,\n",
      "         -1.7873e-01,  4.1587e-03],\n",
      "        ...,\n",
      "        [-9.3087e-02,  1.5906e-02,  6.1081e-02,  ...,  1.5695e-02,\n",
      "          8.0415e-02, -9.9143e-02],\n",
      "        [-4.3025e-02,  1.4876e-01,  5.1615e-02,  ...,  7.3781e-02,\n",
      "          4.3080e-02, -4.2269e-02],\n",
      "        [ 1.7643e-01,  9.2932e-02,  9.0241e-04,  ..., -2.4248e-01,\n",
      "          1.7281e-01, -6.8013e-02]], device='cuda:0', requires_grad=True)\n",
      "layers.2.mlp.linear3.weight Parameter containing:\n",
      "tensor([[ 0.2785,  0.2785, -0.2081,  ...,  0.1879,  0.1520, -0.2365],\n",
      "        [-0.2948, -0.0094,  0.0558,  ...,  0.1777, -0.0519,  0.0312],\n",
      "        [ 0.1023, -0.0428,  0.0175,  ..., -0.0692, -0.0513,  0.0448],\n",
      "        ...,\n",
      "        [ 0.1093, -0.1694, -0.0054,  ..., -0.0294,  0.0268,  0.0393],\n",
      "        [-0.3291,  0.0105,  0.2123,  ..., -0.0035,  0.0644, -0.1899],\n",
      "        [-0.1111, -0.0539,  0.0573,  ..., -0.0648, -0.2986, -0.0579]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.3.norms.w Parameter containing:\n",
      "tensor([0.3743, 0.3938, 0.3777, 0.3942, 0.3841, 0.3967, 0.3781, 0.2946, 0.3786,\n",
      "        0.3618, 0.2928, 0.3831, 0.3908, 0.3992, 0.3736, 0.3879, 0.3758, 0.3669,\n",
      "        0.3801, 0.3927, 0.3897, 0.3485, 0.3660, 0.4002, 0.3678, 0.3632, 0.3942,\n",
      "        0.4227, 0.3627, 0.3861, 0.3923, 0.3622, 0.3784, 0.3644, 0.4000, 0.3502,\n",
      "        0.3761, 0.3749, 0.3773, 0.3757, 0.3440, 0.3648, 0.3630, 0.3854, 0.3732,\n",
      "        0.3966, 0.3825, 0.3496, 0.3892, 0.3894, 0.3563, 0.3975, 0.3766, 0.3939,\n",
      "        0.3972, 0.3816, 0.3700, 0.3895, 0.3754, 0.3738, 0.3682, 0.3902, 0.3631,\n",
      "        0.3964, 0.3700, 0.3906, 0.3686, 0.3877, 0.3620, 0.3797, 0.3905, 0.3799,\n",
      "        0.3947, 0.3708, 0.3746, 0.3693, 0.3727, 0.3653, 0.2918, 0.3785, 0.3729,\n",
      "        0.3682, 0.3706, 0.3689, 0.3834, 0.3761, 0.3703, 0.3669, 0.3737, 0.3764,\n",
      "        0.3881, 0.3537, 0.3652, 0.3834, 0.3848, 0.3602, 0.3564, 0.3937, 0.3925,\n",
      "        0.3783, 0.3821, 0.3519, 0.3862, 0.3834, 0.3858, 0.3744, 0.3850, 0.4258,\n",
      "        0.3856, 0.3720, 0.3888, 0.4196, 0.3734, 0.3863, 0.3835, 0.3920, 0.3895,\n",
      "        0.3833, 0.3621, 0.3656, 0.3656, 0.3735, 0.3745, 0.3764, 0.2663, 0.3698,\n",
      "        0.3691, 0.3559, 0.4004, 0.3639, 0.3818, 0.3862, 0.3714, 0.3929, 0.3710,\n",
      "        0.3895, 0.3616, 0.3948, 0.3757, 0.3746, 0.3685, 0.3835, 0.3573, 0.3833,\n",
      "        0.3776, 0.3667, 0.3678, 0.3775, 0.3758, 0.3698, 0.3891, 0.3723, 0.3885,\n",
      "        0.3842, 0.3802, 0.3788, 0.3735, 0.3547, 0.3844, 0.3765, 0.3719, 0.3761,\n",
      "        0.3535, 0.3786, 0.3641, 0.3808, 0.3668, 0.3967, 0.3896, 0.3889, 0.2628,\n",
      "        0.3699, 0.3875, 0.3903, 0.3919, 0.3954, 0.4038, 0.3855, 0.3794, 0.3706,\n",
      "        0.3728, 0.3844, 0.4045, 0.3802, 0.3930, 0.3859, 0.3770, 0.3824, 0.3727,\n",
      "        0.3754, 0.3763, 0.3731, 0.3733, 0.3622, 0.3829, 0.3679, 0.3995, 0.4096,\n",
      "        0.3766, 0.3833, 0.3743, 0.3735, 0.3699, 0.3917, 0.3899, 0.3818, 0.3785,\n",
      "        0.3703, 0.3656, 0.3730, 0.3751, 0.3848, 0.3614, 0.3832, 0.3829, 0.3652,\n",
      "        0.3891, 0.3842, 0.3781, 0.3710, 0.3670, 0.3878, 0.3784, 0.3770, 0.3675,\n",
      "        0.3504, 0.3676, 0.3713, 0.3756, 0.3943, 0.3692, 0.3749, 0.3730, 0.3951,\n",
      "        0.3821, 0.3871, 0.4094, 0.3854, 0.3999, 0.4094, 0.3866, 0.3859, 0.3648,\n",
      "        0.3844, 0.3657, 0.3930, 0.3734, 0.3712, 0.3738, 0.3986, 0.3844, 0.3708,\n",
      "        0.3627, 0.3924, 0.3609, 0.3879, 0.3839, 0.3886, 0.3904, 0.3848, 0.3766,\n",
      "        0.3634, 0.3858, 0.3873, 0.3611, 0.3549, 0.3803, 0.4023, 0.3776, 0.3739,\n",
      "        0.3796, 0.3708, 0.3708, 0.3869, 0.3777, 0.3638, 0.3794, 0.3857, 0.3654,\n",
      "        0.3849, 0.3712, 0.3817, 0.3891, 0.3707, 0.3674, 0.3817, 0.3603, 0.3729,\n",
      "        0.3813, 0.3989, 0.3664, 0.3562, 0.3814, 0.3527, 0.3827, 0.3896, 0.3736,\n",
      "        0.3661, 0.3844, 0.3837, 0.3744, 0.3675, 0.3973, 0.3914, 0.3660, 0.3896,\n",
      "        0.3672, 0.3713, 0.3860, 0.3727, 0.3768, 0.3885, 0.3688, 0.4043, 0.3580,\n",
      "        0.3856, 0.3770, 0.4036, 0.3976, 0.3550, 0.3875, 0.3745, 0.3812, 0.3656,\n",
      "        0.3617, 0.3582, 0.3885, 0.3756, 0.3803, 0.3694, 0.3690, 0.3758, 0.3871,\n",
      "        0.3776, 0.3963, 0.3839, 0.3606, 0.4406, 0.3845, 0.3872, 0.3945, 0.3766,\n",
      "        0.3775, 0.3798, 0.3712, 0.3714, 0.3600, 0.3213, 0.3809, 0.3723, 0.3744,\n",
      "        0.3535, 0.3907, 0.3623, 0.3702, 0.3730, 0.3842, 0.3799, 0.3695, 0.3487,\n",
      "        0.4027, 0.3670, 0.3614, 0.3699, 0.3829, 0.3831, 0.3668, 0.3821, 0.3942,\n",
      "        0.3933, 0.3823, 0.3681, 0.3900, 0.3982, 0.3879, 0.3645, 0.3749, 0.3729,\n",
      "        0.3753, 0.4065, 0.3761, 0.3846, 0.3726, 0.3854, 0.3795, 0.3793, 0.3461,\n",
      "        0.3502, 0.4107, 0.3733, 0.3884, 0.3764, 0.3667, 0.3846, 0.3759, 0.3687,\n",
      "        0.3840, 0.3833, 0.3701, 0.3874, 0.3603, 0.3887, 0.3644, 0.3170, 0.3791,\n",
      "        0.3859, 0.3783, 0.3863, 0.2525, 0.3609, 0.3726, 0.3939, 0.3835, 0.4090,\n",
      "        0.3837, 0.3812, 0.3838, 0.3643, 0.3765, 0.3879, 0.3866, 0.4205, 0.3813,\n",
      "        0.3816, 0.3441, 0.3771, 0.3707, 0.3912, 0.3795, 0.3770, 0.3623, 0.3539,\n",
      "        0.3790, 0.3792, 0.3908, 0.3632, 0.3918, 0.3837, 0.3785, 0.3859, 0.4063,\n",
      "        0.3682, 0.3986, 0.2337, 0.3621, 0.3756, 0.3748, 0.3887, 0.3547, 0.3810,\n",
      "        0.3763, 0.3794, 0.3803, 0.3721, 0.3655, 0.3904, 0.3754, 0.3756, 0.3523,\n",
      "        0.3649, 0.3595, 0.3815, 0.3487, 0.3780, 0.3792, 0.3740, 0.3784, 0.3887,\n",
      "        0.3873, 0.3495, 0.3720, 0.3475, 0.3804, 0.3757, 0.3798, 0.3731, 0.3802,\n",
      "        0.3795, 0.3983, 0.4021, 0.3440, 0.3938, 0.3921, 0.3682, 0.3687, 0.3894,\n",
      "        0.3977, 0.3694, 0.3808, 0.4338, 0.3749, 0.3832, 0.3657, 0.3667, 0.3911,\n",
      "        0.3773, 0.3913, 0.3877, 0.3865, 0.3667, 0.3957, 0.3690, 0.3817, 0.3691,\n",
      "        0.3830, 0.3612, 0.3565, 0.3683, 0.3831, 0.3768, 0.3908, 0.3689, 0.3772,\n",
      "        0.3633, 0.3826, 0.3778, 0.3700, 0.3874, 0.3686, 0.3692, 0.3616, 0.3779,\n",
      "        0.3800, 0.3819, 0.3826, 0.3713, 0.3799, 0.3826, 0.3545, 0.3742, 0.3876,\n",
      "        0.3493, 0.3987, 0.3811, 0.4072, 0.3783, 0.3719, 0.3529, 0.3652, 0.3958,\n",
      "        0.3834, 0.3843, 0.3625, 0.3774, 0.3646, 0.3648, 0.3789, 0.3795, 0.3804,\n",
      "        0.3642, 0.3721, 0.3868, 0.3520, 0.3801, 0.3662, 0.3767, 0.2396, 0.3668,\n",
      "        0.3546, 0.3772, 0.3858, 0.3802, 0.3891, 0.3782, 0.3588, 0.3732, 0.3379,\n",
      "        0.3721, 0.3413, 0.3660, 0.3210, 0.3643, 0.3622, 0.3743, 0.3720, 0.3754,\n",
      "        0.3671, 0.3820, 0.3766, 0.3777, 0.3770, 0.3699, 0.3636, 0.3941, 0.3860,\n",
      "        0.3650, 0.3905, 0.3661, 0.3727, 0.3787, 0.3820, 0.3766, 0.3705, 0.3828,\n",
      "        0.1817, 0.3746, 0.3806, 0.3697, 0.3747, 0.3904, 0.3707, 0.3761, 0.3655,\n",
      "        0.3720, 0.4068, 0.3740, 0.3833, 0.3535, 0.3695, 0.3820, 0.3876, 0.3749,\n",
      "        0.3640, 0.3747, 0.3572, 0.3769, 0.3721, 0.3998, 0.3799, 0.3677, 0.3763,\n",
      "        0.3876, 0.3738, 0.3828, 0.3804, 0.3739, 0.3783, 0.3769, 0.3768, 0.3989,\n",
      "        0.3907, 0.3716, 0.3843, 0.1461, 0.4020, 0.3845, 0.3889, 0.3562, 0.3807,\n",
      "        0.3898, 0.3798, 0.3883, 0.3634, 0.3743, 0.3763, 0.3922, 0.3850, 0.3892,\n",
      "        0.3912, 0.3666, 0.3758, 0.3674, 0.3707, 0.3723, 0.3715, 0.3811, 0.3692,\n",
      "        0.3929, 0.3787, 0.3859, 0.3688, 0.3587, 0.3732, 0.3643, 0.3863, 0.3615,\n",
      "        0.3695, 0.3713, 0.3767, 0.3713, 0.2860, 0.3747, 0.3870, 0.3845, 0.3814,\n",
      "        0.3606, 0.3594, 0.3677, 0.3962, 0.3728, 0.3629, 0.3561, 0.3615, 0.3934,\n",
      "        0.3776, 0.3760, 0.3817, 0.3876, 0.3795, 0.3583, 0.3715, 0.3705, 0.3755,\n",
      "        0.3613, 0.3661, 0.4007, 0.3693, 0.3829, 0.3893, 0.3761, 0.3538, 0.3670,\n",
      "        0.3660, 0.3819, 0.3732, 0.3686, 0.3603, 0.3781, 0.3800, 0.3716, 0.3769,\n",
      "        0.2890, 0.3628, 0.3932, 0.3648, 0.3714, 0.3561, 0.3913, 0.3820, 0.3576,\n",
      "        0.3883, 0.3494, 0.3798, 0.3775, 0.3907, 0.3727, 0.3776, 0.3763, 0.3835,\n",
      "        0.3588, 0.3710, 0.3904, 0.3550, 0.3776, 0.3689, 0.3693, 0.3652, 0.3702,\n",
      "        0.3738, 0.3562, 0.4457, 0.3657, 0.3684, 0.3530, 0.3933, 0.3964, 0.3647,\n",
      "        0.3857, 0.3929, 0.3729, 0.3774, 0.3640, 0.3921, 0.3768, 0.3962, 0.3886,\n",
      "        0.3691, 0.3877, 0.3791, 0.3551, 0.3791, 0.3874, 0.3660, 0.3653, 0.3935,\n",
      "        0.3650, 0.3862, 0.3881], device='cuda:0', requires_grad=True)\n",
      "layers.3.attention.wq.weight Parameter containing:\n",
      "tensor([[ 0.0675, -0.0096,  0.0730,  ..., -0.0026,  0.0019, -0.2131],\n",
      "        [ 0.0012,  0.0058, -0.0018,  ..., -0.0288, -0.0180, -0.0191],\n",
      "        [ 0.1371, -0.0902, -0.0482,  ...,  0.0411,  0.0093, -0.0401],\n",
      "        ...,\n",
      "        [ 0.0441,  0.0358,  0.1986,  ..., -0.0242, -0.1886,  0.0896],\n",
      "        [-0.1176, -0.1205,  0.0659,  ..., -0.0413, -0.1092,  0.0118],\n",
      "        [ 0.1558, -0.1336,  0.1705,  ..., -0.1455, -0.1435,  0.1765]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.3.attention.wk.weight Parameter containing:\n",
      "tensor([[-0.0469, -0.0328, -0.0407,  ...,  0.0508,  0.0320,  0.0411],\n",
      "        [-0.0398,  0.0525, -0.1204,  ...,  0.0605,  0.1796, -0.0067],\n",
      "        [ 0.0826, -0.0416,  0.1820,  ..., -0.0978,  0.0568,  0.1482],\n",
      "        ...,\n",
      "        [ 0.1362,  0.0374,  0.0563,  ...,  0.0219,  0.1423, -0.0055],\n",
      "        [ 0.1222, -0.1449, -0.1004,  ...,  0.0623, -0.2416,  0.3346],\n",
      "        [-0.1207,  0.1876, -0.1629,  ..., -0.0045, -0.0729, -0.0933]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.3.attention.wv.weight Parameter containing:\n",
      "tensor([[-0.2452, -0.2075, -0.0267,  ..., -0.0819,  0.1527, -0.1214],\n",
      "        [ 0.2113, -0.0536,  0.0612,  ..., -0.0098,  0.0204,  0.0264],\n",
      "        [ 0.1843,  0.0126,  0.1027,  ..., -0.1911, -0.0663,  0.2708],\n",
      "        ...,\n",
      "        [-0.0180, -0.0062, -0.1592,  ...,  0.0780,  0.3378,  0.2045],\n",
      "        [ 0.3047,  0.0655,  0.0183,  ...,  0.1112,  0.1030, -0.2504],\n",
      "        [-0.1486,  0.0433, -0.0648,  ..., -0.2282,  0.4968, -0.0036]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.3.attention.wo.weight Parameter containing:\n",
      "tensor([[-2.1154e-01, -7.4927e-02,  7.7806e-02,  ...,  2.1335e-01,\n",
      "          1.1492e-01, -1.9564e-02],\n",
      "        [ 3.2506e-01, -1.9452e-01,  6.5582e-02,  ...,  5.5466e-02,\n",
      "         -5.1637e-02, -1.7758e-01],\n",
      "        [ 9.8224e-02,  7.4018e-02, -1.6190e-01,  ...,  3.1717e-01,\n",
      "          3.2120e-02, -1.4635e-01],\n",
      "        ...,\n",
      "        [ 2.6178e-01,  7.6551e-02,  5.3575e-02,  ..., -1.4339e-01,\n",
      "         -1.5698e-01,  1.7318e-01],\n",
      "        [-5.7582e-02,  8.6548e-02, -9.9568e-02,  ...,  6.0690e-02,\n",
      "         -1.4167e-02, -1.7291e-01],\n",
      "        [ 5.2583e-02,  1.3210e-01, -1.4159e-01,  ...,  1.6796e-04,\n",
      "         -2.1151e-02, -1.1141e-01]], device='cuda:0', requires_grad=True)\n",
      "layers.3.mlp.linear1.weight Parameter containing:\n",
      "tensor([[ 0.0896, -0.0832, -0.1297,  ..., -0.0846,  0.1464,  0.1290],\n",
      "        [-0.0111,  0.0076,  0.1847,  ..., -0.0349,  0.1624, -0.1570],\n",
      "        [-0.1012, -0.1487,  0.0329,  ...,  0.0309, -0.0251,  0.0808],\n",
      "        ...,\n",
      "        [-0.1987,  0.1324,  0.1175,  ..., -0.0062, -0.0820, -0.1539],\n",
      "        [ 0.0724, -0.1288,  0.0503,  ..., -0.1228, -0.3090,  0.0350],\n",
      "        [-0.0424, -0.1093,  0.0979,  ...,  0.0659, -0.0081,  0.0237]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.3.mlp.linear2.weight Parameter containing:\n",
      "tensor([[-0.0278,  0.0198, -0.0767,  ..., -0.1917, -0.0270, -0.0719],\n",
      "        [-0.0036,  0.0945, -0.0270,  ..., -0.0424,  0.0075, -0.0358],\n",
      "        [ 0.0540,  0.0199,  0.1593,  ...,  0.0082, -0.1071, -0.0438],\n",
      "        ...,\n",
      "        [ 0.0332,  0.0773, -0.0823,  ...,  0.0127,  0.0786,  0.0913],\n",
      "        [-0.0474, -0.1850,  0.0388,  ..., -0.1681,  0.1783,  0.1229],\n",
      "        [ 0.0689, -0.2817,  0.2123,  ...,  0.0086,  0.1173, -0.0019]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.3.mlp.linear3.weight Parameter containing:\n",
      "tensor([[-0.2524,  0.1446,  0.0563,  ...,  0.1358, -0.0176, -0.1579],\n",
      "        [-0.1043, -0.0730,  0.0506,  ...,  0.0924,  0.0667,  0.0605],\n",
      "        [-0.0601,  0.0638,  0.1864,  ..., -0.0991, -0.1845, -0.0914],\n",
      "        ...,\n",
      "        [ 0.2586,  0.0291, -0.0808,  ..., -0.0321,  0.1143,  0.2454],\n",
      "        [-0.0026,  0.0741,  0.0293,  ...,  0.0053, -0.1401, -0.0858],\n",
      "        [ 0.1960, -0.2841,  0.0289,  ..., -0.3574, -0.2503,  0.2777]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "norm.w Parameter containing:\n",
      "tensor([1.3019, 1.4562, 1.2596, 1.5074, 1.2964, 1.3831, 1.4479, 1.1071, 1.3281,\n",
      "        1.2946, 1.3107, 1.1997, 1.3666, 1.3771, 1.3293, 1.2292, 1.3997, 1.5292,\n",
      "        1.3865, 1.4332, 1.3426, 1.2954, 1.2940, 1.8014, 1.2643, 1.3055, 1.3209,\n",
      "        3.2079, 1.1429, 1.3908, 1.1833, 1.3175, 1.2640, 1.2234, 1.3186, 1.4865,\n",
      "        1.6252, 1.3322, 1.3425, 1.3817, 1.3477, 1.2777, 1.3409, 1.3457, 1.6098,\n",
      "        1.2905, 1.3484, 1.3171, 1.4975, 1.6114, 1.0730, 1.4488, 1.3107, 2.2618,\n",
      "        1.3794, 1.6421, 1.4456, 1.3417, 1.4036, 1.3987, 1.2712, 1.3824, 1.3289,\n",
      "        1.3339, 1.3358, 1.3732, 1.4316, 1.3547, 1.2488, 1.2332, 1.2469, 1.3858,\n",
      "        1.2862, 1.3343, 1.2464, 1.3412, 1.3750, 1.3850, 1.2153, 1.3496, 1.2771,\n",
      "        1.4135, 1.3327, 1.3234, 1.3061, 1.3050, 1.4217, 1.3394, 1.3612, 1.4726,\n",
      "        1.3939, 1.4693, 1.4456, 1.2935, 1.4703, 1.2958, 1.4451, 1.3461, 1.5508,\n",
      "        1.3699, 1.2496, 1.3895, 1.4669, 1.4813, 1.2903, 1.2631, 1.3995, 0.8309,\n",
      "        1.3241, 1.1856, 1.2313, 1.2407, 1.3675, 1.3510, 1.2996, 1.4617, 1.4703,\n",
      "        1.4241, 1.4840, 1.3525, 1.3857, 1.4286, 1.2333, 1.3016, 2.0918, 1.3618,\n",
      "        1.5016, 1.3530, 1.5408, 1.3010, 1.2742, 1.2625, 1.4928, 1.3892, 1.3827,\n",
      "        1.3305, 1.3676, 1.3135, 1.3618, 1.2431, 1.2363, 1.2797, 1.3109, 1.2978,\n",
      "        1.4202, 1.4266, 1.2685, 1.3711, 1.5038, 1.4388, 1.2786, 1.4802, 1.3646,\n",
      "        1.3681, 1.3221, 1.2257, 1.4653, 1.3697, 1.4327, 1.3383, 1.4017, 1.2668,\n",
      "        1.3419, 1.3105, 1.3730, 1.3431, 1.2605, 1.3315, 1.3700, 1.2765, 0.6790,\n",
      "        1.4689, 1.3087, 1.4963, 1.3124, 1.3735, 1.3787, 1.5317, 1.3179, 1.4516,\n",
      "        1.2878, 1.2494, 1.3262, 1.4564, 1.2127, 1.3714, 1.3837, 1.3836, 1.3076,\n",
      "        1.3903, 1.2829, 1.2629, 1.4355, 1.2671, 1.3543, 1.2376, 1.3668, 2.7896,\n",
      "        1.3577, 1.5399, 1.4705, 1.3313, 1.3274, 1.2090, 1.2638, 1.5073, 1.4064,\n",
      "        1.3100, 1.3455, 1.4316, 1.4667, 1.5625, 1.2536, 1.2079, 1.4658, 1.2461,\n",
      "        1.3391, 1.1907, 1.3767, 1.4395, 1.2691, 1.5680, 1.4413, 1.4238, 1.3486,\n",
      "        1.3013, 1.1676, 1.1815, 1.4093, 1.1644, 1.3154, 1.3813, 1.4361, 1.3512,\n",
      "        1.3247, 1.2800, 1.2203, 1.3313, 1.3073, 2.4364, 1.5450, 1.3377, 1.4315,\n",
      "        1.4516, 1.2414, 1.4275, 1.3011, 1.5329, 1.5132, 1.7814, 1.2779, 1.3166,\n",
      "        1.4540, 1.3555, 1.3511, 1.3251, 1.4005, 1.5568, 1.2765, 1.2995, 1.2197,\n",
      "        1.3322, 1.3999, 1.3029, 1.4082, 1.3967, 1.3403, 1.3572, 1.2602, 1.2089,\n",
      "        1.2124, 2.0110, 1.3722, 1.6176, 1.4107, 1.3780, 1.3088, 1.3829, 1.3829,\n",
      "        1.3186, 1.3670, 1.2953, 1.4470, 1.3441, 1.6395, 1.2236, 1.3761, 1.1739,\n",
      "        1.3236, 1.5616, 1.3898, 1.3339, 1.3782, 1.3862, 1.4159, 1.4148, 1.4638,\n",
      "        1.3924, 1.3925, 1.4005, 1.3872, 1.2861, 1.3186, 1.3739, 1.1547, 1.3232,\n",
      "        1.3014, 1.3580, 2.3168, 1.2919, 1.3085, 1.2198, 1.3483, 1.3746, 1.3998,\n",
      "        1.5378, 1.3330, 1.4980, 1.1610, 1.3061, 1.3701, 1.3766, 1.3736, 1.2198,\n",
      "        1.5219, 1.3382, 1.1995, 1.2832, 1.4464, 1.3661, 1.4157, 1.2799, 1.3130,\n",
      "        1.3967, 1.5691, 1.3487, 1.3718, 2.0787, 1.3226, 1.3901, 1.5099, 1.2449,\n",
      "        1.2763, 1.3987, 1.3157, 1.4310, 2.5982, 1.1246, 1.2739, 1.4129, 1.2917,\n",
      "        1.5597, 1.3448, 1.3780, 1.3309, 1.3949, 1.3640, 1.3797, 1.3006, 1.3121,\n",
      "        1.2426, 1.2787, 1.2904, 1.3837, 1.4232, 1.3352, 1.3191, 1.3191, 1.4031,\n",
      "        1.2988, 1.4218, 1.2129, 1.4625, 1.1922, 1.3357, 1.3334, 1.2453, 1.3601,\n",
      "        1.4032, 1.3267, 1.3375, 1.6308, 1.1910, 1.4041, 1.4406, 1.4176, 1.3603,\n",
      "        1.3463, 1.4093, 1.2936, 1.2592, 1.2510, 1.3128, 1.4922, 1.4925, 1.2586,\n",
      "        1.2496, 1.3290, 1.4016, 1.3501, 0.9103, 1.3485, 1.2895, 1.4004, 1.2694,\n",
      "        1.5264, 1.2923, 1.2346, 1.0625, 1.0850, 1.3413, 1.9655, 1.3901, 1.3174,\n",
      "        1.4686, 1.4171, 1.3617, 1.5078, 1.4197, 1.3286, 1.2518, 1.3196, 1.2357,\n",
      "        1.2704, 1.3860, 1.2356, 1.2750, 1.4738, 1.2581, 1.6196, 1.3148, 2.7956,\n",
      "        1.3097, 1.3315, 1.3432, 1.5433, 1.3082, 1.3973, 1.3545, 1.2704, 1.3007,\n",
      "        1.3982, 1.2308, 0.4195, 1.2606, 1.4058, 1.2730, 1.3138, 1.2964, 1.3436,\n",
      "        1.3409, 1.3914, 1.3734, 1.2403, 1.3094, 1.2917, 1.4059, 1.3801, 1.5905,\n",
      "        1.3222, 1.1575, 1.3051, 1.4508, 1.3070, 1.4281, 1.2231, 1.3228, 1.4024,\n",
      "        1.3484, 1.2476, 1.4657, 1.2876, 1.3724, 1.4654, 1.3362, 1.5351, 1.3386,\n",
      "        1.2554, 1.2975, 1.3107, 1.2397, 1.3377, 1.2328, 1.3445, 1.3915, 1.6528,\n",
      "        1.5370, 1.3635, 1.6623, 1.9346, 1.3254, 1.2950, 1.4183, 1.4975, 1.2808,\n",
      "        1.3779, 1.2933, 1.4169, 1.1639, 1.4822, 1.3532, 1.3275, 1.3058, 1.3053,\n",
      "        1.3379, 1.2159, 1.5171, 1.4182, 1.4012, 1.9778, 1.2431, 1.4179, 1.3429,\n",
      "        1.4675, 1.4924, 1.4233, 1.4755, 1.6113, 1.4785, 1.2097, 1.4523, 1.3794,\n",
      "        1.3390, 1.3681, 1.3459, 1.2854, 1.3935, 1.3468, 1.4537, 1.4767, 1.4268,\n",
      "        1.3459, 1.3220, 1.5283, 3.0574, 1.5119, 1.2805, 1.3708, 1.4395, 1.3395,\n",
      "        1.4393, 1.4208, 1.3759, 2.0504, 1.3639, 1.1512, 1.4704, 1.2384, 1.3343,\n",
      "        1.3138, 1.3482, 1.4181, 1.2790, 2.1792, 1.3684, 1.4323, 0.7056, 1.3503,\n",
      "        1.2259, 1.2015, 1.3113, 1.2096, 1.3318, 1.6011, 1.2765, 1.3959, 1.5147,\n",
      "        1.5708, 1.3459, 1.7346, 1.2504, 1.3749, 1.3711, 1.3824, 1.4744, 1.2492,\n",
      "        1.2530, 1.3405, 1.5878, 1.1699, 1.3364, 1.4866, 1.3614, 1.2983, 1.4037,\n",
      "        1.3165, 1.4803, 1.3465, 1.4256, 1.2925, 1.5781, 1.2340, 1.3402, 1.2602,\n",
      "        0.6234, 1.2555, 1.5124, 1.2901, 1.3419, 1.3279, 1.2729, 1.2998, 1.1607,\n",
      "        1.2759, 1.2803, 1.3350, 1.2636, 1.4293, 1.2841, 1.2994, 1.3983, 2.2527,\n",
      "        1.4731, 1.5412, 1.3904, 1.3693, 1.3848, 1.2906, 1.4824, 1.3958, 1.3843,\n",
      "        1.2704, 1.3965, 1.3379, 1.3703, 1.2313, 1.3767, 1.6356, 1.3560, 1.1995,\n",
      "        1.4839, 1.2147, 1.4602, 2.1312, 1.3233, 1.3053, 1.3079, 1.3629, 1.3169,\n",
      "        1.3364, 1.3871, 1.3462, 1.3111, 1.4217, 1.3051, 1.3161, 1.4774, 1.3194,\n",
      "        1.4195, 1.2515, 1.3145, 1.5008, 1.3792, 1.4074, 1.4316, 1.3323, 1.3313,\n",
      "        1.3137, 1.4399, 1.3173, 1.5368, 1.3692, 1.3308, 1.2319, 1.3116, 1.5812,\n",
      "        1.3917, 1.3293, 1.4430, 1.2780, 1.5134, 1.3331, 1.5070, 1.4389, 1.3559,\n",
      "        1.3938, 1.2435, 1.3797, 1.3518, 1.5217, 1.3941, 1.2708, 1.3739, 1.3407,\n",
      "        1.6103, 1.4565, 1.4074, 1.3141, 1.1482, 1.3584, 1.5089, 1.5657, 1.5015,\n",
      "        1.3609, 1.4837, 1.2540, 1.3533, 1.3806, 1.2627, 1.5145, 1.3514, 1.3256,\n",
      "        1.2164, 1.3781, 1.3948, 1.3188, 1.4933, 1.3337, 1.3269, 1.4686, 1.3647,\n",
      "        1.3182, 1.6755, 1.4971, 1.3166, 1.6104, 1.2489, 1.3623, 1.3575, 1.4426,\n",
      "        1.2669, 1.3068, 1.4284, 1.3180, 1.5293, 1.3642, 1.2194, 1.4126, 1.3378,\n",
      "        1.4982, 1.4866, 1.6572, 1.3602, 1.2683, 1.4797, 1.5125, 1.3533, 1.2969,\n",
      "        1.3229, 1.2564, 1.9506, 1.3104, 1.2516, 1.4040, 1.4872, 1.3888, 1.3187,\n",
      "        1.3393, 1.2653, 1.4614, 1.2657, 1.3023, 1.4884, 1.3238, 1.8970, 1.1808,\n",
      "        1.3214, 1.2861, 1.4326, 1.4076, 1.3118, 1.1402, 1.3439, 1.4825, 1.3294,\n",
      "        1.3294, 1.2010, 1.3531], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, value in model.model.named_parameters():\n",
    "    print(name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf7b664e-739b-4c3f-969f-9d5b9ff05e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embd.weight Parameter containing:\n",
      "tensor([[-0.0407, -0.0110, -0.0283,  ...,  0.0344, -0.0348, -0.0187],\n",
      "        [-0.0024,  0.0108, -0.0182,  ...,  0.0005, -0.0436,  0.0270],\n",
      "        [-0.0827, -0.0160, -0.0518,  ..., -0.1533, -0.0490,  0.0909],\n",
      "        ...,\n",
      "        [-0.0407, -0.0110, -0.0283,  ...,  0.0344, -0.0348, -0.0188],\n",
      "        [-0.0407, -0.0110, -0.0283,  ...,  0.0344, -0.0348, -0.0187],\n",
      "        [-0.0406, -0.0110, -0.0283,  ...,  0.0344, -0.0347, -0.0188]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.norms.w Parameter containing:\n",
      "tensor([0.1570, 0.1649, 0.1578, 0.1534, 0.1473, 0.1549, 0.1634, 0.2792, 0.1523,\n",
      "        0.1406, 0.0617, 0.1445, 0.1376, 0.1397, 0.1580, 0.1514, 0.1633, 0.1397,\n",
      "        0.1655, 0.1390, 0.1695, 0.1402, 0.1481, 0.1264, 0.1438, 0.1410, 0.1371,\n",
      "        0.2102, 0.1409, 0.1850, 0.1454, 0.1394, 0.1555, 0.1386, 0.1472, 0.1209,\n",
      "        0.2077, 0.1555, 0.1558, 0.1519, 0.1542, 0.1612, 0.1654, 0.1531, 0.1833,\n",
      "        0.1591, 0.1632, 0.1340, 0.1568, 0.1453, 0.0869, 0.1625, 0.1303, 0.1675,\n",
      "        0.1486, 0.1203, 0.1484, 0.1429, 0.1474, 0.1598, 0.1574, 0.1548, 0.1389,\n",
      "        0.1470, 0.1496, 0.1376, 0.1656, 0.1448, 0.1263, 0.1421, 0.1275, 0.1257,\n",
      "        0.1466, 0.1341, 0.1421, 0.1226, 0.1471, 0.1450, 0.0793, 0.1495, 0.1404,\n",
      "        0.1279, 0.1669, 0.1488, 0.1550, 0.1385, 0.1718, 0.1540, 0.1650, 0.1439,\n",
      "        0.1467, 0.1105, 0.1540, 0.1357, 0.1617, 0.1557, 0.1261, 0.1325, 0.1078,\n",
      "        0.1486, 0.1532, 0.1398, 0.2487, 0.1554, 0.1488, 0.1397, 0.1538, 0.0484,\n",
      "        0.1377, 0.1571, 0.1472, 0.1410, 0.1705, 0.1501, 0.1577, 0.1549, 0.1359,\n",
      "        0.1076, 0.1672, 0.1866, 0.1305, 0.1395, 0.1287, 0.1359, 0.0564, 0.1711,\n",
      "        0.1274, 0.1471, 0.1565, 0.1609, 0.1272, 0.1208, 0.1627, 0.1543, 0.1543,\n",
      "        0.1404, 0.1434, 0.1524, 0.1571, 0.1507, 0.1545, 0.1271, 0.1402, 0.1473,\n",
      "        0.1653, 0.1514, 0.1540, 0.1487, 0.1719, 0.1759, 0.1577, 0.1410, 0.1534,\n",
      "        0.1572, 0.1621, 0.1641, 0.1593, 0.1002, 0.1747, 0.1567, 0.1392, 0.1361,\n",
      "        0.1629, 0.1435, 0.1285, 0.1335, 0.1489, 0.1479, 0.2296, 0.1458, 0.0696,\n",
      "        0.1561, 0.1534, 0.1362, 0.1473, 0.1603, 0.1686, 0.1543, 0.1371, 0.1606,\n",
      "        0.1401, 0.1481, 0.1615, 0.1820, 0.1431, 0.1578, 0.1405, 0.1681, 0.1630,\n",
      "        0.1602, 0.1510, 0.1398, 0.1362, 0.1521, 0.1744, 0.1192, 0.1561, 0.1571,\n",
      "        0.1477, 0.1110, 0.1708, 0.1496, 0.1417, 0.1290, 0.1520, 0.1215, 0.1598,\n",
      "        0.1363, 0.1604, 0.1590, 0.1851, 0.1370, 0.1415, 0.1437, 0.1643, 0.1539,\n",
      "        0.1509, 0.1505, 0.1342, 0.1694, 0.1502, 0.1416, 0.2388, 0.1954, 0.1674,\n",
      "        0.1589, 0.1668, 0.1622, 0.1261, 0.1722, 0.1417, 0.1493, 0.1426, 0.1272,\n",
      "        0.1533, 0.1248, 0.1662, 0.1438, 0.1258, 0.1701, 0.1275, 0.1622, 0.1592,\n",
      "        0.1506, 0.1424, 0.1559, 0.1480, 0.1621, 0.1392, 0.1848, 0.1500, 0.1523,\n",
      "        0.1636, 0.1529, 0.1559, 0.1567, 0.1633, 0.2070, 0.1358, 0.1535, 0.1473,\n",
      "        0.1474, 0.1695, 0.1340, 0.1363, 0.1385, 0.1555, 0.1511, 0.1575, 0.1606,\n",
      "        0.1408, 0.1783, 0.1365, 0.1546, 0.1288, 0.1499, 0.1674, 0.1601, 0.1728,\n",
      "        0.1585, 0.1577, 0.1616, 0.1588, 0.1355, 0.1355, 0.1430, 0.1540, 0.1531,\n",
      "        0.1435, 0.1571, 0.1392, 0.1627, 0.1337, 0.1433, 0.1525, 0.1527, 0.1547,\n",
      "        0.1377, 0.1562, 0.1489, 0.1393, 0.1457, 0.1722, 0.1433, 0.1452, 0.1232,\n",
      "        0.1330, 0.1347, 0.1970, 0.1234, 0.1627, 0.1374, 0.1481, 0.1457, 0.1526,\n",
      "        0.1884, 0.1517, 0.1464, 0.1502, 0.1573, 0.1439, 0.1464, 0.1585, 0.1238,\n",
      "        0.1342, 0.1205, 0.1540, 0.1736, 0.1543, 0.1534, 0.1469, 0.1526, 0.1497,\n",
      "        0.1569, 0.1926, 0.1540, 0.1543, 0.1197, 0.1574, 0.1462, 0.1710, 0.1488,\n",
      "        0.1584, 0.1464, 0.1607, 0.1645, 0.1145, 0.1189, 0.1341, 0.1634, 0.1414,\n",
      "        0.1601, 0.1514, 0.1461, 0.1536, 0.1576, 0.1643, 0.1625, 0.1415, 0.1413,\n",
      "        0.1485, 0.1480, 0.1500, 0.1367, 0.1421, 0.1530, 0.1508, 0.1486, 0.1731,\n",
      "        0.1479, 0.1614, 0.1602, 0.1455, 0.1336, 0.1397, 0.1378, 0.1467, 0.1457,\n",
      "        0.1785, 0.1488, 0.1378, 0.1012, 0.1469, 0.1696, 0.1391, 0.1569, 0.1250,\n",
      "        0.1789, 0.1459, 0.1520, 0.1740, 0.1555, 0.1154, 0.1672, 0.1570, 0.1410,\n",
      "        0.1365, 0.1608, 0.1672, 0.1513, 0.0765, 0.1488, 0.1434, 0.1362, 0.1419,\n",
      "        0.1678, 0.1508, 0.1424, 0.0747, 0.0920, 0.1108, 0.1644, 0.1350, 0.1300,\n",
      "        0.1607, 0.1457, 0.1283, 0.1496, 0.1407, 0.1302, 0.1579, 0.1414, 0.1472,\n",
      "        0.1428, 0.1642, 0.1418, 0.1673, 0.1471, 0.1401, 0.3928, 0.1474, 0.2239,\n",
      "        0.1902, 0.1501, 0.1394, 0.1625, 0.1490, 0.1606, 0.1675, 0.1597, 0.1487,\n",
      "        0.1557, 0.1407, 0.2428, 0.1344, 0.1576, 0.1546, 0.1619, 0.1461, 0.1520,\n",
      "        0.1660, 0.1474, 0.1489, 0.1655, 0.1565, 0.1504, 0.1417, 0.1158, 0.1367,\n",
      "        0.1511, 0.1487, 0.1328, 0.1058, 0.1540, 0.1613, 0.1656, 0.1465, 0.1464,\n",
      "        0.1557, 0.1457, 0.1517, 0.1443, 0.1556, 0.1708, 0.1568, 0.1733, 0.1477,\n",
      "        0.1510, 0.1482, 0.1570, 0.1087, 0.1307, 0.1564, 0.1555, 0.1554, 0.1665,\n",
      "        0.1676, 0.1366, 0.3098, 0.1716, 0.1706, 0.1571, 0.1468, 0.1603, 0.1474,\n",
      "        0.1396, 0.1578, 0.1736, 0.1556, 0.1532, 0.1459, 0.1351, 0.1688, 0.1363,\n",
      "        0.1530, 0.1512, 0.1639, 0.1739, 0.1937, 0.1547, 0.1435, 0.1508, 0.1533,\n",
      "        0.1532, 0.1572, 0.1341, 0.1814, 0.1590, 0.1644, 0.1438, 0.1651, 0.1455,\n",
      "        0.1374, 0.1445, 0.1507, 0.1404, 0.1600, 0.1558, 0.1423, 0.1520, 0.1496,\n",
      "        0.1359, 0.1418, 0.1556, 0.1629, 0.1558, 0.1618, 0.1584, 0.1807, 0.1461,\n",
      "        0.1380, 0.1485, 0.1554, 0.1669, 0.1698, 0.1251, 0.1616, 0.1145, 0.1400,\n",
      "        0.1587, 0.1436, 0.1699, 0.1426, 0.2263, 0.1469, 0.1475, 0.0733, 0.1135,\n",
      "        0.1359, 0.1568, 0.1670, 0.1606, 0.1628, 0.1390, 0.1424, 0.1751, 0.1465,\n",
      "        0.1061, 0.1490, 0.1638, 0.0793, 0.1521, 0.1750, 0.1350, 0.1452, 0.1482,\n",
      "        0.1483, 0.1523, 0.1809, 0.1652, 0.1465, 0.1995, 0.1634, 0.2114, 0.1693,\n",
      "        0.1552, 0.1801, 0.0964, 0.1583, 0.1518, 0.1484, 0.1242, 0.1427, 0.1481,\n",
      "        0.0620, 0.1564, 0.1421, 0.1444, 0.1529, 0.1666, 0.1461, 0.1568, 0.1503,\n",
      "        0.1433, 0.1376, 0.1469, 0.1329, 0.1669, 0.1486, 0.1392, 0.1611, 0.2089,\n",
      "        0.1028, 0.1897, 0.1473, 0.1612, 0.1616, 0.1360, 0.1598, 0.1655, 0.1503,\n",
      "        0.1205, 0.1059, 0.1602, 0.1445, 0.1877, 0.1439, 0.1713, 0.1489, 0.1661,\n",
      "        0.1589, 0.1510, 0.1593, 0.0607, 0.1419, 0.1417, 0.1434, 0.1532, 0.1510,\n",
      "        0.1607, 0.1625, 0.1501, 0.1448, 0.1523, 0.1456, 0.1498, 0.1233, 0.1500,\n",
      "        0.1583, 0.1515, 0.1486, 0.1429, 0.1441, 0.1663, 0.1456, 0.1620, 0.1661,\n",
      "        0.1362, 0.1650, 0.1499, 0.1348, 0.1333, 0.1376, 0.0788, 0.1407, 0.1385,\n",
      "        0.1444, 0.1236, 0.1540, 0.1137, 0.0547, 0.1421, 0.1563, 0.1412, 0.1472,\n",
      "        0.1644, 0.1539, 0.1479, 0.1528, 0.1502, 0.1451, 0.1396, 0.1358, 0.1573,\n",
      "        0.1586, 0.1315, 0.1356, 0.1571, 0.1032, 0.1635, 0.1573, 0.1482, 0.1326,\n",
      "        0.1596, 0.1535, 0.1384, 0.1540, 0.1491, 0.1487, 0.1470, 0.1194, 0.1525,\n",
      "        0.1217, 0.1594, 0.1430, 0.1448, 0.1214, 0.1478, 0.1318, 0.1430, 0.1734,\n",
      "        0.2255, 0.1423, 0.1378, 0.1559, 0.1398, 0.1530, 0.1475, 0.1606, 0.1628,\n",
      "        0.1483, 0.1425, 0.1359, 0.1442, 0.1756, 0.1531, 0.1499, 0.1206, 0.1486,\n",
      "        0.1475, 0.1466, 0.1451, 0.1501, 0.1461, 0.1728, 0.1693, 0.1594, 0.1325,\n",
      "        0.1778, 0.1427, 0.0767, 0.1397, 0.1405, 0.1385, 0.1487, 0.1526, 0.1621,\n",
      "        0.1531, 0.1453, 0.1547, 0.1516, 0.1372, 0.1530, 0.1428, 0.1409, 0.1557,\n",
      "        0.1459, 0.1309, 0.1629, 0.1487, 0.1535, 0.1532, 0.1332, 0.1350, 0.1755,\n",
      "        0.1505, 0.1453, 0.1465], device='cuda:0', requires_grad=True)\n",
      "layers.0.attention.wq.weight Parameter containing:\n",
      "tensor([[-5.2307e-02, -5.5769e-02, -1.3177e-01,  ...,  2.2935e-01,\n",
      "         -5.4016e-02,  1.1902e-01],\n",
      "        [ 3.2817e-02,  1.1855e-04,  3.0871e-02,  ..., -4.8728e-02,\n",
      "         -4.3831e-02, -1.0355e-01],\n",
      "        [-1.2030e-03, -2.2414e-01,  5.3518e-02,  ..., -1.0539e-01,\n",
      "         -1.3322e-01, -8.1382e-02],\n",
      "        ...,\n",
      "        [-6.4690e-02,  2.9343e-02, -1.5258e-04,  ..., -5.8976e-02,\n",
      "          1.8575e-01,  4.4744e-02],\n",
      "        [ 1.7963e-01,  1.1678e-01, -5.4585e-02,  ..., -5.7763e-02,\n",
      "         -2.2228e-01,  6.4764e-02],\n",
      "        [ 7.1337e-02, -2.0702e-01,  2.8848e-02,  ...,  9.6218e-02,\n",
      "         -2.9840e-02, -1.2775e-01]], device='cuda:0', requires_grad=True)\n",
      "layers.0.attention.wk.weight Parameter containing:\n",
      "tensor([[ 0.0006, -0.1292,  0.0835,  ..., -0.1895,  0.0720, -0.1217],\n",
      "        [ 0.0056, -0.0291, -0.3082,  ...,  0.0387, -0.0865, -0.1162],\n",
      "        [-0.0627, -0.1507, -0.0728,  ...,  0.2257,  0.1081,  0.2148],\n",
      "        ...,\n",
      "        [ 0.1453,  0.0486, -0.1470,  ..., -0.0579,  0.0439,  0.0829],\n",
      "        [ 0.0662, -0.1418,  0.0414,  ..., -0.0160,  0.1373,  0.1748],\n",
      "        [ 0.1188,  0.0646, -0.3359,  ...,  0.1219, -0.0254,  0.0006]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.attention.wv.weight Parameter containing:\n",
      "tensor([[-0.0579, -0.1393,  0.1843,  ..., -0.1480, -0.0053,  0.1488],\n",
      "        [-0.1899, -0.0990, -0.0106,  ..., -0.1089,  0.0936,  0.1428],\n",
      "        [ 0.0740,  0.0675,  0.0707,  ..., -0.1837, -0.1718,  0.0409],\n",
      "        ...,\n",
      "        [-0.1198,  0.1662,  0.1590,  ...,  0.1460, -0.1002, -0.0185],\n",
      "        [-0.0616, -0.0752,  0.1309,  ...,  0.0089,  0.0010,  0.0256],\n",
      "        [ 0.0797, -0.0114, -0.0983,  ..., -0.0765,  0.0154, -0.1365]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.attention.wo.weight Parameter containing:\n",
      "tensor([[ 0.1466,  0.0022,  0.0365,  ...,  0.1039,  0.1192, -0.0293],\n",
      "        [-0.2208, -0.1050,  0.0068,  ...,  0.0054,  0.0104,  0.0332],\n",
      "        [-0.0068,  0.1179,  0.0631,  ...,  0.0935,  0.0745, -0.0198],\n",
      "        ...,\n",
      "        [-0.0868,  0.0450,  0.0660,  ..., -0.0938,  0.0283, -0.1204],\n",
      "        [-0.2067,  0.1453,  0.0244,  ..., -0.0800,  0.0015, -0.0023],\n",
      "        [-0.1743,  0.0218,  0.0772,  ...,  0.0145, -0.0647,  0.0795]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.mlp.linear1.weight Parameter containing:\n",
      "tensor([[ 0.0463,  0.0428, -0.0611,  ...,  0.1543, -0.1007,  0.0822],\n",
      "        [ 0.0036, -0.0542, -0.1335,  ...,  0.2278,  0.2084, -0.0972],\n",
      "        [ 0.2158, -0.1678, -0.1142,  ..., -0.0280,  0.0036, -0.1184],\n",
      "        ...,\n",
      "        [ 0.0762,  0.0098, -0.0840,  ..., -0.0320,  0.1233,  0.1713],\n",
      "        [-0.1489,  0.1182, -0.1447,  ..., -0.0983,  0.1620, -0.1003],\n",
      "        [ 0.0796,  0.1287,  0.0659,  ..., -0.0119, -0.1098,  0.0349]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.mlp.linear2.weight Parameter containing:\n",
      "tensor([[-0.0482, -0.1685, -0.0115,  ...,  0.1840, -0.0708, -0.0035],\n",
      "        [-0.1788,  0.0912, -0.0607,  ...,  0.0726,  0.2293,  0.2272],\n",
      "        [ 0.2103,  0.0572,  0.0478,  ...,  0.0556, -0.0617, -0.0632],\n",
      "        ...,\n",
      "        [ 0.0539,  0.0029, -0.0104,  ...,  0.3167,  0.0363,  0.0702],\n",
      "        [ 0.0062, -0.1024, -0.1725,  ..., -0.0316,  0.0492, -0.1754],\n",
      "        [-0.0151, -0.1473, -0.1108,  ...,  0.0243,  0.0513,  0.0362]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.0.mlp.linear3.weight Parameter containing:\n",
      "tensor([[-0.0573,  0.0738,  0.0055,  ...,  0.1500, -0.1777, -0.0330],\n",
      "        [ 0.1465,  0.0298,  0.1787,  ..., -0.1816, -0.1352, -0.2725],\n",
      "        [ 0.0308, -0.2491, -0.0600,  ..., -0.0421, -0.3520, -0.1553],\n",
      "        ...,\n",
      "        [-0.0705,  0.0605, -0.1933,  ..., -0.2671,  0.1019, -0.1490],\n",
      "        [ 0.0701,  0.0906, -0.0620,  ..., -0.0149, -0.0144, -0.1427],\n",
      "        [-0.1563,  0.0960, -0.0023,  ...,  0.1178,  0.0552,  0.1532]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.norms.w Parameter containing:\n",
      "tensor([0.2850, 0.3023, 0.2806, 0.2770, 0.2713, 0.2849, 0.2808, 0.2160, 0.2665,\n",
      "        0.2793, 0.1345, 0.2768, 0.2551, 0.2790, 0.2649, 0.2702, 0.2542, 0.2526,\n",
      "        0.2837, 0.2656, 0.2484, 0.2960, 0.2846, 0.2475, 0.2595, 0.2864, 0.2667,\n",
      "        0.2964, 0.2628, 0.2866, 0.3046, 0.2519, 0.2867, 0.2783, 0.2638, 0.2228,\n",
      "        0.2779, 0.2955, 0.2709, 0.2708, 0.2811, 0.2665, 0.2782, 0.2995, 0.2267,\n",
      "        0.3002, 0.2482, 0.2866, 0.2573, 0.2640, 0.2442, 0.2808, 0.2650, 0.2713,\n",
      "        0.2690, 0.2434, 0.2591, 0.2640, 0.2756, 0.2854, 0.2870, 0.2528, 0.2844,\n",
      "        0.2363, 0.2793, 0.2621, 0.2817, 0.2486, 0.2451, 0.2579, 0.2749, 0.2643,\n",
      "        0.2742, 0.2767, 0.2715, 0.2648, 0.2519, 0.2931, 0.1343, 0.2446, 0.2816,\n",
      "        0.2502, 0.2981, 0.2693, 0.2692, 0.2688, 0.2818, 0.2748, 0.2729, 0.2524,\n",
      "        0.2512, 0.2076, 0.2830, 0.2696, 0.2857, 0.2616, 0.2139, 0.2538, 0.2633,\n",
      "        0.2675, 0.2544, 0.2699, 0.3037, 0.2655, 0.2747, 0.2562, 0.2782, 0.1908,\n",
      "        0.2717, 0.2818, 0.2716, 0.2787, 0.2976, 0.2734, 0.2559, 0.2694, 0.2591,\n",
      "        0.2447, 0.2483, 0.2767, 0.2387, 0.2827, 0.2716, 0.2659, 0.0962, 0.2593,\n",
      "        0.2618, 0.3057, 0.2751, 0.2787, 0.2775, 0.2613, 0.2648, 0.2630, 0.2733,\n",
      "        0.2630, 0.2550, 0.2663, 0.2813, 0.2894, 0.2803, 0.2955, 0.3064, 0.2732,\n",
      "        0.2754, 0.2481, 0.2582, 0.2652, 0.2748, 0.2635, 0.2967, 0.2612, 0.2693,\n",
      "        0.2764, 0.2708, 0.2983, 0.2818, 0.1921, 0.2805, 0.2688, 0.2635, 0.2609,\n",
      "        0.2945, 0.2528, 0.2501, 0.2395, 0.2692, 0.2658, 0.2720, 0.2681, 0.1324,\n",
      "        0.2730, 0.2718, 0.2875, 0.2732, 0.2821, 0.2977, 0.2715, 0.2701, 0.2781,\n",
      "        0.2550, 0.2743, 0.2987, 0.2601, 0.2556, 0.2770, 0.2789, 0.2802, 0.2804,\n",
      "        0.2799, 0.2852, 0.2598, 0.2666, 0.2780, 0.2665, 0.2609, 0.2723, 0.2803,\n",
      "        0.2448, 0.1995, 0.2821, 0.2758, 0.2741, 0.2581, 0.2776, 0.2488, 0.2652,\n",
      "        0.2375, 0.2689, 0.2546, 0.2838, 0.2864, 0.2734, 0.2711, 0.2775, 0.2989,\n",
      "        0.2780, 0.2423, 0.2430, 0.2864, 0.2773, 0.2767, 0.2850, 0.2611, 0.2694,\n",
      "        0.2705, 0.2643, 0.2973, 0.2743, 0.2931, 0.2444, 0.2652, 0.2502, 0.2559,\n",
      "        0.2684, 0.2430, 0.2848, 0.2772, 0.2323, 0.2249, 0.2665, 0.2780, 0.2663,\n",
      "        0.2746, 0.2701, 0.2535, 0.2663, 0.2889, 0.2658, 0.2553, 0.2702, 0.2745,\n",
      "        0.2685, 0.2884, 0.2831, 0.2720, 0.2696, 0.2839, 0.2858, 0.2871, 0.2859,\n",
      "        0.2862, 0.2982, 0.2688, 0.2687, 0.2502, 0.2784, 0.2615, 0.2736, 0.2903,\n",
      "        0.2685, 0.2823, 0.2492, 0.2818, 0.2484, 0.2817, 0.2782, 0.2801, 0.2865,\n",
      "        0.2731, 0.2552, 0.2906, 0.2971, 0.2746, 0.2700, 0.2835, 0.2865, 0.2953,\n",
      "        0.2670, 0.2360, 0.2838, 0.2801, 0.2666, 0.2817, 0.2725, 0.2682, 0.2756,\n",
      "        0.2551, 0.2815, 0.2714, 0.2683, 0.2636, 0.2570, 0.2565, 0.2567, 0.2256,\n",
      "        0.2595, 0.2715, 0.2788, 0.2080, 0.2801, 0.2513, 0.2825, 0.2662, 0.2776,\n",
      "        0.2884, 0.2556, 0.2675, 0.2871, 0.2890, 0.2552, 0.2474, 0.2689, 0.2226,\n",
      "        0.2360, 0.2061, 0.2806, 0.2924, 0.2814, 0.2789, 0.2720, 0.2555, 0.2459,\n",
      "        0.2721, 0.2577, 0.2790, 0.2905, 0.3048, 0.2794, 0.2917, 0.2758, 0.2873,\n",
      "        0.2599, 0.2738, 0.2617, 0.2700, 0.1831, 0.1809, 0.2620, 0.2429, 0.2971,\n",
      "        0.2820, 0.2565, 0.2903, 0.2916, 0.2722, 0.2732, 0.2866, 0.2563, 0.2713,\n",
      "        0.2838, 0.2827, 0.2874, 0.2627, 0.2760, 0.2786, 0.2679, 0.2792, 0.2758,\n",
      "        0.2693, 0.2781, 0.2667, 0.2617, 0.2538, 0.2524, 0.2286, 0.2626, 0.2733,\n",
      "        0.2744, 0.2614, 0.2886, 0.2333, 0.2795, 0.2767, 0.2769, 0.2757, 0.2644,\n",
      "        0.1988, 0.2430, 0.2585, 0.2802, 0.2907, 0.2246, 0.2794, 0.2682, 0.2706,\n",
      "        0.2881, 0.2447, 0.2891, 0.2644, 0.2444, 0.2948, 0.2812, 0.2370, 0.2723,\n",
      "        0.2728, 0.2793, 0.2771, 0.1062, 0.2465, 0.2708, 0.2598, 0.2628, 0.2565,\n",
      "        0.2944, 0.2559, 0.2723, 0.2994, 0.2573, 0.2559, 0.2672, 0.2911, 0.2798,\n",
      "        0.2696, 0.2507, 0.2553, 0.3058, 0.2565, 0.2721, 0.2588, 0.2771, 0.2817,\n",
      "        0.2573, 0.2707, 0.2726, 0.2657, 0.2620, 0.2908, 0.2751, 0.2682, 0.2638,\n",
      "        0.2692, 0.2966, 0.2115, 0.2702, 0.2911, 0.2530, 0.2888, 0.2850, 0.2849,\n",
      "        0.2803, 0.2527, 0.2552, 0.2679, 0.2718, 0.2724, 0.2364, 0.2648, 0.2592,\n",
      "        0.2678, 0.2872, 0.2691, 0.1741, 0.2675, 0.2774, 0.2650, 0.2695, 0.2932,\n",
      "        0.2746, 0.2748, 0.2671, 0.2634, 0.2779, 0.2535, 0.2940, 0.2833, 0.2743,\n",
      "        0.2554, 0.2781, 0.2867, 0.1810, 0.2863, 0.2702, 0.2759, 0.2628, 0.2718,\n",
      "        0.2880, 0.2743, 0.2539, 0.2357, 0.2915, 0.2674, 0.2443, 0.2630, 0.2762,\n",
      "        0.2517, 0.2646, 0.2836, 0.2720, 0.2693, 0.2601, 0.2831, 0.2809, 0.2445,\n",
      "        0.2492, 0.2770, 0.2654, 0.2786, 0.2854, 0.2478, 0.2916, 0.3008, 0.2773,\n",
      "        0.2681, 0.2345, 0.2741, 0.2732, 0.2386, 0.2895, 0.2746, 0.2817, 0.2767,\n",
      "        0.2818, 0.2768, 0.2923, 0.3095, 0.2667, 0.2563, 0.2290, 0.2745, 0.2899,\n",
      "        0.2534, 0.2739, 0.2997, 0.2864, 0.2608, 0.2767, 0.2544, 0.2642, 0.2594,\n",
      "        0.2682, 0.2673, 0.2688, 0.2373, 0.2784, 0.2779, 0.2969, 0.2365, 0.2784,\n",
      "        0.2645, 0.2724, 0.2753, 0.2728, 0.2459, 0.2785, 0.2782, 0.1499, 0.2519,\n",
      "        0.2378, 0.2760, 0.2742, 0.2752, 0.2766, 0.2592, 0.2571, 0.2775, 0.2646,\n",
      "        0.2643, 0.2469, 0.2599, 0.1390, 0.2709, 0.2697, 0.2534, 0.2695, 0.2951,\n",
      "        0.2748, 0.2872, 0.2753, 0.2532, 0.2485, 0.2811, 0.2907, 0.2962, 0.2671,\n",
      "        0.2745, 0.2717, 0.1966, 0.2622, 0.2784, 0.2933, 0.2401, 0.2780, 0.2766,\n",
      "        0.0856, 0.2584, 0.2927, 0.2662, 0.2746, 0.2539, 0.2819, 0.2797, 0.2719,\n",
      "        0.2787, 0.2712, 0.2581, 0.2608, 0.2812, 0.2515, 0.2642, 0.3038, 0.2265,\n",
      "        0.2579, 0.2906, 0.2605, 0.2699, 0.2751, 0.2757, 0.2819, 0.2877, 0.2768,\n",
      "        0.2565, 0.2105, 0.2659, 0.2800, 0.2586, 0.2806, 0.2648, 0.2809, 0.2931,\n",
      "        0.2721, 0.2939, 0.2762, 0.1266, 0.2867, 0.2975, 0.2670, 0.2810, 0.2630,\n",
      "        0.2735, 0.2700, 0.2601, 0.2819, 0.2897, 0.2629, 0.2737, 0.2650, 0.2747,\n",
      "        0.2657, 0.2846, 0.2979, 0.2630, 0.2429, 0.2802, 0.2500, 0.2729, 0.2828,\n",
      "        0.2768, 0.2861, 0.2828, 0.2616, 0.2310, 0.2784, 0.2198, 0.2806, 0.2345,\n",
      "        0.2724, 0.2690, 0.2749, 0.2530, 0.1021, 0.2817, 0.2740, 0.2732, 0.2494,\n",
      "        0.2981, 0.2845, 0.2748, 0.2719, 0.2712, 0.2887, 0.2455, 0.2643, 0.2846,\n",
      "        0.2843, 0.2289, 0.2217, 0.2942, 0.2675, 0.2719, 0.2721, 0.2633, 0.2440,\n",
      "        0.2856, 0.2795, 0.2820, 0.2766, 0.2758, 0.2872, 0.2991, 0.2085, 0.2734,\n",
      "        0.2174, 0.2652, 0.2821, 0.2662, 0.1942, 0.2947, 0.2593, 0.2654, 0.2713,\n",
      "        0.1670, 0.2440, 0.2736, 0.2812, 0.2791, 0.2839, 0.2805, 0.2680, 0.2717,\n",
      "        0.2740, 0.2624, 0.2657, 0.2619, 0.2935, 0.2664, 0.2652, 0.2351, 0.2649,\n",
      "        0.2687, 0.2763, 0.2500, 0.2753, 0.2638, 0.2834, 0.2729, 0.2779, 0.2660,\n",
      "        0.2847, 0.2505, 0.2178, 0.2730, 0.2891, 0.2449, 0.2377, 0.2879, 0.2964,\n",
      "        0.2872, 0.2804, 0.2920, 0.2657, 0.2950, 0.2684, 0.2541, 0.2514, 0.2783,\n",
      "        0.2674, 0.2776, 0.2738, 0.2798, 0.2835, 0.2725, 0.2434, 0.2724, 0.2726,\n",
      "        0.2797, 0.2806, 0.2693], device='cuda:0', requires_grad=True)\n",
      "layers.1.attention.wq.weight Parameter containing:\n",
      "tensor([[ 0.1390, -0.1411,  0.0004,  ...,  0.2166, -0.0291, -0.1342],\n",
      "        [ 0.0057, -0.0263, -0.1326,  ..., -0.0381, -0.0341,  0.1987],\n",
      "        [-0.0964, -0.0650,  0.1598,  ..., -0.2221,  0.0157,  0.0901],\n",
      "        ...,\n",
      "        [ 0.0146,  0.0882, -0.0193,  ..., -0.3531, -0.0884,  0.0472],\n",
      "        [-0.1676,  0.0052, -0.1263,  ...,  0.2171, -0.0755,  0.2778],\n",
      "        [ 0.0967, -0.1660, -0.1133,  ..., -0.0924, -0.2848,  0.0450]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.attention.wk.weight Parameter containing:\n",
      "tensor([[-0.1904,  0.0781, -0.0260,  ..., -0.0074, -0.1658,  0.1359],\n",
      "        [-0.1281,  0.0373,  0.0576,  ..., -0.0533,  0.0069, -0.1535],\n",
      "        [ 0.0775, -0.0207,  0.0156,  ...,  0.2091, -0.1287,  0.0878],\n",
      "        ...,\n",
      "        [-0.1049, -0.1896, -0.2463,  ...,  0.1776,  0.2710,  0.0579],\n",
      "        [ 0.0821,  0.1606,  0.1427,  ..., -0.2233,  0.1811,  0.1808],\n",
      "        [ 0.1790, -0.2561, -0.1975,  ..., -0.3736, -0.0985, -0.0709]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.attention.wv.weight Parameter containing:\n",
      "tensor([[-0.0717,  0.0770, -0.0991,  ...,  0.0491, -0.0862,  0.0481],\n",
      "        [-0.0431, -0.1444, -0.1412,  ...,  0.0051, -0.0497,  0.0012],\n",
      "        [-0.1839,  0.0764,  0.2220,  ..., -0.0076,  0.0298,  0.1254],\n",
      "        ...,\n",
      "        [ 0.0397, -0.0081, -0.1985,  ..., -0.1305,  0.1607, -0.0330],\n",
      "        [ 0.0062, -0.0797,  0.1006,  ...,  0.1296, -0.0172, -0.1084],\n",
      "        [ 0.1955,  0.1953, -0.0558,  ...,  0.2237, -0.1092,  0.0137]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.attention.wo.weight Parameter containing:\n",
      "tensor([[ 0.1373, -0.0884,  0.0753,  ..., -0.1109, -0.0735, -0.1243],\n",
      "        [-0.0710, -0.0559,  0.1128,  ..., -0.0860, -0.0689,  0.1237],\n",
      "        [-0.1219, -0.0910, -0.1905,  ...,  0.0503,  0.0199, -0.2239],\n",
      "        ...,\n",
      "        [-0.1278, -0.2060,  0.0073,  ...,  0.0131,  0.0392,  0.0389],\n",
      "        [ 0.0973,  0.0558, -0.1478,  ...,  0.0671,  0.0944, -0.2563],\n",
      "        [-0.2048, -0.0466, -0.0445,  ..., -0.0501, -0.0100, -0.1828]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.mlp.linear1.weight Parameter containing:\n",
      "tensor([[-0.1883,  0.2186, -0.0378,  ...,  0.1630, -0.0320,  0.1056],\n",
      "        [-0.1005,  0.0234, -0.0973,  ...,  0.0336,  0.0236,  0.0625],\n",
      "        [-0.1987,  0.2180, -0.3647,  ...,  0.2351,  0.0707, -0.1502],\n",
      "        ...,\n",
      "        [-0.2060, -0.1977,  0.1359,  ..., -0.0702,  0.0640,  0.0009],\n",
      "        [-0.1134, -0.2038, -0.1146,  ...,  0.0463, -0.1863, -0.0404],\n",
      "        [-0.1248, -0.0313, -0.0459,  ...,  0.2766, -0.0022,  0.0652]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.mlp.linear2.weight Parameter containing:\n",
      "tensor([[-0.2300, -0.1832,  0.1219,  ..., -0.1696,  0.1809, -0.0236],\n",
      "        [ 0.1215, -0.0513, -0.1755,  ...,  0.1809,  0.0510, -0.0402],\n",
      "        [ 0.0611,  0.0124,  0.0978,  ...,  0.0641,  0.0073, -0.0616],\n",
      "        ...,\n",
      "        [-0.0740,  0.0539, -0.1510,  ...,  0.1079,  0.0107,  0.0847],\n",
      "        [ 0.0589, -0.0205,  0.1125,  ..., -0.1424,  0.0640, -0.4017],\n",
      "        [ 0.0836, -0.1071, -0.1104,  ...,  0.1783, -0.1031,  0.0903]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.1.mlp.linear3.weight Parameter containing:\n",
      "tensor([[ 2.8169e-02,  1.3795e-01,  1.2291e-01,  ..., -1.5917e-02,\n",
      "          4.2374e-02,  7.5753e-02],\n",
      "        [-4.3080e-02,  2.7758e-03, -6.1867e-02,  ..., -1.5814e-01,\n",
      "          1.9948e-01,  8.1876e-02],\n",
      "        [-3.9830e-02,  1.6483e-01,  2.1466e-02,  ...,  1.9038e-02,\n",
      "          1.0850e-02,  6.9070e-02],\n",
      "        ...,\n",
      "        [-1.4547e-01,  3.7087e-01, -1.2494e-01,  ...,  2.5284e-01,\n",
      "         -1.9718e-01,  4.0925e-02],\n",
      "        [ 3.1494e-01, -3.9862e-01, -2.6485e-01,  ...,  6.8914e-02,\n",
      "          3.0751e-04, -2.9491e-02],\n",
      "        [ 1.8489e-02,  1.3556e-01, -6.8992e-02,  ...,  8.1240e-02,\n",
      "          1.2838e-01, -1.5892e-01]], device='cuda:0', requires_grad=True)\n",
      "layers.2.norms.w Parameter containing:\n",
      "tensor([0.3518, 0.3676, 0.3597, 0.3433, 0.3411, 0.3489, 0.3537, 0.2279, 0.3508,\n",
      "        0.3320, 0.2193, 0.3603, 0.3486, 0.3446, 0.3627, 0.3496, 0.3488, 0.3270,\n",
      "        0.3341, 0.3417, 0.3232, 0.3589, 0.3529, 0.3384, 0.3328, 0.3493, 0.3684,\n",
      "        0.3746, 0.3196, 0.3640, 0.3692, 0.3440, 0.3425, 0.3166, 0.3603, 0.2676,\n",
      "        0.3713, 0.3493, 0.3461, 0.3337, 0.3444, 0.3456, 0.3401, 0.3431, 0.2688,\n",
      "        0.3528, 0.3672, 0.3399, 0.3610, 0.3236, 0.2983, 0.3549, 0.3372, 0.3399,\n",
      "        0.3640, 0.3164, 0.3444, 0.3567, 0.3464, 0.3585, 0.3389, 0.3428, 0.3551,\n",
      "        0.3445, 0.3372, 0.3317, 0.3412, 0.3303, 0.3297, 0.3298, 0.3319, 0.3386,\n",
      "        0.3593, 0.3308, 0.3532, 0.3349, 0.3306, 0.3405, 0.2189, 0.3232, 0.3668,\n",
      "        0.3093, 0.3727, 0.3453, 0.3490, 0.3541, 0.3240, 0.3468, 0.3416, 0.3362,\n",
      "        0.3510, 0.2871, 0.3451, 0.3441, 0.3521, 0.3631, 0.2884, 0.3489, 0.3512,\n",
      "        0.3709, 0.3525, 0.3454, 0.3696, 0.3539, 0.3609, 0.3139, 0.3385, 0.2819,\n",
      "        0.3650, 0.3548, 0.3400, 0.3648, 0.3650, 0.3307, 0.3296, 0.3360, 0.3465,\n",
      "        0.3408, 0.3160, 0.3530, 0.3312, 0.3367, 0.3539, 0.3421, 0.1906, 0.3471,\n",
      "        0.3197, 0.3453, 0.3565, 0.3335, 0.3420, 0.3443, 0.3425, 0.3479, 0.3357,\n",
      "        0.3239, 0.3291, 0.3335, 0.3645, 0.3668, 0.3484, 0.3590, 0.3436, 0.3422,\n",
      "        0.3460, 0.3260, 0.3458, 0.3245, 0.3424, 0.3425, 0.3553, 0.3344, 0.3376,\n",
      "        0.3488, 0.3460, 0.3740, 0.3759, 0.2967, 0.3509, 0.3545, 0.3585, 0.3376,\n",
      "        0.3687, 0.3258, 0.3374, 0.3225, 0.3321, 0.3617, 0.3547, 0.3629, 0.1720,\n",
      "        0.3730, 0.3592, 0.3414, 0.3589, 0.3550, 0.3603, 0.3506, 0.3418, 0.3469,\n",
      "        0.3402, 0.3662, 0.3697, 0.3514, 0.3463, 0.3580, 0.3502, 0.3755, 0.3256,\n",
      "        0.3326, 0.3456, 0.3242, 0.3332, 0.3423, 0.3460, 0.3359, 0.3476, 0.3408,\n",
      "        0.3354, 0.3154, 0.3600, 0.3491, 0.3346, 0.3504, 0.3600, 0.3396, 0.3518,\n",
      "        0.3280, 0.3490, 0.3454, 0.3576, 0.3634, 0.3382, 0.3407, 0.3678, 0.3660,\n",
      "        0.3501, 0.3462, 0.3423, 0.3578, 0.3547, 0.3443, 0.3516, 0.3626, 0.3565,\n",
      "        0.3395, 0.3629, 0.3479, 0.3599, 0.3593, 0.3397, 0.3538, 0.3289, 0.3355,\n",
      "        0.3545, 0.3285, 0.3813, 0.3565, 0.3297, 0.2964, 0.3473, 0.3452, 0.3501,\n",
      "        0.3514, 0.3277, 0.3479, 0.3679, 0.3552, 0.3371, 0.3469, 0.3533, 0.3378,\n",
      "        0.3495, 0.3576, 0.3471, 0.3437, 0.3498, 0.3630, 0.3659, 0.3579, 0.3702,\n",
      "        0.3530, 0.3576, 0.3433, 0.3346, 0.3157, 0.3536, 0.3378, 0.3541, 0.3617,\n",
      "        0.3462, 0.3614, 0.3191, 0.3441, 0.3125, 0.3455, 0.3472, 0.3376, 0.3469,\n",
      "        0.3689, 0.3423, 0.3851, 0.3775, 0.3604, 0.3381, 0.3483, 0.3239, 0.3556,\n",
      "        0.3474, 0.3031, 0.3432, 0.3499, 0.3386, 0.3243, 0.3441, 0.3397, 0.3567,\n",
      "        0.3306, 0.3398, 0.3616, 0.3247, 0.3470, 0.3557, 0.3189, 0.3466, 0.3271,\n",
      "        0.3286, 0.3530, 0.3538, 0.2904, 0.3554, 0.3412, 0.3364, 0.3447, 0.3446,\n",
      "        0.3682, 0.3490, 0.3501, 0.3555, 0.3269, 0.3403, 0.3430, 0.3605, 0.3050,\n",
      "        0.3033, 0.3031, 0.3588, 0.3395, 0.3525, 0.3627, 0.3403, 0.3492, 0.3217,\n",
      "        0.3564, 0.3420, 0.3412, 0.3593, 0.3576, 0.3660, 0.3362, 0.3534, 0.3521,\n",
      "        0.3279, 0.3547, 0.3522, 0.3512, 0.2485, 0.2501, 0.3300, 0.3161, 0.3726,\n",
      "        0.3515, 0.3438, 0.3606, 0.3548, 0.3587, 0.3496, 0.3519, 0.3247, 0.3250,\n",
      "        0.3564, 0.3485, 0.3630, 0.3142, 0.3496, 0.3689, 0.3275, 0.3525, 0.3568,\n",
      "        0.3592, 0.3464, 0.3553, 0.3610, 0.3597, 0.3473, 0.3174, 0.3291, 0.3502,\n",
      "        0.3547, 0.3555, 0.3566, 0.3304, 0.3570, 0.3556, 0.3420, 0.3424, 0.3266,\n",
      "        0.3016, 0.3388, 0.3446, 0.3523, 0.3433, 0.3255, 0.3537, 0.3466, 0.3196,\n",
      "        0.3360, 0.3284, 0.3558, 0.3350, 0.3149, 0.3629, 0.3248, 0.3068, 0.3398,\n",
      "        0.3361, 0.3520, 0.3572, 0.1866, 0.3164, 0.3495, 0.3512, 0.3434, 0.3499,\n",
      "        0.3725, 0.3280, 0.3416, 0.3696, 0.3311, 0.3374, 0.3537, 0.3417, 0.3458,\n",
      "        0.3436, 0.3262, 0.3233, 0.3598, 0.3362, 0.3517, 0.3542, 0.3476, 0.3477,\n",
      "        0.3480, 0.3542, 0.3467, 0.3408, 0.3476, 0.3455, 0.3565, 0.3493, 0.3499,\n",
      "        0.3393, 0.3605, 0.2359, 0.3357, 0.3540, 0.3508, 0.3501, 0.3463, 0.3563,\n",
      "        0.3469, 0.3573, 0.3325, 0.3274, 0.3395, 0.3530, 0.3379, 0.3436, 0.3258,\n",
      "        0.3532, 0.3452, 0.3500, 0.2725, 0.3440, 0.3525, 0.3345, 0.3498, 0.3596,\n",
      "        0.3553, 0.3462, 0.3521, 0.3409, 0.3554, 0.3311, 0.3660, 0.3100, 0.3516,\n",
      "        0.3110, 0.3474, 0.3522, 0.2679, 0.3671, 0.3588, 0.3225, 0.3439, 0.3670,\n",
      "        0.3547, 0.3367, 0.3345, 0.3477, 0.3666, 0.3480, 0.3261, 0.3630, 0.3377,\n",
      "        0.3107, 0.3538, 0.3569, 0.3484, 0.3535, 0.3424, 0.3524, 0.3687, 0.3340,\n",
      "        0.3507, 0.3666, 0.3452, 0.3579, 0.3634, 0.3435, 0.3724, 0.3706, 0.3608,\n",
      "        0.3342, 0.3237, 0.3604, 0.3379, 0.3272, 0.3487, 0.3578, 0.3473, 0.3516,\n",
      "        0.3415, 0.3582, 0.3535, 0.3673, 0.3404, 0.3330, 0.3076, 0.3616, 0.3461,\n",
      "        0.3241, 0.3475, 0.3733, 0.3783, 0.3475, 0.3555, 0.3551, 0.3462, 0.3532,\n",
      "        0.3337, 0.3544, 0.3463, 0.3254, 0.3547, 0.3491, 0.3538, 0.3422, 0.3513,\n",
      "        0.3626, 0.3290, 0.3595, 0.3512, 0.3454, 0.3721, 0.3545, 0.1540, 0.3278,\n",
      "        0.3264, 0.3709, 0.3588, 0.3607, 0.3469, 0.3620, 0.3182, 0.3612, 0.3307,\n",
      "        0.3279, 0.3290, 0.3332, 0.2292, 0.3372, 0.3643, 0.3407, 0.3368, 0.3580,\n",
      "        0.3584, 0.3379, 0.3711, 0.3255, 0.3515, 0.3574, 0.3570, 0.3654, 0.3512,\n",
      "        0.3412, 0.3613, 0.2837, 0.3506, 0.3490, 0.3505, 0.3374, 0.3470, 0.3778,\n",
      "        0.1257, 0.3033, 0.3547, 0.3452, 0.3566, 0.3521, 0.3458, 0.3648, 0.3622,\n",
      "        0.3595, 0.3484, 0.3347, 0.3398, 0.3423, 0.3295, 0.3569, 0.3585, 0.3118,\n",
      "        0.3422, 0.3519, 0.3321, 0.3466, 0.3599, 0.3452, 0.3543, 0.3483, 0.3482,\n",
      "        0.3323, 0.3258, 0.3499, 0.3563, 0.3427, 0.3542, 0.3347, 0.3509, 0.3682,\n",
      "        0.3584, 0.3662, 0.3274, 0.1227, 0.3599, 0.3442, 0.3476, 0.3532, 0.3564,\n",
      "        0.3621, 0.3433, 0.3566, 0.3465, 0.3477, 0.3431, 0.3678, 0.3367, 0.3635,\n",
      "        0.3547, 0.3584, 0.3655, 0.3617, 0.3250, 0.3612, 0.3257, 0.3487, 0.3468,\n",
      "        0.3483, 0.3606, 0.3606, 0.3392, 0.3349, 0.3526, 0.3039, 0.3570, 0.3072,\n",
      "        0.3674, 0.3399, 0.3464, 0.3315, 0.1760, 0.3576, 0.3463, 0.3377, 0.3428,\n",
      "        0.3561, 0.3587, 0.3566, 0.3514, 0.3462, 0.3714, 0.3192, 0.3337, 0.3475,\n",
      "        0.3657, 0.3054, 0.3183, 0.3612, 0.3431, 0.3410, 0.3559, 0.3486, 0.3064,\n",
      "        0.3347, 0.3588, 0.3665, 0.3663, 0.3484, 0.3624, 0.3502, 0.3064, 0.3492,\n",
      "        0.2967, 0.3368, 0.3398, 0.3368, 0.2664, 0.3420, 0.3501, 0.3554, 0.3591,\n",
      "        0.2331, 0.3333, 0.3575, 0.3505, 0.3691, 0.3540, 0.3318, 0.3421, 0.3377,\n",
      "        0.3481, 0.3315, 0.3423, 0.3475, 0.3513, 0.3401, 0.3398, 0.3247, 0.3436,\n",
      "        0.3522, 0.3655, 0.3417, 0.3707, 0.3768, 0.3491, 0.3500, 0.3340, 0.3369,\n",
      "        0.3570, 0.3403, 0.2978, 0.3563, 0.3354, 0.3203, 0.3410, 0.3624, 0.3637,\n",
      "        0.3541, 0.3552, 0.3799, 0.3420, 0.3468, 0.3326, 0.3368, 0.3316, 0.3646,\n",
      "        0.3384, 0.3497, 0.3461, 0.3589, 0.3590, 0.3444, 0.3354, 0.3590, 0.3545,\n",
      "        0.3446, 0.3675, 0.3417], device='cuda:0', requires_grad=True)\n",
      "layers.2.attention.wq.weight Parameter containing:\n",
      "tensor([[ 0.0110, -0.0384,  0.1478,  ..., -0.0050, -0.0287,  0.0347],\n",
      "        [ 0.1204, -0.1248,  0.0857,  ..., -0.1175,  0.0246,  0.0074],\n",
      "        [ 0.1169,  0.0324, -0.1075,  ...,  0.0991, -0.0522,  0.0723],\n",
      "        ...,\n",
      "        [-0.0557,  0.0235,  0.1326,  ..., -0.1731,  0.1146,  0.0797],\n",
      "        [ 0.1369, -0.1856, -0.0150,  ...,  0.1018,  0.1408,  0.0055],\n",
      "        [-0.1245,  0.2008,  0.0055,  ..., -0.0573, -0.0868,  0.1566]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.2.attention.wk.weight Parameter containing:\n",
      "tensor([[-0.0648, -0.0207,  0.1214,  ...,  0.1373, -0.0800,  0.1300],\n",
      "        [-0.1054,  0.0838,  0.0417,  ...,  0.0338, -0.0864, -0.1369],\n",
      "        [-0.0509, -0.0559, -0.0021,  ...,  0.0506, -0.0477,  0.0035],\n",
      "        ...,\n",
      "        [-0.0399, -0.0829,  0.0117,  ...,  0.0612,  0.1573, -0.0190],\n",
      "        [ 0.1008, -0.0611, -0.0782,  ..., -0.2823,  0.0308, -0.1728],\n",
      "        [-0.1292,  0.0757,  0.1048,  ..., -0.0037,  0.0791,  0.0942]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.2.attention.wv.weight Parameter containing:\n",
      "tensor([[ 1.0897e-02, -9.0498e-02,  2.2373e-01,  ...,  3.6049e-01,\n",
      "         -9.9876e-02, -1.0756e-02],\n",
      "        [-4.1102e-02, -1.6898e-02,  2.1661e-01,  ..., -4.0525e-02,\n",
      "         -8.4477e-02, -9.2151e-05],\n",
      "        [-4.7211e-02,  2.6803e-02, -8.3850e-02,  ...,  1.7512e-01,\n",
      "         -9.1698e-02,  7.1176e-02],\n",
      "        ...,\n",
      "        [ 2.0098e-02, -3.7404e-01,  1.3127e-01,  ..., -4.0419e-01,\n",
      "         -2.9134e-02, -1.1611e-02],\n",
      "        [-2.3137e-02,  3.0902e-01, -1.7956e-01,  ...,  7.1167e-02,\n",
      "         -4.9178e-02, -1.0871e-02],\n",
      "        [-1.3688e-01, -8.8135e-04, -2.0948e-01,  ...,  2.1960e-01,\n",
      "         -1.4430e-02, -3.0970e-01]], device='cuda:0', requires_grad=True)\n",
      "layers.2.attention.wo.weight Parameter containing:\n",
      "tensor([[-0.0657, -0.2317, -0.0383,  ...,  0.2351, -0.0264,  0.1177],\n",
      "        [ 0.0873, -0.1856, -0.0678,  ...,  0.1665, -0.1700, -0.1056],\n",
      "        [ 0.0099, -0.0973,  0.1773,  ...,  0.0411,  0.0166, -0.0575],\n",
      "        ...,\n",
      "        [-0.1179, -0.1997, -0.1489,  ...,  0.1020, -0.1261,  0.0717],\n",
      "        [ 0.0245,  0.0589, -0.0324,  ...,  0.1328, -0.0154, -0.2206],\n",
      "        [-0.1238,  0.0565, -0.0963,  ..., -0.1724,  0.1173,  0.4158]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.2.mlp.linear1.weight Parameter containing:\n",
      "tensor([[ 0.1750,  0.0520, -0.1616,  ..., -0.0774,  0.0702, -0.0073],\n",
      "        [ 0.1220, -0.1289,  0.1700,  ...,  0.0056, -0.0832,  0.0090],\n",
      "        [ 0.0194,  0.0837, -0.0302,  ..., -0.1318,  0.0255, -0.1849],\n",
      "        ...,\n",
      "        [ 0.1489, -0.0244, -0.0553,  ..., -0.2343,  0.1858,  0.1101],\n",
      "        [-0.1167, -0.0551, -0.0879,  ...,  0.1352, -0.0206,  0.0667],\n",
      "        [-0.0910,  0.2155,  0.1605,  ...,  0.1922, -0.0854,  0.0740]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.2.mlp.linear2.weight Parameter containing:\n",
      "tensor([[-0.0841,  0.1528, -0.1812,  ..., -0.0041, -0.1300, -0.1827],\n",
      "        [ 0.0352, -0.0320,  0.1980,  ..., -0.2392, -0.0829, -0.0450],\n",
      "        [ 0.0145,  0.0416,  0.0254,  ..., -0.1590, -0.1779,  0.0059],\n",
      "        ...,\n",
      "        [-0.0889,  0.0133,  0.0582,  ...,  0.0159,  0.0795, -0.0977],\n",
      "        [-0.0382,  0.1480,  0.0501,  ...,  0.0724,  0.0376, -0.0458],\n",
      "        [ 0.1823,  0.0883,  0.0020,  ..., -0.2385,  0.1661, -0.0669]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.2.mlp.linear3.weight Parameter containing:\n",
      "tensor([[ 2.7730e-01,  2.7336e-01, -2.0194e-01,  ...,  1.8070e-01,\n",
      "          1.4725e-01, -2.3327e-01],\n",
      "        [-2.9346e-01, -2.6627e-03,  5.7417e-02,  ...,  1.8221e-01,\n",
      "         -4.1698e-02,  2.7278e-02],\n",
      "        [ 1.0522e-01, -4.5232e-02,  1.7893e-02,  ..., -7.6254e-02,\n",
      "         -5.4964e-02,  4.3374e-02],\n",
      "        ...,\n",
      "        [ 1.1214e-01, -1.6850e-01, -8.2301e-03,  ..., -2.9580e-02,\n",
      "          3.3160e-02,  3.9649e-02],\n",
      "        [-3.3311e-01,  1.1592e-02,  2.0787e-01,  ...,  1.3691e-04,\n",
      "          6.3022e-02, -1.9400e-01],\n",
      "        [-1.1334e-01, -5.7125e-02,  6.2066e-02,  ..., -6.4543e-02,\n",
      "         -3.0105e-01, -5.7921e-02]], device='cuda:0', requires_grad=True)\n",
      "layers.3.norms.w Parameter containing:\n",
      "tensor([0.3713, 0.3945, 0.3791, 0.3904, 0.3871, 0.3935, 0.3811, 0.2951, 0.3790,\n",
      "        0.3594, 0.2912, 0.3829, 0.3914, 0.3983, 0.3806, 0.3858, 0.3760, 0.3683,\n",
      "        0.3792, 0.3881, 0.3910, 0.3515, 0.3614, 0.4026, 0.3709, 0.3635, 0.3912,\n",
      "        0.4223, 0.3681, 0.3825, 0.3939, 0.3584, 0.3840, 0.3660, 0.3978, 0.3505,\n",
      "        0.3792, 0.3697, 0.3829, 0.3702, 0.3469, 0.3586, 0.3711, 0.3844, 0.3739,\n",
      "        0.3968, 0.3819, 0.3493, 0.3891, 0.3886, 0.3514, 0.3978, 0.3818, 0.3961,\n",
      "        0.3955, 0.3830, 0.3750, 0.3892, 0.3690, 0.3657, 0.3687, 0.3897, 0.3645,\n",
      "        0.3951, 0.3654, 0.3903, 0.3705, 0.3950, 0.3593, 0.3771, 0.3875, 0.3780,\n",
      "        0.3877, 0.3708, 0.3781, 0.3663, 0.3698, 0.3643, 0.2914, 0.3868, 0.3794,\n",
      "        0.3691, 0.3759, 0.3668, 0.3807, 0.3779, 0.3684, 0.3692, 0.3699, 0.3771,\n",
      "        0.3845, 0.3528, 0.3646, 0.3820, 0.3859, 0.3625, 0.3580, 0.3987, 0.3904,\n",
      "        0.3743, 0.3863, 0.3493, 0.3793, 0.3766, 0.3841, 0.3726, 0.3878, 0.4299,\n",
      "        0.3875, 0.3688, 0.3873, 0.4176, 0.3748, 0.3880, 0.3796, 0.3977, 0.3865,\n",
      "        0.3785, 0.3630, 0.3644, 0.3707, 0.3711, 0.3762, 0.3770, 0.2683, 0.3671,\n",
      "        0.3704, 0.3595, 0.3972, 0.3642, 0.3814, 0.3863, 0.3692, 0.3923, 0.3768,\n",
      "        0.3873, 0.3645, 0.3937, 0.3703, 0.3713, 0.3729, 0.3832, 0.3530, 0.3741,\n",
      "        0.3743, 0.3613, 0.3648, 0.3743, 0.3723, 0.3708, 0.3877, 0.3729, 0.3898,\n",
      "        0.3824, 0.3764, 0.3856, 0.3777, 0.3548, 0.3856, 0.3730, 0.3680, 0.3739,\n",
      "        0.3554, 0.3785, 0.3626, 0.3815, 0.3710, 0.3940, 0.3849, 0.3924, 0.2624,\n",
      "        0.3751, 0.3826, 0.3872, 0.3941, 0.3892, 0.3995, 0.3838, 0.3826, 0.3739,\n",
      "        0.3744, 0.3781, 0.3967, 0.3768, 0.3923, 0.3823, 0.3798, 0.3807, 0.3709,\n",
      "        0.3758, 0.3819, 0.3696, 0.3723, 0.3629, 0.3814, 0.3736, 0.3999, 0.4095,\n",
      "        0.3787, 0.3818, 0.3724, 0.3735, 0.3735, 0.3864, 0.3891, 0.3773, 0.3731,\n",
      "        0.3672, 0.3639, 0.3768, 0.3758, 0.3826, 0.3572, 0.3839, 0.3763, 0.3740,\n",
      "        0.3881, 0.3835, 0.3728, 0.3673, 0.3675, 0.3861, 0.3716, 0.3780, 0.3709,\n",
      "        0.3543, 0.3667, 0.3694, 0.3711, 0.3891, 0.3687, 0.3741, 0.3760, 0.3940,\n",
      "        0.3801, 0.3810, 0.4079, 0.3885, 0.3959, 0.4075, 0.3883, 0.3814, 0.3640,\n",
      "        0.3810, 0.3647, 0.3925, 0.3747, 0.3722, 0.3706, 0.4007, 0.3821, 0.3717,\n",
      "        0.3578, 0.3878, 0.3619, 0.3914, 0.3816, 0.3788, 0.3877, 0.3810, 0.3719,\n",
      "        0.3663, 0.3879, 0.3793, 0.3555, 0.3578, 0.3830, 0.4022, 0.3796, 0.3710,\n",
      "        0.3768, 0.3639, 0.3711, 0.3854, 0.3808, 0.3558, 0.3790, 0.3759, 0.3714,\n",
      "        0.3905, 0.3700, 0.3862, 0.3878, 0.3730, 0.3659, 0.3806, 0.3596, 0.3729,\n",
      "        0.3766, 0.3930, 0.3671, 0.3564, 0.3857, 0.3529, 0.3787, 0.3889, 0.3757,\n",
      "        0.3604, 0.3817, 0.3852, 0.3749, 0.3662, 0.3989, 0.3863, 0.3695, 0.3891,\n",
      "        0.3677, 0.3718, 0.3834, 0.3671, 0.3787, 0.3900, 0.3658, 0.3986, 0.3590,\n",
      "        0.3852, 0.3807, 0.3981, 0.3958, 0.3558, 0.3777, 0.3720, 0.3793, 0.3644,\n",
      "        0.3633, 0.3570, 0.3876, 0.3766, 0.3820, 0.3717, 0.3697, 0.3756, 0.3911,\n",
      "        0.3827, 0.3980, 0.3819, 0.3595, 0.4458, 0.3777, 0.3771, 0.3895, 0.3718,\n",
      "        0.3782, 0.3799, 0.3693, 0.3712, 0.3679, 0.3270, 0.3772, 0.3680, 0.3753,\n",
      "        0.3535, 0.3938, 0.3663, 0.3664, 0.3699, 0.3857, 0.3782, 0.3724, 0.3430,\n",
      "        0.4039, 0.3628, 0.3626, 0.3704, 0.3872, 0.3841, 0.3627, 0.3780, 0.3893,\n",
      "        0.3889, 0.3810, 0.3696, 0.3818, 0.3962, 0.3830, 0.3690, 0.3743, 0.3762,\n",
      "        0.3694, 0.3997, 0.3770, 0.3905, 0.3732, 0.3876, 0.3799, 0.3818, 0.3419,\n",
      "        0.3506, 0.4107, 0.3768, 0.3879, 0.3749, 0.3644, 0.3847, 0.3745, 0.3689,\n",
      "        0.3849, 0.3819, 0.3712, 0.3921, 0.3667, 0.3904, 0.3656, 0.3169, 0.3821,\n",
      "        0.3829, 0.3745, 0.3861, 0.2540, 0.3626, 0.3674, 0.3893, 0.3868, 0.4140,\n",
      "        0.3760, 0.3863, 0.3806, 0.3703, 0.3756, 0.3904, 0.3830, 0.4201, 0.3805,\n",
      "        0.3780, 0.3425, 0.3701, 0.3682, 0.3870, 0.3742, 0.3806, 0.3688, 0.3521,\n",
      "        0.3821, 0.3770, 0.3907, 0.3713, 0.3897, 0.3831, 0.3808, 0.3882, 0.4043,\n",
      "        0.3631, 0.3975, 0.2314, 0.3602, 0.3799, 0.3700, 0.3891, 0.3521, 0.3826,\n",
      "        0.3799, 0.3735, 0.3812, 0.3668, 0.3641, 0.3893, 0.3759, 0.3721, 0.3523,\n",
      "        0.3667, 0.3577, 0.3846, 0.3525, 0.3796, 0.3772, 0.3726, 0.3748, 0.3852,\n",
      "        0.3809, 0.3500, 0.3751, 0.3466, 0.3785, 0.3698, 0.3786, 0.3736, 0.3767,\n",
      "        0.3713, 0.3993, 0.3999, 0.3427, 0.3934, 0.3877, 0.3649, 0.3727, 0.3887,\n",
      "        0.3972, 0.3682, 0.3797, 0.4320, 0.3709, 0.3866, 0.3789, 0.3673, 0.3898,\n",
      "        0.3769, 0.3851, 0.3864, 0.3812, 0.3713, 0.3972, 0.3724, 0.3750, 0.3669,\n",
      "        0.3803, 0.3672, 0.3580, 0.3683, 0.3795, 0.3778, 0.3908, 0.3714, 0.3768,\n",
      "        0.3639, 0.3792, 0.3817, 0.3643, 0.3856, 0.3701, 0.3714, 0.3663, 0.3759,\n",
      "        0.3788, 0.3833, 0.3788, 0.3733, 0.3834, 0.3827, 0.3542, 0.3810, 0.3902,\n",
      "        0.3554, 0.3953, 0.3885, 0.4058, 0.3786, 0.3728, 0.3492, 0.3641, 0.3948,\n",
      "        0.3769, 0.3843, 0.3668, 0.3709, 0.3756, 0.3653, 0.3790, 0.3755, 0.3783,\n",
      "        0.3673, 0.3730, 0.3857, 0.3544, 0.3834, 0.3637, 0.3747, 0.2548, 0.3641,\n",
      "        0.3540, 0.3748, 0.3839, 0.3810, 0.3880, 0.3810, 0.3617, 0.3718, 0.3406,\n",
      "        0.3720, 0.3430, 0.3698, 0.3200, 0.3666, 0.3646, 0.3754, 0.3701, 0.3792,\n",
      "        0.3643, 0.3812, 0.3717, 0.3751, 0.3754, 0.3736, 0.3627, 0.3917, 0.3783,\n",
      "        0.3638, 0.3894, 0.3684, 0.3744, 0.3804, 0.3853, 0.3764, 0.3716, 0.3810,\n",
      "        0.1871, 0.3737, 0.3769, 0.3650, 0.3827, 0.3912, 0.3686, 0.3728, 0.3706,\n",
      "        0.3702, 0.4069, 0.3679, 0.3734, 0.3538, 0.3702, 0.3785, 0.3840, 0.3714,\n",
      "        0.3643, 0.3687, 0.3563, 0.3711, 0.3736, 0.3969, 0.3808, 0.3736, 0.3736,\n",
      "        0.3842, 0.3789, 0.3846, 0.3765, 0.3752, 0.3801, 0.3725, 0.3757, 0.4024,\n",
      "        0.3902, 0.3728, 0.3828, 0.1559, 0.4006, 0.3819, 0.3887, 0.3598, 0.3827,\n",
      "        0.3889, 0.3755, 0.3911, 0.3632, 0.3742, 0.3784, 0.3925, 0.3841, 0.3908,\n",
      "        0.3924, 0.3595, 0.3800, 0.3692, 0.3709, 0.3742, 0.3669, 0.3822, 0.3684,\n",
      "        0.3917, 0.3775, 0.3803, 0.3695, 0.3574, 0.3722, 0.3676, 0.3885, 0.3604,\n",
      "        0.3705, 0.3733, 0.3731, 0.3808, 0.2871, 0.3727, 0.3860, 0.3897, 0.3818,\n",
      "        0.3567, 0.3610, 0.3703, 0.3965, 0.3747, 0.3644, 0.3550, 0.3588, 0.3912,\n",
      "        0.3749, 0.3663, 0.3801, 0.3942, 0.3814, 0.3585, 0.3718, 0.3682, 0.3785,\n",
      "        0.3624, 0.3658, 0.4025, 0.3710, 0.3835, 0.3802, 0.3714, 0.3527, 0.3679,\n",
      "        0.3633, 0.3778, 0.3716, 0.3619, 0.3619, 0.3761, 0.3816, 0.3686, 0.3775,\n",
      "        0.2882, 0.3668, 0.3910, 0.3676, 0.3768, 0.3570, 0.3944, 0.3831, 0.3642,\n",
      "        0.3876, 0.3437, 0.3796, 0.3775, 0.3875, 0.3716, 0.3752, 0.3766, 0.3854,\n",
      "        0.3640, 0.3738, 0.3925, 0.3528, 0.3743, 0.3699, 0.3696, 0.3628, 0.3705,\n",
      "        0.3691, 0.3550, 0.4446, 0.3675, 0.3664, 0.3563, 0.3950, 0.4005, 0.3670,\n",
      "        0.3832, 0.3865, 0.3674, 0.3820, 0.3694, 0.3934, 0.3777, 0.4005, 0.3928,\n",
      "        0.3659, 0.3924, 0.3724, 0.3578, 0.3836, 0.3906, 0.3705, 0.3629, 0.3889,\n",
      "        0.3660, 0.3827, 0.3833], device='cuda:0', requires_grad=True)\n",
      "layers.3.attention.wq.weight Parameter containing:\n",
      "tensor([[ 0.0722, -0.0047,  0.0783,  ..., -0.0076,  0.0008, -0.2108],\n",
      "        [ 0.0049,  0.0102,  0.0020,  ..., -0.0323, -0.0189, -0.0157],\n",
      "        [ 0.1389, -0.0908, -0.0463,  ...,  0.0377,  0.0059, -0.0395],\n",
      "        ...,\n",
      "        [ 0.0428,  0.0411,  0.1981,  ..., -0.0224, -0.1870,  0.0865],\n",
      "        [-0.1242, -0.1252,  0.0630,  ..., -0.0356, -0.1044,  0.0098],\n",
      "        [ 0.1563, -0.1354,  0.1659,  ..., -0.1444, -0.1398,  0.1767]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.3.attention.wk.weight Parameter containing:\n",
      "tensor([[-0.0460, -0.0379, -0.0401,  ...,  0.0476,  0.0321,  0.0358],\n",
      "        [-0.0355,  0.0476, -0.1207,  ...,  0.0656,  0.1795, -0.0086],\n",
      "        [ 0.0821, -0.0430,  0.1778,  ..., -0.0965,  0.0581,  0.1444],\n",
      "        ...,\n",
      "        [ 0.1316,  0.0373,  0.0659,  ...,  0.0169,  0.1417, -0.0027],\n",
      "        [ 0.1209, -0.1443, -0.0909,  ...,  0.0578, -0.2439,  0.3363],\n",
      "        [-0.1147,  0.1879, -0.1689,  ..., -0.0074, -0.0727, -0.0906]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.3.attention.wv.weight Parameter containing:\n",
      "tensor([[-0.2427, -0.2113, -0.0253,  ..., -0.0862,  0.1536, -0.1181],\n",
      "        [ 0.2168, -0.0495,  0.0598,  ..., -0.0095,  0.0185,  0.0239],\n",
      "        [ 0.1868,  0.0171,  0.1034,  ..., -0.1902, -0.0679,  0.2683],\n",
      "        ...,\n",
      "        [-0.0154, -0.0044, -0.1579,  ...,  0.0800,  0.3382,  0.2036],\n",
      "        [ 0.3032,  0.0618,  0.0185,  ...,  0.1109,  0.1052, -0.2469],\n",
      "        [-0.1466,  0.0466, -0.0626,  ..., -0.2297,  0.4947, -0.0035]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.3.attention.wo.weight Parameter containing:\n",
      "tensor([[-0.2110, -0.0767,  0.0830,  ...,  0.2152,  0.1082, -0.0229],\n",
      "        [ 0.3265, -0.1967,  0.0702,  ...,  0.0594, -0.0560, -0.1756],\n",
      "        [ 0.0990,  0.0758, -0.1612,  ...,  0.3231,  0.0286, -0.1451],\n",
      "        ...,\n",
      "        [ 0.2602,  0.0765,  0.0527,  ..., -0.1468, -0.1554,  0.1721],\n",
      "        [-0.0602,  0.0849, -0.1016,  ...,  0.0579, -0.0085, -0.1748],\n",
      "        [ 0.0534,  0.1322, -0.1432,  ..., -0.0021, -0.0172, -0.1150]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.3.mlp.linear1.weight Parameter containing:\n",
      "tensor([[ 0.0934, -0.0842, -0.1267,  ..., -0.0886,  0.1414,  0.1329],\n",
      "        [-0.0092,  0.0080,  0.1881,  ..., -0.0357,  0.1608, -0.1541],\n",
      "        [-0.1057, -0.1517,  0.0334,  ...,  0.0348, -0.0199,  0.0779],\n",
      "        ...,\n",
      "        [-0.2029,  0.1299,  0.1227,  ..., -0.0051, -0.0906, -0.1540],\n",
      "        [ 0.0746, -0.1266,  0.0520,  ..., -0.1229, -0.3139,  0.0379],\n",
      "        [-0.0429, -0.1099,  0.1019,  ...,  0.0661, -0.0074,  0.0269]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.3.mlp.linear2.weight Parameter containing:\n",
      "tensor([[-0.0232,  0.0222, -0.0735,  ..., -0.1948, -0.0307, -0.0704],\n",
      "        [-0.0071,  0.0945, -0.0295,  ..., -0.0415,  0.0115, -0.0357],\n",
      "        [ 0.0509,  0.0199,  0.1576,  ...,  0.0089, -0.1033, -0.0434],\n",
      "        ...,\n",
      "        [ 0.0364,  0.0775, -0.0801,  ...,  0.0121,  0.0747,  0.0911],\n",
      "        [-0.0436, -0.1864,  0.0425,  ..., -0.1689,  0.1751,  0.1225],\n",
      "        [ 0.0765, -0.2803,  0.2194,  ...,  0.0033,  0.1204,  0.0034]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layers.3.mlp.linear3.weight Parameter containing:\n",
      "tensor([[-0.2554,  0.1462,  0.0575,  ...,  0.1379, -0.0166, -0.1563],\n",
      "        [-0.1056, -0.0730,  0.0490,  ...,  0.0955,  0.0691,  0.0593],\n",
      "        [-0.0600,  0.0618,  0.1890,  ..., -0.0995, -0.1821, -0.0936],\n",
      "        ...,\n",
      "        [ 0.2628,  0.0374, -0.0786,  ..., -0.0333,  0.1229,  0.2434],\n",
      "        [-0.0059,  0.0756,  0.0266,  ...,  0.0075, -0.1401, -0.0854],\n",
      "        [ 0.1918, -0.2848,  0.0277,  ..., -0.3538, -0.2456,  0.2765]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "norm.w Parameter containing:\n",
      "tensor([1.3040, 1.4584, 1.2607, 1.5102, 1.3036, 1.3770, 1.4489, 1.1113, 1.3322,\n",
      "        1.2926, 1.3145, 1.2013, 1.3715, 1.3766, 1.3332, 1.2285, 1.4015, 1.5318,\n",
      "        1.3893, 1.4325, 1.3398, 1.2962, 1.2964, 1.8019, 1.2643, 1.3031, 1.3199,\n",
      "        3.2042, 1.1455, 1.3905, 1.1837, 1.3179, 1.2688, 1.2223, 1.3189, 1.4870,\n",
      "        1.6226, 1.3320, 1.3485, 1.3837, 1.3537, 1.2795, 1.3385, 1.3480, 1.6117,\n",
      "        1.2895, 1.3533, 1.3244, 1.4944, 1.6099, 1.0699, 1.4447, 1.3139, 2.2648,\n",
      "        1.3770, 1.6408, 1.4498, 1.3391, 1.4044, 1.3943, 1.2731, 1.3879, 1.3294,\n",
      "        1.3337, 1.3335, 1.3750, 1.4352, 1.3532, 1.2510, 1.2363, 1.2447, 1.3914,\n",
      "        1.2876, 1.3379, 1.2488, 1.3441, 1.3789, 1.3867, 1.2118, 1.3480, 1.2778,\n",
      "        1.4108, 1.3282, 1.3219, 1.3070, 1.3135, 1.4252, 1.3385, 1.3623, 1.4744,\n",
      "        1.3953, 1.4672, 1.4450, 1.2932, 1.4717, 1.2973, 1.4411, 1.3474, 1.5583,\n",
      "        1.3681, 1.2533, 1.3936, 1.4662, 1.4743, 1.2864, 1.2645, 1.4023, 0.8315,\n",
      "        1.3211, 1.1973, 1.2355, 1.2401, 1.3724, 1.3543, 1.2970, 1.4589, 1.4709,\n",
      "        1.4202, 1.4841, 1.3553, 1.3841, 1.4266, 1.2322, 1.3007, 2.0951, 1.3666,\n",
      "        1.5020, 1.3595, 1.5419, 1.3018, 1.2675, 1.2690, 1.4922, 1.3914, 1.3826,\n",
      "        1.3293, 1.3720, 1.3155, 1.3588, 1.2400, 1.2352, 1.2817, 1.3112, 1.2979,\n",
      "        1.4201, 1.4280, 1.2699, 1.3717, 1.5031, 1.4410, 1.2803, 1.4783, 1.3627,\n",
      "        1.3680, 1.3184, 1.2270, 1.4649, 1.3709, 1.4302, 1.3386, 1.4017, 1.2640,\n",
      "        1.3418, 1.3090, 1.3758, 1.3424, 1.2607, 1.3349, 1.3643, 1.2763, 0.6834,\n",
      "        1.4682, 1.3118, 1.4934, 1.3120, 1.3772, 1.3779, 1.5302, 1.3158, 1.4529,\n",
      "        1.2883, 1.2505, 1.3263, 1.4487, 1.2193, 1.3699, 1.3851, 1.3913, 1.3113,\n",
      "        1.3889, 1.2839, 1.2635, 1.4425, 1.2698, 1.3564, 1.2383, 1.3692, 2.7880,\n",
      "        1.3609, 1.5373, 1.4708, 1.3275, 1.3225, 1.2034, 1.2665, 1.5137, 1.4102,\n",
      "        1.3075, 1.3441, 1.4323, 1.4654, 1.5626, 1.2515, 1.2028, 1.4641, 1.2469,\n",
      "        1.3398, 1.1899, 1.3728, 1.4424, 1.2732, 1.5686, 1.4327, 1.4265, 1.3534,\n",
      "        1.2994, 1.1673, 1.1837, 1.4085, 1.1617, 1.3135, 1.3790, 1.4340, 1.3507,\n",
      "        1.3241, 1.2802, 1.2195, 1.3306, 1.3075, 2.4403, 1.5410, 1.3379, 1.4347,\n",
      "        1.4544, 1.2413, 1.4266, 1.3027, 1.5309, 1.5134, 1.7792, 1.2773, 1.3193,\n",
      "        1.4583, 1.3543, 1.3453, 1.3259, 1.3983, 1.5526, 1.2769, 1.2989, 1.2160,\n",
      "        1.3304, 1.3995, 1.3040, 1.4095, 1.3930, 1.3378, 1.3546, 1.2570, 1.2091,\n",
      "        1.2146, 2.0151, 1.3701, 1.6218, 1.4133, 1.3803, 1.3069, 1.3803, 1.3839,\n",
      "        1.3216, 1.3717, 1.2964, 1.4507, 1.3395, 1.6384, 1.2219, 1.3771, 1.1722,\n",
      "        1.3207, 1.5577, 1.3918, 1.3319, 1.3765, 1.3835, 1.4141, 1.4117, 1.4651,\n",
      "        1.3958, 1.3926, 1.4033, 1.3894, 1.2815, 1.3147, 1.3814, 1.1564, 1.3245,\n",
      "        1.2963, 1.3553, 2.3196, 1.2976, 1.3179, 1.2208, 1.3543, 1.3782, 1.3949,\n",
      "        1.5368, 1.3302, 1.4989, 1.1647, 1.3053, 1.3689, 1.3785, 1.3731, 1.2187,\n",
      "        1.5268, 1.3381, 1.2042, 1.2815, 1.4491, 1.3647, 1.4115, 1.2785, 1.3142,\n",
      "        1.3954, 1.5668, 1.3473, 1.3680, 2.0813, 1.3206, 1.3924, 1.5123, 1.2470,\n",
      "        1.2734, 1.4017, 1.3136, 1.4297, 2.6005, 1.1218, 1.2733, 1.4083, 1.2895,\n",
      "        1.5610, 1.3386, 1.3873, 1.3327, 1.3962, 1.3660, 1.3824, 1.2987, 1.3089,\n",
      "        1.2388, 1.2747, 1.2985, 1.3835, 1.4207, 1.3379, 1.3210, 1.3213, 1.4043,\n",
      "        1.3022, 1.4203, 1.2091, 1.4534, 1.1927, 1.3374, 1.3338, 1.2448, 1.3537,\n",
      "        1.4019, 1.3203, 1.3305, 1.6271, 1.1928, 1.4045, 1.4436, 1.4215, 1.3612,\n",
      "        1.3454, 1.4095, 1.2948, 1.2592, 1.2476, 1.3165, 1.4927, 1.4911, 1.2589,\n",
      "        1.2480, 1.3289, 1.3945, 1.3513, 0.9100, 1.3414, 1.2920, 1.4045, 1.2743,\n",
      "        1.5279, 1.2920, 1.2346, 1.0656, 1.0894, 1.3417, 1.9723, 1.3903, 1.3137,\n",
      "        1.4771, 1.4176, 1.3652, 1.5102, 1.4205, 1.3316, 1.2488, 1.3183, 1.2316,\n",
      "        1.2746, 1.3844, 1.2334, 1.2697, 1.4785, 1.2576, 1.6215, 1.3167, 2.7962,\n",
      "        1.3092, 1.3330, 1.3440, 1.5439, 1.3071, 1.4006, 1.3557, 1.2675, 1.2991,\n",
      "        1.3990, 1.2403, 0.4153, 1.2549, 1.4084, 1.2696, 1.3125, 1.2960, 1.3456,\n",
      "        1.3489, 1.3921, 1.3750, 1.2407, 1.3152, 1.2936, 1.3989, 1.3800, 1.5891,\n",
      "        1.3220, 1.1512, 1.3008, 1.4502, 1.3057, 1.4263, 1.2180, 1.3197, 1.4010,\n",
      "        1.3531, 1.2503, 1.4632, 1.2908, 1.3710, 1.4644, 1.3376, 1.5353, 1.3400,\n",
      "        1.2517, 1.2949, 1.3108, 1.2413, 1.3377, 1.2381, 1.3469, 1.3898, 1.6549,\n",
      "        1.5326, 1.3636, 1.6660, 1.9324, 1.3187, 1.2948, 1.4186, 1.4979, 1.2781,\n",
      "        1.3772, 1.2929, 1.4142, 1.1619, 1.4827, 1.3544, 1.3251, 1.3070, 1.3138,\n",
      "        1.3354, 1.2180, 1.5183, 1.4262, 1.4008, 1.9789, 1.2425, 1.4183, 1.3501,\n",
      "        1.4728, 1.4975, 1.4165, 1.4783, 1.6144, 1.4862, 1.2110, 1.4552, 1.3792,\n",
      "        1.3377, 1.3728, 1.3412, 1.2838, 1.3990, 1.3422, 1.4565, 1.4732, 1.4309,\n",
      "        1.3499, 1.3244, 1.5212, 3.0558, 1.5093, 1.2791, 1.3673, 1.4399, 1.3390,\n",
      "        1.4358, 1.4208, 1.3767, 2.0519, 1.3620, 1.1495, 1.4683, 1.2370, 1.3384,\n",
      "        1.3109, 1.3437, 1.4133, 1.2790, 2.1794, 1.3694, 1.4340, 0.7033, 1.3489,\n",
      "        1.2231, 1.1991, 1.3140, 1.2091, 1.3308, 1.6000, 1.2731, 1.4004, 1.5139,\n",
      "        1.5767, 1.3487, 1.7359, 1.2539, 1.3715, 1.3703, 1.3833, 1.4717, 1.2509,\n",
      "        1.2476, 1.3417, 1.5846, 1.1683, 1.3339, 1.4897, 1.3571, 1.3042, 1.4046,\n",
      "        1.3156, 1.4753, 1.3476, 1.4227, 1.2936, 1.5743, 1.2347, 1.3384, 1.2594,\n",
      "        0.6225, 1.2578, 1.5127, 1.2868, 1.3433, 1.3299, 1.2744, 1.3048, 1.1624,\n",
      "        1.2801, 1.2828, 1.3343, 1.2656, 1.4331, 1.2826, 1.3001, 1.3968, 2.2564,\n",
      "        1.4724, 1.5363, 1.3903, 1.3693, 1.3818, 1.2923, 1.4805, 1.3939, 1.3852,\n",
      "        1.2728, 1.3955, 1.3389, 1.3701, 1.2322, 1.3784, 1.6343, 1.3553, 1.2057,\n",
      "        1.4831, 1.2139, 1.4552, 2.1320, 1.3306, 1.3050, 1.3066, 1.3598, 1.3205,\n",
      "        1.3364, 1.3906, 1.3454, 1.3112, 1.4236, 1.3080, 1.3163, 1.4770, 1.3201,\n",
      "        1.4160, 1.2479, 1.3088, 1.5043, 1.3817, 1.3995, 1.4323, 1.3326, 1.3318,\n",
      "        1.3111, 1.4447, 1.3152, 1.5391, 1.3707, 1.3327, 1.2356, 1.3120, 1.5779,\n",
      "        1.3890, 1.3280, 1.4438, 1.2743, 1.5150, 1.3296, 1.5081, 1.4394, 1.3525,\n",
      "        1.3953, 1.2434, 1.3784, 1.3521, 1.5215, 1.3938, 1.2729, 1.3706, 1.3464,\n",
      "        1.6167, 1.4588, 1.4070, 1.3170, 1.1508, 1.3590, 1.5099, 1.5660, 1.4965,\n",
      "        1.3631, 1.4813, 1.2508, 1.3526, 1.3803, 1.2655, 1.5117, 1.3511, 1.3196,\n",
      "        1.2133, 1.3857, 1.3962, 1.3181, 1.4943, 1.3382, 1.3241, 1.4704, 1.3628,\n",
      "        1.3179, 1.6786, 1.4971, 1.3174, 1.6102, 1.2511, 1.3634, 1.3541, 1.4448,\n",
      "        1.2662, 1.3029, 1.4265, 1.3215, 1.5390, 1.3648, 1.2245, 1.4137, 1.3375,\n",
      "        1.4988, 1.4877, 1.6557, 1.3571, 1.2700, 1.4828, 1.5108, 1.3532, 1.2914,\n",
      "        1.3265, 1.2587, 1.9530, 1.3132, 1.2494, 1.3995, 1.4885, 1.3892, 1.3162,\n",
      "        1.3471, 1.2714, 1.4589, 1.2616, 1.3068, 1.4872, 1.3257, 1.8999, 1.1811,\n",
      "        1.3179, 1.2886, 1.4297, 1.4076, 1.3134, 1.1388, 1.3382, 1.4866, 1.3278,\n",
      "        1.3293, 1.1998, 1.3625], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from src.models.blm.pl_training import Transformer\n",
    "\n",
    "MODEL_CHECKPOINT = args.paths.base_model_checkpoint\n",
    "\n",
    "base_model = Transformer.load_from_checkpoint(MODEL_CHECKPOINT)\n",
    "\n",
    "for name, value in base_model.named_parameters():\n",
    "    print(name, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4decea2d-0de2-460f-904b-443b0bc60887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
