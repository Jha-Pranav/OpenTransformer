{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7b3c98-bb62-476a-b2ee-55a06547f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REF : https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2911b6-528f-4a09-9cbd-28045d63cd68",
   "metadata": {},
   "source": [
    "#### Enable asynchronous data loading and augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b65bf-2741-46b5-9a93-5a1e32e598d3",
   "metadata": {},
   "source": [
    "- torch.utils.data.DataLoader supports asynchronous data loading and data augmentation in separate worker subprocesses. \n",
    "- Setting num_workers > 0 enables asynchronous data loading and overlap between the training and data loading.\n",
    "- set pin_memory=True, this instructs DataLoader to use pinned memory and enables faster and asynchronous memory copy from the host to the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc890e9f-36a6-4dcf-86dc-14e3ad8e7ef3",
   "metadata": {},
   "source": [
    "#### Disable gradient calculation for validation or inference\n",
    "\n",
    "-  torch.no_grad() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91827a45-abe3-4a79-bf58-f346cb3c492f",
   "metadata": {},
   "source": [
    "#### Disable bias for convolutions directly followed by a batch norm\n",
    "\n",
    "- If a nn.Conv2d layer is directly followed by a nn.BatchNorm2d layer, then the bias in the convolution is not needed, instead use nn.Conv2d(..., bias=False, ....). Bias is not needed because in the first step BatchNorm subtracts the mean, which effectively cancels out the effect of bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38541a6c-52bc-4a7e-9862-6aedaadefe48",
   "metadata": {},
   "source": [
    "#### zero out gradients, use the following method\n",
    "- optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776fd674-134b-422a-9c0e-febe0b978dc3",
   "metadata": {},
   "source": [
    "####  Fuse pointwise operations\n",
    "Pointwise operations (elementwise addition, multiplication, math functions - sin(), cos(), sigmoid() etc.) can be fused into a single kernel to amortize memory access time and kernel launch time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bdd4384-fe94-407a-97a2-1b80cea4c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b5a5a27-6f53-4ddb-a76e-33f12d3bc015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.27 ms ± 9.39 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def fused_gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\n",
    "%timeit fused_gelu(torch.rand(512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00489245-7b1f-47a3-a9c6-a5f88012f6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994 µs ± 118 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "@torch.compile\n",
    "def fused_gelu_compile(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\n",
    "%timeit fused_gelu_compile(torch.rand(512,512))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7668b0b4-aac5-4cc0-ab5c-6e221b4e228d",
   "metadata": {},
   "source": [
    "#### Enable channels_last memory format for computer vision models\n",
    "PyTorch 1.5 introduced support for channels_last memory format for convolutional networks. This format is meant to be used in conjunction with AMP to further accelerate convolutional neural networks with Tensor Cores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac8f99-af4a-47e5-b6f2-6d49d41f9ec2",
   "metadata": {},
   "source": [
    "#### Checkpoint intermediate buffers\n",
    "- Checkpointing targets should be selected carefully. The best is not to store large layer outputs that have small re-computation cost. The example target layers are activation functions (e.g. ReLU, Sigmoid, Tanh), up/down sampling and matrix-vector operations with small accumulation depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9037d08a-7189-44fd-81b2-594faca04507",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
