{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4474d4-7369-4682-bf40-e532a487c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref : https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2999b4ba-b2b5-4be8-8734-549e8071c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6476031-610a-43d7-b900-85d4b167e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e0978f-caef-4e05-b80c-34bc4ba32c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This returns a compute capability of the gpu, (major,minor)\n",
    "# My GPU has the compute capability of 8.9\n",
    "\n",
    "torch.cuda.get_device_capability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0075663a-a6ee-499c-9600-8575bed5a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any arbitary function\n",
    "\n",
    "\n",
    "def f1(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2c7fa4-c204-4730-aa7f-a145d36e9d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_foo1 = torch.compile(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d04d2ea6-3e96-44b5-a191-1da54479b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_bench(dim, device):\n",
    "    print(\"dim >>\", dim)\n",
    "    %timeit f1(torch.randn(dim, dim,device=device), torch.randn(dim, dim,device=device))\n",
    "    print(\"with torch.compile\")\n",
    "    %timeit opt_foo1(torch.randn(dim, dim,device=device), torch.randn(dim, dim,device=device))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "405d3fb1-9669-4d01-8e8c-a248ff2f8f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim >> 10\n",
      "4.24 µs ± 29.3 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "with torch.compile\n",
      "The slowest run took 5.63 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "40 µs ± 32.2 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "dim >> 100\n",
      "112 µs ± 2.84 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "with torch.compile\n",
      "68.5 µs ± 480 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "dim >> 1000\n",
      "7.38 ms ± 194 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "with torch.compile\n",
      "7.13 ms ± 147 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "dim >> 10000\n",
      "724 ms ± 8.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "with torch.compile\n",
      "783 ms ± 10.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "compile_bench(10, device)\n",
    "compile_bench(100, device)\n",
    "compile_bench(1000, device)\n",
    "compile_bench(10000, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3003aed6-84cd-4241-8d04-9e78ca81d2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim >> 10\n",
      "14.6 µs ± 235 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "with torch.compile\n",
      "63 µs ± 38.2 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "dim >> 100\n",
      "15.1 µs ± 450 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "with torch.compile\n",
      "32.7 µs ± 120 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "dim >> 1000\n",
      "51.2 µs ± 1.11 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "with torch.compile\n",
      "37.8 µs ± 968 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "dim >> 10000\n",
      "9.6 ms ± 2.7 ms per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "with torch.compile\n",
      "4.7 ms ± 178 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "compile_bench(10, device)\n",
    "compile_bench(100, device)\n",
    "compile_bench(1000, device)\n",
    "compile_bench(10000, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9e4473f-2099-464c-a28a-87fde48dd77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also decorate the func with torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f644e87f-565e-4868-af03-40acf290e6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3345,  0.3350, -0.9634,  ..., -0.4105,  0.9161, -0.2657],\n",
       "         [ 0.1466,  0.2699, -0.7055,  ..., -0.2511, -0.4539, -0.2040],\n",
       "         [ 0.7115,  0.6667,  0.6049,  ..., -0.0721, -0.8734, -0.1940],\n",
       "         ...,\n",
       "         [-0.3434,  0.0315,  0.0081,  ..., -0.2987, -0.3092, -0.4870],\n",
       "         [-0.9001,  0.1684, -0.2905,  ..., -0.7135,  0.7816, -0.5135],\n",
       "         [-0.8590, -0.6099,  0.0650,  ..., -0.9240, -0.1360,  0.7181]],\n",
       "        device='cuda:0'),\n",
       " 0.008256511688232422)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def timed(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end) / 1000  # second\n",
    "\n",
    "\n",
    "def foo(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a * b\n",
    "\n",
    "\n",
    "dim = 10000\n",
    "device = \"cuda\"\n",
    "timed(lambda: foo(torch.randn(dim, dim, device=device), torch.randn(dim, dim, device=device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12d41bb2-44e6-4cf5-af59-8f408cf1da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Pytorch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f58838-4cdc-4c10-acd5-a34bd4698056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "def f2(x):\n",
    "    x = x * 2\n",
    "    x = scipy.fft.dct(x.cpu().numpy())\n",
    "    x = torch.from_numpy(x)\n",
    "    x = x * 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d1e4c8a-02f4-4bf6-bb7f-f08e5f73893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_f2 = torch.compile(\n",
    "    f2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07a63c52-62a0-4ed8-a10f-ec83b2e57a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_bench(dim, device):\n",
    "    print(\"dim >>\", dim)\n",
    "    %timeit f2(torch.randn(dim, dim,device=device))\n",
    "    print(\"with torch.compile\")\n",
    "    %timeit opt_f2(torch.randn(dim, dim,device=device))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a8bd81f-04a1-4308-9c35-fd18460da9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim >> 1000\n",
      "1.76 ms ± 124 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "with torch.compile\n",
      "11.8 ms ± 3.36 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n",
      "dim >> 10000\n",
      "390 ms ± 110 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "with torch.compile\n",
      "311 ms ± 24.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import torch._dynamo\n",
    "# torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "compile_bench(1000, device)\n",
    "compile_bench(10000, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "062ef59a-a508-4c31-97d3-b4da34387c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inductor you can see the full list of configs that it supports by calling\n",
    "# torch._inductor.list_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e025b8-fe92-42d5-94ad-2da033c56671",
   "metadata": {},
   "source": [
    "### Demonstrating Speedups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25734ebd-3b29-4b11-aa49-4baadbebc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models import densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62c0cef6-2be5-4191-856a-e2e655680b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav-pc/.env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = densenet121(weights=torchvision.models.densenet.DenseNet121_Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0beee8d-4cf2-4b0f-b580-0941b19da10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "opt_model = torch.compile(model, mode=\"reduce-overhead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97067e82-a3ef-4f41-aae5-67cf68e4a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Infrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "322a4fbf-8d9e-4143-b23e-028b0ca4f4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.4 ms ± 218 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav-pc/.env/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:140: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 2290.62 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "51.3 ms ± 125 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "inp = torch.rand(128, 3, 128, 128).cuda()\n",
    "with torch.no_grad():\n",
    "    %timeit model(inp)\n",
    "    %timeit opt_model(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9cdf5-c3bf-4385-9c04-550807601c71",
   "metadata": {},
   "source": [
    "Notice that torch.compile takes a  approx same time to complete compared to eager. This is because torch.compile compiles the model into optimized kernels as it executes. In our example, the structure of the model doesn’t change, and so recompilation is not needed. So if we run our optimized model several more times, we should see a significant improvement compared to eager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "953b127a-ad3c-432a-956f-a73f3f37756e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.69 ms, sys: 300 µs, total: 5.99 ms\n",
      "Wall time: 6.01 ms\n",
      "CPU times: user 4.63 ms, sys: 73 µs, total: 4.7 ms\n",
      "Wall time: 4.7 ms\n",
      "CPU times: user 4.47 ms, sys: 0 ns, total: 4.47 ms\n",
      "Wall time: 4.48 ms\n",
      "CPU times: user 4.49 ms, sys: 0 ns, total: 4.49 ms\n",
      "Wall time: 4.5 ms\n",
      "CPU times: user 4.53 ms, sys: 0 ns, total: 4.53 ms\n",
      "Wall time: 4.54 ms\n",
      "CPU times: user 8.93 ms, sys: 0 ns, total: 8.93 ms\n",
      "Wall time: 8.77 ms\n",
      "CPU times: user 16.9 ms, sys: 0 ns, total: 16.9 ms\n",
      "Wall time: 17 ms\n",
      "CPU times: user 9.81 ms, sys: 0 ns, total: 9.81 ms\n",
      "Wall time: 9.82 ms\n",
      "CPU times: user 4.39 ms, sys: 0 ns, total: 4.39 ms\n",
      "Wall time: 4.4 ms\n",
      "CPU times: user 5.56 ms, sys: 0 ns, total: 5.56 ms\n",
      "Wall time: 5.46 ms\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Compile mode\n",
      "CPU times: user 518 µs, sys: 0 ns, total: 518 µs\n",
      "Wall time: 524 µs\n",
      "CPU times: user 344 µs, sys: 0 ns, total: 344 µs\n",
      "Wall time: 348 µs\n",
      "CPU times: user 295 µs, sys: 52 µs, total: 347 µs\n",
      "Wall time: 350 µs\n",
      "CPU times: user 308 µs, sys: 54 µs, total: 362 µs\n",
      "Wall time: 366 µs\n",
      "CPU times: user 336 µs, sys: 0 ns, total: 336 µs\n",
      "Wall time: 341 µs\n",
      "CPU times: user 510 µs, sys: 0 ns, total: 510 µs\n",
      "Wall time: 520 µs\n",
      "CPU times: user 403 µs, sys: 22 µs, total: 425 µs\n",
      "Wall time: 429 µs\n",
      "CPU times: user 316 µs, sys: 0 ns, total: 316 µs\n",
      "Wall time: 319 µs\n",
      "CPU times: user 304 µs, sys: 0 ns, total: 304 µs\n",
      "Wall time: 311 µs\n",
      "CPU times: user 283 µs, sys: 0 ns, total: 283 µs\n",
      "Wall time: 285 µs\n"
     ]
    }
   ],
   "source": [
    "## let's repeat this\n",
    "for _ in range(10):\n",
    "    inp = torch.rand(128, 3, 128, 128).cuda()\n",
    "    # Eager mode\n",
    "    with torch.no_grad():\n",
    "        %time model(inp)\n",
    "\n",
    "print(\"~\" * 30)\n",
    "\n",
    "print(\"Compile mode\")\n",
    "\n",
    "for _ in range(10):\n",
    "    inp = torch.rand(128, 3, 128, 128).cuda()\n",
    "    # Compile mode\n",
    "    with torch.no_grad():\n",
    "        %time opt_model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71d6c681-8897-4cb9-a981-5153ac122e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see a significant speed bump after first iteration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed6d9c8a-29c6-4941-8e0d-8cfdb2567750",
   "metadata": {},
   "source": [
    "Args :     \n",
    "    fullgraph: bool = False,\n",
    "    dynamic: Optional[bool] = None,\n",
    "    backend: Union[str, Callable] = 'inductor',\n",
    "    mode: Optional[str] = None,\n",
    "    options: Optional[Dict[str, Union[str, int, bool]]] = None,\n",
    "    disable: bool = False,\n",
    "\n",
    "mode specifies what the compiler should be optimizing while compiling.\n",
    "The default mode is a preset that tries to compile efficiently without taking too long to compile or using extra memory.\n",
    "Other modes such as reduce-overhead reduce the framework overhead by a lot more, but cost a small amount of extra memory. max-autotune compiles for a long time, trying to give you the fastest code it can generate.\n",
    "\n",
    "dynamic specifies whether to enable the code path for Dynamic Shapes. Certain compiler optimizations cannot be applied to dynamic shaped programs. Making it explicit whether you want a compiled program with dynamic shapes or with static shapes will help the compiler give you better optimized code.\n",
    "\n",
    "fullgraph is similar to Numba’s nopython. It compiles the entire program into a single graph or gives an error explaining why it could not do so. Most users don’t need to use this mode. If you are very performance conscious, then you try to use it.\n",
    "\n",
    "backend specifies which compiler backend to use. By default, TorchInductor is used, but there are a few others available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c664b403-e62f-4f3b-96fb-1b895e4597a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8d348ac-69a3-4f46-a892-302f645bedba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 125 ms, sys: 0 ns, total: 125 ms\n",
      "Wall time: 124 ms\n",
      "CPU times: user 21.2 ms, sys: 203 µs, total: 21.4 ms\n",
      "Wall time: 21.4 ms\n",
      "CPU times: user 20.1 ms, sys: 13 µs, total: 20.1 ms\n",
      "Wall time: 20.1 ms\n",
      "CPU times: user 20 ms, sys: 0 ns, total: 20 ms\n",
      "Wall time: 20.1 ms\n",
      "CPU times: user 19.9 ms, sys: 0 ns, total: 19.9 ms\n",
      "Wall time: 20 ms\n",
      "CPU times: user 20 ms, sys: 0 ns, total: 20 ms\n",
      "Wall time: 20 ms\n",
      "CPU times: user 20.1 ms, sys: 0 ns, total: 20.1 ms\n",
      "Wall time: 20.2 ms\n",
      "CPU times: user 20 ms, sys: 0 ns, total: 20 ms\n",
      "Wall time: 20 ms\n",
      "CPU times: user 20 ms, sys: 0 ns, total: 20 ms\n",
      "Wall time: 20.1 ms\n",
      "CPU times: user 13.1 ms, sys: 0 ns, total: 13.1 ms\n",
      "Wall time: 12.9 ms\n",
      "CPU times: user 5.99 ms, sys: 0 ns, total: 5.99 ms\n",
      "Wall time: 6 ms\n",
      "CPU times: user 4.99 ms, sys: 0 ns, total: 4.99 ms\n",
      "Wall time: 5 ms\n",
      "CPU times: user 5.06 ms, sys: 0 ns, total: 5.06 ms\n",
      "Wall time: 5.07 ms\n",
      "CPU times: user 5 ms, sys: 0 ns, total: 5 ms\n",
      "Wall time: 5.01 ms\n",
      "CPU times: user 4.96 ms, sys: 0 ns, total: 4.96 ms\n",
      "Wall time: 4.97 ms\n",
      "CPU times: user 4.94 ms, sys: 0 ns, total: 4.94 ms\n",
      "Wall time: 4.89 ms\n",
      "CPU times: user 4.94 ms, sys: 0 ns, total: 4.94 ms\n",
      "Wall time: 4.94 ms\n",
      "CPU times: user 4.89 ms, sys: 0 ns, total: 4.89 ms\n",
      "Wall time: 4.89 ms\n",
      "CPU times: user 4.91 ms, sys: 0 ns, total: 4.91 ms\n",
      "Wall time: 4.93 ms\n",
      "CPU times: user 4.99 ms, sys: 0 ns, total: 4.99 ms\n",
      "Wall time: 4.99 ms\n"
     ]
    }
   ],
   "source": [
    "# There are one 1000 image sample in the dataset\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_fun = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "y = torch.randint(1000, (12,), device=\"cuda\")\n",
    "num_epoch = 20\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    optimizer.zero_grad()\n",
    "    x = torch.rand(12, 3, 128, 128, device=\"cuda\")\n",
    "    %time pred = model(x)\n",
    "    loss = loss_fun(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38779efe-b4e2-4b9e-96de-197fc80f0333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59 s, sys: 10.8 s, total: 1min 9s\n",
      "Wall time: 1min 12s\n",
      "CPU times: user 944 ms, sys: 4.03 ms, total: 948 ms\n",
      "Wall time: 945 ms\n",
      "CPU times: user 3.46 ms, sys: 0 ns, total: 3.46 ms\n",
      "Wall time: 3.46 ms\n",
      "CPU times: user 1.53 ms, sys: 0 ns, total: 1.53 ms\n",
      "Wall time: 1.54 ms\n",
      "CPU times: user 1.38 ms, sys: 0 ns, total: 1.38 ms\n",
      "Wall time: 1.39 ms\n",
      "CPU times: user 1.29 ms, sys: 0 ns, total: 1.29 ms\n",
      "Wall time: 1.3 ms\n",
      "CPU times: user 1.25 ms, sys: 0 ns, total: 1.25 ms\n",
      "Wall time: 1.28 ms\n",
      "CPU times: user 1.15 ms, sys: 117 µs, total: 1.26 ms\n",
      "Wall time: 1.27 ms\n",
      "CPU times: user 1.91 ms, sys: 0 ns, total: 1.91 ms\n",
      "Wall time: 1.91 ms\n",
      "CPU times: user 0 ns, sys: 1.9 ms, total: 1.9 ms\n",
      "Wall time: 1.91 ms\n",
      "CPU times: user 1.91 ms, sys: 0 ns, total: 1.91 ms\n",
      "Wall time: 1.91 ms\n",
      "CPU times: user 1.67 ms, sys: 185 µs, total: 1.85 ms\n",
      "Wall time: 1.86 ms\n",
      "CPU times: user 1.82 ms, sys: 0 ns, total: 1.82 ms\n",
      "Wall time: 1.83 ms\n",
      "CPU times: user 1.43 ms, sys: 247 µs, total: 1.68 ms\n",
      "Wall time: 1.68 ms\n",
      "CPU times: user 1.49 ms, sys: 258 µs, total: 1.75 ms\n",
      "Wall time: 1.75 ms\n",
      "CPU times: user 1.84 ms, sys: 0 ns, total: 1.84 ms\n",
      "Wall time: 1.84 ms\n",
      "CPU times: user 1.92 ms, sys: 0 ns, total: 1.92 ms\n",
      "Wall time: 1.93 ms\n",
      "CPU times: user 1.64 ms, sys: 284 µs, total: 1.92 ms\n",
      "Wall time: 1.93 ms\n",
      "CPU times: user 1.93 ms, sys: 0 ns, total: 1.93 ms\n",
      "Wall time: 1.93 ms\n",
      "CPU times: user 1.91 ms, sys: 0 ns, total: 1.91 ms\n",
      "Wall time: 1.92 ms\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    optimizer.zero_grad()\n",
    "    x = torch.rand(12, 3, 128, 128, device=\"cuda\")\n",
    "    %time pred = opt_model(x)\n",
    "    loss = loss_fun(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6392f-3c8e-4779-94bd-bdb269df7479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
