{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8bea34-4eb8-4011-bffd-8d28f6de0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f720c101-d35d-463b-978c-5c5cdf33eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT_PATH = (\n",
    "    \"/home/pranav-pc/projects/OpenTransformer/multiformer/blm-1024/checkpoints/last.ckpt\"\n",
    ")\n",
    "model_dict = torch.load(MODEL_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa91683f-3c41-48ba-8d0a-991dd6411ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.weights_adapter.blm2other import _FROM_HF, blm_to_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7234458c-092d-42aa-a011-8ded05128a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/pranav-pc/projects/OpenTransformer/multiformer/blm-1024/checkpoints/blm2hf/pytorch_model.bin\"\n",
    "state_dict = model_dict[\"state_dict\"]\n",
    "converted_weights = blm_to_hf(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7299e816-d3d2-4fdb-8d2f-f7face1f867e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['tok_embd.weight', 'layers.0.norms.w', 'layers.0.attention.wq.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wv.weight', 'layers.0.attention.wo.weight', 'layers.0.mlp.linear1.weight', 'layers.0.mlp.linear2.weight', 'layers.0.mlp.linear3.weight', 'layers.1.norms.w', 'layers.1.attention.wq.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wv.weight', 'layers.1.attention.wo.weight', 'layers.1.mlp.linear1.weight', 'layers.1.mlp.linear2.weight', 'layers.1.mlp.linear3.weight', 'layers.2.norms.w', 'layers.2.attention.wq.weight', 'layers.2.attention.wk.weight', 'layers.2.attention.wv.weight', 'layers.2.attention.wo.weight', 'layers.2.mlp.linear1.weight', 'layers.2.mlp.linear2.weight', 'layers.2.mlp.linear3.weight', 'layers.3.norms.w', 'layers.3.attention.wq.weight', 'layers.3.attention.wk.weight', 'layers.3.attention.wv.weight', 'layers.3.attention.wo.weight', 'layers.3.mlp.linear1.weight', 'layers.3.mlp.linear2.weight', 'layers.3.mlp.linear3.weight', 'norm.w', 'output.weight'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict[\"state_dict\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d19f4f92-2ee4-4712-b24c-0b5c2ab023ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.embed_tokens.weight': 'tok_embd.weight',\n",
       " 'model.layers.{}.self_attn.q_proj.weight': 'layers.{}.attention.wq.weight',\n",
       " 'model.layers.{}.self_attn.k_proj.weight': 'layers.{}.attention.wk.weight',\n",
       " 'model.layers.{}.self_attn.v_proj.weight': 'layers.{}.attention.wv.weight',\n",
       " 'model.layers.{}.self_attn.o_proj.weight': 'layers.{}.attention.wo.weight',\n",
       " 'model.layers.{}.self_attn.rotary_emb.inv_freq': None,\n",
       " 'model.layers.{}.mlp.gate_proj.weight': 'layers.{}.mlp.linear1.weight',\n",
       " 'model.layers.{}.mlp.up_proj.weight': 'layers.{}.mlp.linear3.weight',\n",
       " 'model.layers.{}.mlp.down_proj.weight': 'layers.{}.mlp.linear2.weight',\n",
       " 'model.layers.{}.input_layernorm.weight': 'layers.{}.norms.w',\n",
       " 'model.layers.{}.post_attention_layernorm.weight': 'layers.{}.norms.w',\n",
       " 'model.norm.weight': 'norm.w',\n",
       " 'lm_head.weight': 'output.weight'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_FROM_HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10316449-e4ae-4317-90d0-9c16b72b3d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "# for key in _FROM_HF:\n",
    "#     if key not in converted_weights:\n",
    "#         print(f'{key} is missing in converted weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7419a8a9-701b-46cb-8bc3-1be4e1410f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(4):  # total number of transformer layers # TODO : FIX THIS Jugaar\n",
    "    converted_weights[f\"model.layers.{_}.input_layernorm.weight\"] = converted_weights[\n",
    "        f\"model.layers.{_}.post_attention_layernorm.weight\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7664db9b-d9ed-4afc-a61e-00c27c424164",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(converted_weights, PATH)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "918ae141-c1af-4bbb-a875-f76ce4e27fca",
   "metadata": {},
   "source": [
    "Args:\n",
    "    vocab_size (`int`, *optional*, defaults to 32000):\n",
    "        Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n",
    "        `inputs_ids` passed when calling [`LlamaModel`]\n",
    "    hidden_size (`int`, *optional*, defaults to 4096):\n",
    "        Dimension of the hidden representations.\n",
    "    intermediate_size (`int`, *optional*, defaults to 11008):\n",
    "        Dimension of the MLP representations.\n",
    "    num_hidden_layers (`int`, *optional*, defaults to 32):\n",
    "        Number of hidden layers in the Transformer decoder.\n",
    "    num_attention_heads (`int`, *optional*, defaults to 32):\n",
    "        Number of attention heads for each attention layer in the Transformer decoder.\n",
    "    num_key_value_heads (`int`, *optional*):\n",
    "        This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
    "        `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
    "        `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n",
    "        converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n",
    "        by meanpooling all the original heads within that group. For more details checkout [this\n",
    "        paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n",
    "        `num_attention_heads`.\n",
    "    hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n",
    "        The non-linear activation function (function or string) in the decoder.\n",
    "    max_position_embeddings (`int`, *optional*, defaults to 2048):\n",
    "        The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n",
    "        Llama 2 up to 4096, CodeLlama up to 16384.\n",
    "    initializer_range (`float`, *optional*, defaults to 0.02):\n",
    "        The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "    rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n",
    "        The epsilon used by the rms normalization layers.\n",
    "    use_cache (`bool`, *optional*, defaults to `True`):\n",
    "        Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
    "        relevant if `config.is_decoder=True`.\n",
    "    pad_token_id (`int`, *optional*):\n",
    "        Padding token id.\n",
    "    bos_token_id (`int`, *optional*, defaults to 1):\n",
    "        Beginning of stream token id.\n",
    "    eos_token_id (`int`, *optional*, defaults to 2):\n",
    "        End of stream token id.\n",
    "    pretraining_tp (`int`, *optional*, defaults to 1):\n",
    "        Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
    "        document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to understand more about it. This value is\n",
    "        necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
    "        issue](https://github.com/pytorch/pytorch/issues/76232).\n",
    "    tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n",
    "        Whether to tie weight embeddings\n",
    "    rope_theta (`float`, *optional*, defaults to 10000.0):\n",
    "        The base period of the RoPE embeddings.\n",
    "    rope_scaling (`Dict`, *optional*):\n",
    "        Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
    "        strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
    "        `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
    "        `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n",
    "        these scaling strategies behave:\n",
    "        https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n",
    "        experimental feature, subject to breaking API changes in future versions.\n",
    "    attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n",
    "        Whether to use a bias in the query, key, value and output projection layers during self-attention.\n",
    "    attention_dropout (`float`, *optional*, defaults to 0.0):\n",
    "        The dropout ratio for the attention probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28bae8f-fce1-4ad3-b462-eaa18148d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#     \"architectures\": [\"LlamaForCausalLM\"],\n",
    "#     \"attention_bias\": false,\n",
    "#     \"bos_token_id\": 1,\n",
    "#     \"eos_token_id\": 2,\n",
    "#     \"hidden_act\": \"silu\",\n",
    "#     \"hidden_size\": 768,\n",
    "#     \"initializer_range\": 0.02,\n",
    "#     \"intermediate_size\": 998,\n",
    "#     \"max_position_embeddings\": 1024,\n",
    "#     \"model_type\": \"llama\",\n",
    "#     \"num_attention_heads\": 12,\n",
    "#     \"num_hidden_layers\": 4,\n",
    "#     \"num_key_value_heads\": 12,\n",
    "#     \"pretraining_tp\": 1,\n",
    "#     \"rms_norm_eps\": 1e-05,\n",
    "#     \"rope_scaling\": null,\n",
    "#     \"rope_theta\": 10000.0,\n",
    "#     \"tie_word_embeddings\": false,\n",
    "#     \"torch_dtype\": \"bfloat16\",\n",
    "#     \"transformers_version\": \"4.35.0\",\n",
    "#     \"use_cache\": true,\n",
    "#     \"vocab_size\": 32000,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d635558-ad28-4ed6-8c82-c1d1791b3863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
