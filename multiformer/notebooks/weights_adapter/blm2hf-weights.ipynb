{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be60d3d-d06b-44e6-a836-c534a0e3149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0fce681-1435-4fc2-acef-d3c3f65c81f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHECKPOINT_PATH = (\n",
    "    \"/home/pranav-pc/projects/OpenTransformer/multiformer/blm-1024/checkpoints/last.ckpt\"\n",
    ")\n",
    "model_dict = torch.load(MODEL_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe8c61e-cc38-4c46-8728-7362bf94a06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'hparams_name', 'hyper_parameters'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d60f48a8-a802-43fb-beca-ae0c3575b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model_dict['hyper_parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cc12325-6901-4313-9c2c-39d1539aa890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embd.weight\n",
      "layers.0.norms.w\n",
      "layers.0.attention.wq.weight\n",
      "layers.0.attention.wk.weight\n",
      "layers.0.attention.wv.weight\n",
      "layers.0.attention.wo.weight\n",
      "layers.0.mlp.linear1.weight\n",
      "layers.0.mlp.linear2.weight\n",
      "layers.0.mlp.linear3.weight\n",
      "layers.1.norms.w\n",
      "layers.1.attention.wq.weight\n",
      "layers.1.attention.wk.weight\n",
      "layers.1.attention.wv.weight\n",
      "layers.1.attention.wo.weight\n",
      "layers.1.mlp.linear1.weight\n",
      "layers.1.mlp.linear2.weight\n",
      "layers.1.mlp.linear3.weight\n",
      "layers.2.norms.w\n",
      "layers.2.attention.wq.weight\n",
      "layers.2.attention.wk.weight\n",
      "layers.2.attention.wv.weight\n",
      "layers.2.attention.wo.weight\n",
      "layers.2.mlp.linear1.weight\n",
      "layers.2.mlp.linear2.weight\n",
      "layers.2.mlp.linear3.weight\n",
      "layers.3.norms.w\n",
      "layers.3.attention.wq.weight\n",
      "layers.3.attention.wk.weight\n",
      "layers.3.attention.wv.weight\n",
      "layers.3.attention.wo.weight\n",
      "layers.3.mlp.linear1.weight\n",
      "layers.3.mlp.linear2.weight\n",
      "layers.3.mlp.linear3.weight\n",
      "norm.w\n",
      "output.weight\n"
     ]
    }
   ],
   "source": [
    "for key in model_dict[\"state_dict\"]:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3afeb9c2-ff7b-4b1b-b340-3e2b8a5e98cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_FROM_HF = {\n",
    "    \"model.embed_tokens.weight\": \"tok_embd.weight\",\n",
    "    \"model.layers.{}.self_attn.q_proj.weight\": \"layers.{}.attention.wq.weight\",\n",
    "    \"model.layers.{}.self_attn.k_proj.weight\": \"layers.{}.attention.wk.weight\",\n",
    "    \"model.layers.{}.self_attn.v_proj.weight\": \"layers.{}.attention.wv.weight\",\n",
    "    \"model.layers.{}.self_attn.o_proj.weight\": \"layers.{}.attention.wo.weight\",\n",
    "    \"model.layers.{}.self_attn.rotary_emb.inv_freq\": None,\n",
    "    \"model.layers.{}.mlp.gate_proj.weight\": \"layers.{}.mlp.linear1.weight\",\n",
    "    \"model.layers.{}.mlp.up_proj.weight\": \"layers.{}.mlp.linear3.weight\",\n",
    "    \"model.layers.{}.mlp.down_proj.weight\": \"layers.{}.mlp.linear2.weight\",\n",
    "    \"model.layers.{}.input_layernorm.weight\": \"layers.{}.norms.w2\",\n",
    "    \"model.layers.{}.post_attention_layernorm.weight\": \"layers.{}.norms.w\",\n",
    "    \"model.norm.weight\": \"norm.w\",\n",
    "    \"lm_head.weight\": \"output.weight\",\n",
    "}\n",
    "{v: k for k, v in _FROM_HF.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb3091ab-d775-4705-b027-b702021b88ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tok_embd.weight': 'model.embed_tokens.weight',\n",
       " 'layers.{}.attention.wq.weight': 'model.layers.{}.self_attn.q_proj.weight',\n",
       " 'layers.{}.attention.wk.weight': 'model.layers.{}.self_attn.k_proj.weight',\n",
       " 'layers.{}.attention.wv.weight': 'model.layers.{}.self_attn.v_proj.weight',\n",
       " 'layers.{}.attention.wo.weight': 'model.layers.{}.self_attn.o_proj.weight',\n",
       " None: 'model.layers.{}.self_attn.rotary_emb.inv_freq',\n",
       " 'layers.{}.mlp.linear1.weight': 'model.layers.{}.mlp.gate_proj.weight',\n",
       " 'layers.{}.mlp.linear3.weight': 'model.layers.{}.mlp.up_proj.weight',\n",
       " 'layers.{}.mlp.linear2.weight': 'model.layers.{}.mlp.down_proj.weight',\n",
       " 'layers.{}.norms.w': 'model.layers.{}.post_attention_layernorm.weight',\n",
       " 'norm.w': 'model.norm.weight',\n",
       " 'output.weight': 'lm_head.weight'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{v: k for k, v in _FROM_HF.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b9ac7b-368d-451c-9d1e-1d27a177b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "_FROM_META = {\n",
    "    \"tok_embeddings.weight\": \"tok_embd.weight\",\n",
    "    \"norm.weight\": \"norm.w\",\n",
    "    \"output.weight\": \"output.weight\",\n",
    "    \"layers.{}.attention.wk.weight\": \"layers.{}.attention.wq.weight\",\n",
    "    \"layers.{}.attention.wq.weight\": \"layers.{}.attention.wk.weight\",\n",
    "    \"layers.{}.attention.wv.weight\": \"layers.{}.attention.wv.weight\",\n",
    "    \"layers.{}.attention.wo.weight\": \"layers.{}.attention.wo.weight\",\n",
    "    \"layers.{}.attention_norm.weight\": \"layers.{}.norms.w2\",\n",
    "    \"layers.{}.ffn_norm.weight\": \"layers.{}.norms.w\",\n",
    "    \"layers.{}.feed_forward.w1.weight\": \"layers.{}.mlp.linear1.weight\",\n",
    "    \"layers.{}.feed_forward.w2.weight\": \"layers.{}.mlp.linear2.weight\",\n",
    "    \"layers.{}.feed_forward.w3.weight\": \"layers.{}.mlp.linear3.weight\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1efab83c-34f6-437b-add0-0052462ffe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mapped_key(key: str, mapping_dict: Dict[str, str]) -> str:\n",
    "    try:\n",
    "        if \"layers\" in key:\n",
    "            # Replace layer number with \"{}\" to create key for lookup\n",
    "            abstract_key = re.sub(r\"(\\.\\d+)\", \".{}\", key)\n",
    "            layer_num = re.search(r\"\\d+\", key).group(0)\n",
    "\n",
    "            new_key = mapping_dict[abstract_key]\n",
    "\n",
    "            new_key = new_key.format(layer_num)\n",
    "        else:\n",
    "            new_key = mapping_dict[key]\n",
    "    except KeyError as e:\n",
    "        raise Exception(\n",
    "            f'Error converting the state dict. Found unexpected key: \"{key}\". '\n",
    "            \"Please make sure you're loading a checkpoint with the right format. \"\n",
    "        ) from e\n",
    "\n",
    "    return new_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28164057-6838-4467-b239-c48cccc9406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blm_to_hf(\n",
    "    state_dict: Dict[str, torch.Tensor],\n",
    "    num_heads: int = 12,\n",
    "    num_kv_heads: int = 12,\n",
    "    dim: int = 768,\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        state_dict (Dict[str, torch.Tensor]): State dict in blm's format.\n",
    "        num_heads (int): Number of heads in the model.\n",
    "        num_kv_heads (int): Number of heads in the key/value projection layers.\n",
    "        dim (int): Dimension of the model.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, torch.Tensor]: State dict in Meta's format.\n",
    "    \"\"\"\n",
    "    converted_state_dict = {}\n",
    "    inverted_mapping_dict = {v: k for k, v in _FROM_HF.items()}\n",
    "    head_dim = dim // num_heads\n",
    "\n",
    "    def _permute(t, n_heads):\n",
    "        return (\n",
    "            t.view(n_heads, head_dim // 2, 2, dim)\n",
    "            .transpose(1, 2)\n",
    "            .reshape((head_dim * n_heads), dim)\n",
    "        )\n",
    "\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = _get_mapped_key(key, inverted_mapping_dict)\n",
    "        if \"q_proj\" in key:\n",
    "            value = _permute(value, num_heads)\n",
    "        elif \"k_proj\" in key:\n",
    "            value = _permute(value, num_kv_heads)\n",
    "        converted_state_dict[new_key] = value\n",
    "\n",
    "    return converted_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6d6fa6d-8346-4ecb-82fd-803b873da954",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/pranav-pc/projects/OpenTransformer/multiformer/blm-1024/checkpoints/blm2hf/consolidated.00.pth\"\n",
    "# torch.save(blm_to_hf(model_dict['state_dict']),PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab092e01-8adc-4127-aa40-afc05f8a5454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, LlamaConfig, LlamaModel, LlamaTokenizerFast\n",
    "\n",
    "LlamaConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3539930-4506-482c-82a3-c5a2028824e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b41285d-91d1-40f5-b046-84af76cd25d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea5566-6a73-4f55-ac2f-5b287218f94b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "415f5d76-d3e0-41ab-b905-9d596c649a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_to_blm(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        state_dict (Dict[str, torch.Tensor]): State dict in Meta's format.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, torch.Tensor]: State dict in blm's format.\n",
    "    \"\"\"\n",
    "    converted_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key not in [\"rope.freqs\"]:  # Skip loading the position embeddings\n",
    "            new_key = _get_mapped_key(key, _FROM_META)\n",
    "            converted_state_dict[new_key] = value\n",
    "\n",
    "    return converted_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c26eacb0-10e7-40c8-83c7-ff935fbfb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_model = torch.load('/home/pranav-pc/projects/OpenTransformer/checkpoints/llama-2-7b/consolidated.00.pth')\n",
    "# llama_model = meta_to_blm(llama_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d83b7dcc-a888-46f0-92bf-49df283d0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = \"/home/pranav-pc/projects/OpenTransformer/multiformer/blm-1024/checkpoints/llama/blm-llama-7b.pth\"\n",
    "\n",
    "# llama_model['pytorch-lightning_version'] = '2.3.0.dev20240318'\n",
    "# llama_model['hparams_name'] = 'kwargs'\n",
    "# from src.models.blm.config import ModelArgs\n",
    "# llama_model['hyper_parameters'] = {'args': ModelArgs(vocab_size=32000, embedding_dim=4096, max_seq_len=4096, embedding_dropout=0.0, rms_norm_eps=1e-05, rope_scaling=1.0, rope_theta=10000.0, attention_bias=False, attention_dropout=0.0, num_attention_heads=32, num_key_value_heads=32, use_cache=True, use_sliding_window=True, residual_dropout=0.1, mlp_hidden_size=11008, mlp_dropout=0.0, num_layers=32, device='cpu', padding_idx=2),\n",
    "#  'is_causal': True,\n",
    "#  'attn_mask': None,\n",
    "#  'lr': 0.0005,\n",
    "#  'cosine_t_max': 1000}\n",
    "# torch.save(llama_model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3feaeacc-9039-465c-bd65-8ae91a8f185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blm_to_meta(state_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        state_dict (Dict[str, torch.Tensor]): State dict in blm's format.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, torch.Tensor]: State dict in Meta's format.\n",
    "    \"\"\"\n",
    "    converted_state_dict = {}\n",
    "    inverted_mapping_dict = {v: k for k, v in _FROM_META.items()}\n",
    "\n",
    "    for key, value in state_dict.items():\n",
    "        new_key = _get_mapped_key(key, inverted_mapping_dict)\n",
    "        converted_state_dict[new_key] = value\n",
    "\n",
    "    return converted_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b4c93f5-82f9-47dc-9636-681801e7ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blm_to_meta(model_dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35e47916-2b9e-4ee4-988a-c35acd6a5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_to_blm(\n",
    "    state_dict: Dict[str, torch.Tensor],\n",
    "    num_heads: int = 32,\n",
    "    num_kv_heads: int = 32,\n",
    "    dim: int = 4096,\n",
    "    head_dim: int = None,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Convert a state dict from HF's format to blm's format. State dicts\n",
    "    from multiple checkpoint files should be consolidated into a single state dict\n",
    "    before calling this function.\n",
    "\n",
    "    Eg of HF-format state dict can be found in the ``meta-llama/Llama-2-7b-hf``\n",
    "    repo in HF (https://huggingface.co/meta-llama/Llama-2-7b-hf).\n",
    "\n",
    "    Args:\n",
    "        state_dict (Dict[str, torch.Tensor]): State dict in Meta's format.\n",
    "        num_heads (int): Number of heads in the model.\n",
    "        num_kv_heads (int): Number of heads in the key/value projection layers.\n",
    "        dim (int): Dimension of the model.\n",
    "        head_dim (int): Dimension of the head. If not provided, it will be calculated\n",
    "            as dim // num_heads.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, torch.Tensor]: State dict in blm's format.\n",
    "    \"\"\"\n",
    "    converted_state_dict = {}\n",
    "    if head_dim is None:\n",
    "        head_dim = dim // num_heads\n",
    "\n",
    "    def _permute(t, n_heads):\n",
    "        return (\n",
    "            t.view(n_heads, 2, head_dim // 2, dim)\n",
    "            .transpose(1, 2)\n",
    "            .reshape((head_dim * n_heads), dim)\n",
    "        )\n",
    "\n",
    "    for key, value in state_dict.items():\n",
    "        if \"rotary_emb.inv_freq\" not in key:  # Skip loading the position embeddings\n",
    "            new_key = _get_mapped_key(key, _FROM_HF)\n",
    "            if \"q_proj\" in key:\n",
    "                value = _permute(value, num_heads)\n",
    "            elif \"k_proj\" in key:\n",
    "                value = _permute(value, num_kv_heads)\n",
    "            converted_state_dict[new_key] = value\n",
    "    return converted_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a78ce-7e2c-4edb-a250-2aab99c8ab44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
