files:
  data_path_train: "/home/pranav-pc/projects/OpenTransformer/multiformer/data/interim/TinyStories-Instruct-hf_train_65>tk>1024.hf"
  data_path_val: "/home/pranav-pc/projects/OpenTransformer/multiformer/data/interim/TinyStories-Instruct-hf_val_65>tk>1024.hf"
  tokenizer_path: "/home/pranav-pc/projects/OpenTransformer/multiformer/tokenizer_checkpoints"
paths:
  base_model_checkpoint: "/home/pranav-pc/projects/OpenTransformer/multiformer/blm-1024/checkpoints/last.ckpt"
  resume_from_checkpoint: "/home/pranav-pc/projects/OpenTransformer/multiformer/blm-finetuned-tinnystories/last.ckpt"

trainer_params:
  batch_size: 16
  num_workers: 25
  resume_training: true
  subset_ratio: 1 # Train on only x% of the data
  gradient_accumulation_scheduler: { 0: 4, 5: 3, 7: 2 }
  wandb_enabled: false
  # wandb:
  #   name: "blm-1024"
  #   save_dir: "blm-1024/"
  #   version: "v1.2"
  #   offline: true
  #   project: "tiny-stories"
  checkpoint:
    save_top_k: 2
    monitor: "train_loss"
    mode: "min"
    dirpath: "blm-finetuned-tinnystories/"
    filename: "baby-llm-instruct-{epoch:02d}-{train_loss:.3f}"
    save_last: true
    every_n_train_steps: 500
    save_on_train_epoch_end: true
    save_weights_only: true
  earlystopping:
    monitor: "train_loss"
    patience: 10
    verbose: true
  trainer:
    min_epochs: 1
    max_epochs: 5
    precision: "bf16-mixed"
    enable_model_summary: false
    # default_root_dir: "/home/pranav-pc/projects/OpenTransformer/multiformer/"
    enable_checkpointing: true
    fast_dev_run: false
    # log_every_n_steps: 10
    enable_progress_bar: true
    gradient_clip_val: 1.0
    # profiler: "simple"
    # check_val_every_n_epoch: null
    val_check_interval: 10000
